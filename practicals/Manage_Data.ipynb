{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "865d5018-bf37-425b-90d9-118da34fae93",
   "metadata": {},
   "source": [
    "# Data Creation / Curation\n",
    "\n",
    "This notebook assembles the source data (found in the `src_dir`) into a set of parquet and geoparquet files that will be easier to work with. \n",
    "\n",
    "This first block of code imports the coding libraries that we need (`pandas` and `geopandas`), as well as `Path` from the `pathlib` library, and `os` (operating system) and `re` (regular expressions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badeab13-b7a1-46b4-8599-67830f0d52c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "src_dir = Path.home() / 'data'\n",
    "out_dir = Path.home() / Path('work/data/clean')\n",
    "if not out_dir.exists():\n",
    "    out_dir.mkdir(parents=True)\n",
    "    print(f\"Creating 'clean' directory for output data in {out_dir}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56172e3f-66e3-4c54-bb76-a7127e45c903",
   "metadata": {},
   "source": [
    "## Jaipur Boundary\n",
    "\n",
    "Read in the Jaipur shape file (actually 3â€“5 files) and converting it to a single geoparquet file. This is going to make it easier to work with the data *and* save us disk space. In *total* we reduce the size of the files by something like 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211bcbbd-fa5e-49e8-b89a-ade6aff1f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "jp = gpd.read_file(src_dir / 'Jaipur_Boundary/18JMC.shp')\n",
    "jp.plot()\n",
    "jp.to_parquet(out_dir / 'Jaipur_Boundary.geoparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3467f78-e601-41b9-9054-d1a61b8958cc",
   "metadata": {},
   "source": [
    "## Jaipur Wards\n",
    "\n",
    "The Jaipur wards are split across *multiple* shapefiles, which makes them hard to work with. Here we read in all of the shapefiles in a directory and concatentate (`concat`) them together into a single 'wards geo-data frame'. This is the sort of thing that is easy to do in code but hard to do in ArcGIS, or even QGIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b7635-23f6-4f9b-8cda-2d2e1baa4f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path  = os.path.join(src_dir / 'Jaipur_Wards')\n",
    "wards = [os.path.join(path,x) for x in os.listdir(path) if x.endswith('.shp')]\n",
    "\n",
    "wgdf  = gpd.GeoDataFrame(pd.concat([gpd.read_file(w) for w in wards], ignore_index=True), crs=gpd.read_file(wards[0]).crs)\n",
    "wgdf.to_parquet(out_dir / 'Jaipur_Wards.geoparquet')\n",
    "wgdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e93e01-eb52-4b5d-884b-55e8262500d6",
   "metadata": {},
   "source": [
    "## Using OpenStreetMap Data\n",
    "\n",
    "This next section is a bit of a side-show but is important to understand the kinds of issues that you can encounter when trying to automate things. If we want to see what data OSM has for Jaipur, it would be handy if we could quickly have a look by generating a link directly to the area of interest. Unfortunately, OSM uses the `EPSG:4326` projection (or `CRS`) while cadastral agencies in India use `ESPG:32643`, so the below link does not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239ad505-369e-486c-ad9c-8b1c95227176",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Jaipur data CRS is {jp.crs}\")\n",
    "bounds = jp.bounds # The bounding box for Jaipur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944e46a6-afdf-497f-a5f0-6ca71dd21089",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = f\"https://api.openstreetmap.org/api/0.6/map?bbox={bounds.minx[0]:0.3f},{bounds.miny[0]:0.3f},{bounds.maxx[0]:0.3f},{bounds.maxy[0]:0.3f}\"\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bbccc9-e04e-477e-b7f0-f2edbf05cc22",
   "metadata": {},
   "source": [
    "To solve this, we can reproject Jaipur (`jpr` == `Jaipur Reprojected`) into the OSM projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c366a5-addd-4630-8d87-33536caf0cd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jpr = jp.to_crs('EPSG:4326')\n",
    "print(f\"Reprojected Jaipur data CRS is {jpr.crs}\")\n",
    "jpr.plot(); # Notice the units on the side!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6428023a-5afc-4f74-8211-0224a1583236",
   "metadata": {},
   "source": [
    "This link will now work to show the area of Jaipur in OSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc823f-8b63-477a-9cc3-fdc0decffddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = jpr.bounds\n",
    "print(f\"https://api.openstreetmap.org/api/0.6/map?bbox={bounds.minx[0]:0.3f},{bounds.miny[0]:0.3f},{bounds.maxx[0]:0.3f},{bounds.maxy[0]:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a6d2b-930d-4228-94af-c9b111b33617",
   "metadata": {},
   "source": [
    "OK, technically that *still* doesn't work, but that is only because we are requesting too much data. We could reduce our request by requesting only some *types* of data or breaking the area of interest down into sub-requests that we reassemble on our side. In this case, the simpler way is either to use the [HOT Export](https://export.hotosm.org/) tool or to pick up a 'pre-pack' from [Geofabrik](https://download.geofabrik.de/).\n",
    "\n",
    "But perhaps it would be better to buffer the boundary so that roads aren't cut off *right* at the border of the city? However, can you spot what is wrong with the map below? Why is the middle of Jaipur still white and not filled in with grey?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a1385-af2e-41a2-9186-981774202121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jp.buffer(1000).plot(color='lightgrey', edgecolor='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576c5d5-afd7-4fd1-8e0c-c8443b93e8e2",
   "metadata": {},
   "source": [
    "Luckily, there's some code to solve that for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb1491-4eb6-484b-b909-1538e9d169e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons, cuts, dangles, invalid = jp.polygonize(full=True)\n",
    "polygons.buffer(1000).plot(color='lightgrey', edgecolor='red')\n",
    "jpdf = gpd.GeoDataFrame({'city':['Jaipur'], 'geometry':polygons})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d629286-7247-4d5b-b3b6-39c24a286f4b",
   "metadata": {},
   "source": [
    "## Processing OSM Data for Jaipur\n",
    "\n",
    "From Geofabrik I downloaded the all-in-one `pbf` file for Jaipur. The PBF file contains multiple *types* of geographic object: the layer types are 'points' (default), 'lines', 'multilinestrings', 'multipolygons', 'other_relations'. So we need/want to split these into different files.\n",
    "\n",
    "Notice that this line below converts the buffered Jaipur data into the OSM projection (and extracts the boundary of the geometry) in a single step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01daf366-a46a-480a-acec-dc0082a071a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jpdf.buffer(5000).to_crs('EPSG:4326').geometry[0].bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85caa5b-370c-41ba-9d8a-9cb975788feb",
   "metadata": {},
   "source": [
    "It's worth remembering that these outputs have *not* been reprojected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c1c3c-0b4b-4059-9177-4b19d86f1efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.read_file(src_dir / 'OSM/northern-zone-latest.osm.pbf',\n",
    "             engine='pyogrio', use_arrow=True, bbox=jpdf.buffer(5000).to_crs('EPSG:4326').geometry[0].bounds, layer='lines'\n",
    "        ).to_parquet(out_dir / 'northern-zone-lines.geoparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d53f2-4a14-4f48-b955-2eb7b30ff464",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.read_file(src_dir / 'OSM/northern-zone-latest.osm.pbf', \n",
    "            engine='pyogrio', use_arrow=True, bbox=jpdf.buffer(5000).to_crs('EPSG:4326').geometry[0].bounds, layer='points'\n",
    "        ).to_parquet(out_dir / 'northern-zone-points.geoparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745328c0-ebbf-4140-83ec-7da03afddfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.read_file(src_dir / 'OSM/northern-zone-latest.osm.pbf', \n",
    "            engine='pyogrio', use_arrow=True, bbox=jpdf.buffer(5000).to_crs('EPSG:4326').geometry[0].bounds, layer='multipolygons'\n",
    "        ).to_parquet(out_dir / 'northern-zone-multipolygons.geoparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb9cd7-003f-4ab8-a226-596d90d05950",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.read_file(src_dir / 'OSM/northern-zone-latest.osm.pbf', \n",
    "            engine='pyogrio', use_arrow=True, bbox=jpdf.buffer(5000).to_crs('EPSG:4326').geometry[0].bounds, layer='multilinestrings'\n",
    "        ).to_parquet(out_dir / 'northern-zone-multilines.geoparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21ede3-d9d6-40ed-a6bf-b71cb8b75ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.read_file(src_dir / 'OSM/northern-zone-latest.osm.pbf', \n",
    "            engine='pyogrio', use_arrow=True, bbox=jpdf.buffer(5000).to_crs('EPSG:4326').geometry[0].bounds, layer='other_relations'\n",
    "        ).to_parquet(out_dir / 'northern-zone-other.geoparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0dc30b-61f1-49f8-84a9-16ab83d1e490",
   "metadata": {},
   "source": [
    "## Census Data\n",
    "\n",
    "This section shows how code allows us to deal with complex tables in a way that is reproducible and verifiable: we don't need to wonder where a field came from or how it was derived, we can *see* in the code what happened and judge accordingly. Handling this in ArcGIS would require manual copying and pasting of data into new tables and that is the point where errors can sneak in. A slip of the finger and columns are transposed or other errors introduced.\n",
    "\n",
    "What makes this section particularly tricky is that we're dealing with *hierarchical* columns: there are columnar groups of data (e.g. housing condition, access to sewerage, etc.) that relate to a single theme. We want to keep those together while also making the data a little easier to handle. But when we first load this data what we see is lots of \"Unnamed: X_level_Y\" values in bold and it looks like we've made a mistake!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb7549f-7b8f-40e0-b7c1-35d78e2a511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\n",
    "    src_dir / 'Wardwise/households/HLPCA-08110-2011_H14_census.xlsx', \n",
    "    sheet_name='Housing', \n",
    "    header=[2,3,4,5]\n",
    ")\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1871410-6681-4882-8d25-563b00359c00",
   "metadata": {},
   "source": [
    "### Dealing with Multi-Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06acab2-555e-4911-874a-d238c604201a",
   "metadata": {},
   "source": [
    "This retrieves a single group of columns from the 'top' level of the hierarchy: all columns relating the 'availability of assets'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9e1ba2-23b5-43bd-b3e0-804580a2975e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[:,(['Availability of assets'])].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092b421-4f47-4c9b-89c5-a6efb3011cf3",
   "metadata": {},
   "source": [
    "This retrieves a group of columns from the 'bottom' level of the hierarchy: all columns in the range from 125 to 139. So we need to tell Python that we're not interested in levels 1, 2, or 3, just the columns numbered 125..139. That's why you'll see `slice(None)` there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddf4075-1b6c-471b-ad28-51462bd723fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,(slice(None), slice(None), slice(None), range(125,140))].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddc422c-fe36-4158-b5f1-d52598ae68bb",
   "metadata": {},
   "source": [
    "Here's how we get a list of all the top-level values, you get one value for *each* column, so that's why you see `'Households by Type of Structure of Census Houses'` repeated multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a260aea3-d8cd-4de9-b105-f9ef1ff600d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.get_level_values(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e375e2f-ff13-4a41-936a-a2a212ad43df",
   "metadata": {},
   "source": [
    "The first 9 columns (skipping the empty first column) are 'required' for linking the data to other data sets. We're going to stash those in a list variable called `required`. For the rest, we're going to ask for the *unique* top-level values and print them out so that we can create our own groups. Notice that we check for a *really* annoying problem that often crops up with data: invisible characters in a column name that always give mapping software fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2251ba-b5d5-4b51-b2db-2da5bd3e75f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "required = df.columns.get_level_values(0).to_list()[0:10]\n",
    "print(f\"These fields are required for matching: '{\"', '\".join(required)}'\")\n",
    "print()\n",
    "print(f\"These are the top-level columns groups in the data...\")\n",
    "for c in set(df.columns.get_level_values(0).to_list()[10:]):\n",
    "    print(f\"'{c}'\")\n",
    "    if c.strip() != c:\n",
    "        print(\"^^^^^ Watch out, there are hidden characters here! ^^^^^^\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d4eb60-2ac6-4c76-8ea6-443545b8c497",
   "metadata": {},
   "source": [
    "We're going to create a 'map' (in the sense of connections) between the column groups and an output file here, so the 'condition' output file will contain all columns from the `'Number of households with condition of Census House as '` group. And so on. At this point it's worth remembering that the lowest level of the column groupings is just a unique number between 1 and 145... which might be useful as a short-cut but can't be used as the label itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8230e76f-aa27-45d6-85ef-47cf1eade54d",
   "metadata": {},
   "source": [
    "### Rewriting and Outputting Column Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127d7aaf-b40e-492d-8fa4-0cae5efe7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_map = {\n",
    "    'condition': ['Number of households with condition of Census House as '],\n",
    "    'materials': ['Material of Roof','Material of Wall','Material of Floor'],\n",
    "    'occupancy': ['Number of Dwelling Rooms','Household size','Ownership status','Married couple'],\n",
    "    'utilities': ['Main Source of Drinking Water','Location of drinking water source','Main Source of lighting', \n",
    "                  'Number of households having latrine facility within the premises','Flush/pour flush latrine connected to','Pit latrine', \n",
    "                  'Night soil disposed into open drain\\n','Service Latrine','Number of households not having latrine facility within the premises',\n",
    "                  'Alternative source','Number of households having bathing facility within the premises','Waste water outlet connected to',\n",
    "                  'Type of Fuel used for Cooking','Kitchen facility'],\n",
    "    'assets':    ['Total number of households availing banking services','Availability of assets'],\n",
    "    'structure': ['Households by Type of Structure of Census Houses']\n",
    "}\n",
    "output_colmap = {\n",
    "    'Number of households with condition of Census House as ': 'house_condition',\n",
    "    'Material of Roof': 'roof',\n",
    "    'Material of Wall': 'wall',\n",
    "    'Material of Floor': 'floor',\n",
    "    'Number of Dwelling Rooms': 'rooms',\n",
    "    'Household size': 'hh_size',\n",
    "    'Ownership status': 'own',\n",
    "    'Married couple': 'married',\n",
    "    'Main Source of Drinking Water': 'water_src',\n",
    "    'Location of drinking water source': 'water_loc',\n",
    "    'Main Source of lighting': 'light_src', \n",
    "    'Number of households having latrine facility within the premises': 'latrine_on_site',\n",
    "    'Flush/pour flush latrine connected to': 'latrine_connect',\n",
    "    'Pit latrine': 'latrine_pit', \n",
    "    'Night soil disposed into open drain\\n': 'night_soil_open_drain',\n",
    "    'Service Latrine': 'latrine_service',\n",
    "    'Number of households not having latrine facility within the premises': 'latrine_none',\n",
    "    'Alternative source': 'latrine_other',\n",
    "    'Number of households having bathing facility within the premises': 'bathing',\n",
    "    'Waste water outlet connected to': 'water_out',\n",
    "    'Type of Fuel used for Cooking': 'cook_fuel',\n",
    "    'Kitchen facility': 'kitchen',\n",
    "    'Total number of households availing banking services': 'banked',\n",
    "    'Availability of assets': 'assets',\n",
    "    'Households by Type of Structure of Census Houses': 'structure'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111bbb47-803b-4113-87e2-18bacb898ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "for k,v in output_map.items():\n",
    "    \n",
    "    print(f\"Creating data file for '{k}' columns...\")\n",
    "    \n",
    "    tmp = df.loc[:,required+v]\n",
    "    cols = tmp.columns\n",
    "    #new_cols = [x.replace('\\n','').strip() for x in required] # Our new column labels\n",
    "    col_map = {}\n",
    "    for c in cols:\n",
    "        if DEBUG: print(f\"Raw, hierarchical column name: {c}\")\n",
    "        idx = c[-1] # Short-hand for the last item in the list, which is an integer identifier\n",
    "        nms = [x for x in c[:-1] if not x.startswith('Unnamed') and not x in v]\n",
    "        if c[0] in output_colmap:\n",
    "            nms.insert(0, output_colmap[c[0]]) # The top-level is too long to be practical\n",
    "        # Have found *one* case of duplicate columns names that we fix here\n",
    "        if idx == 124 or idx == 125:\n",
    "            nms.append('bis')\n",
    "        if DEBUG: print(f\"\\tIntermediate, post-cleaning label: {nms}\")\n",
    "        lbl = f\"{'_'.join(re.sub(r'\\W+','_',x.strip().strip('_').lower()) for x in nms)}\"\n",
    "        if DEBUG: \n",
    "            print(f\"\\tFinal label: {lbl}\")\n",
    "            print()\n",
    "        col_map[c] = lbl\n",
    "    if DEBUG: print(f\"Column mapping: {col_map}\")\n",
    "    # Convert multi-index to flat index\n",
    "    tmp.columns = col_map.values()\n",
    "    # And save\n",
    "    try: \n",
    "        tmp.to_parquet(out_dir / f'{k}.parquet')\n",
    "    except ValueError:\n",
    "        print(\"Error with duplicated column names?\")\n",
    "        #print(col_map.keys())\n",
    "        #print(sorted(col_map.values()))\n",
    "    if DEBUG: print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5e06ed-a417-4cb1-ba14-32df8f5b48f1",
   "metadata": {},
   "source": [
    "## Population Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73937dec-2aab-4dd9-abba-087c4e462ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wpop = pd.read_excel(\n",
    "    src_dir / 'Wardwise/population/wardwise population data as per 2011 census.xlsx'\n",
    ")\n",
    "print(f\"Loaded {wpop.shape} cells\")\n",
    "wpop.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4522811-17f0-4324-a9d0-cff1c7b32c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wpop.to_parquet(out_dir / 'ward_population.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d055f-4bba-4766-a49a-31e358c8f609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Then need to filter down for Jaipur\n",
    "jpop = wpop[wpop.Name.str.contains('Jaipur (M Corp.)', regex=False)]\n",
    "\n",
    "print(f\"Extracted {jpop.shape} cells\")\n",
    "jpop.head(2) # Note the levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e2a4e-5f89-45e3-be1c-531e8afa6231",
   "metadata": {},
   "outputs": [],
   "source": [
    "jpop.to_parquet(out_dir / 'jaipur_population.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
