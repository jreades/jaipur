[
  {
    "objectID": "setup/virtualisation.html",
    "href": "setup/virtualisation.html",
    "title": "Understanding Virtualisation",
    "section": "",
    "text": "You’ll see below that we talk about both Podman and Docker interchangeably. You may have heard of one, but not the other. Is there a difference? In terms of what they do there is no difference that you need to care about at the moment: both allow you to build images of virtual machines, share them with other users, and run any image you find as a container. All of these words are explained below. The point being, they aim to do the same thing.\nHowever, how they do this is quite different. Docker is/was the market leader in this kind of image/container sharing system and is very stable and well-supported; however, to be installed and run Docker requires administrator access (also known as root).\nPodman is a more recent, open source tool: it is slightly less stable and widely supported than Docker, and to make the most of it you need to change quite a few things. However, the biggest benefit is that you can run it as a user: this makes installation and execution much easier.\nWe are trying to move over to Podman because it will make everyone’s life easier, but sometimes we run into issues that cause us to fall back to using Docker instead. Hope that makes sense!\n\n\n\n\n\n\nTL;DR\n\n\n\nChoose Podman unless you have a specific reason to choose Docker instead or you have been unable to successfully install and run Podman."
  },
  {
    "objectID": "setup/virtualisation.html#podman-or-docker",
    "href": "setup/virtualisation.html#podman-or-docker",
    "title": "Understanding Virtualisation",
    "section": "",
    "text": "You’ll see below that we talk about both Podman and Docker interchangeably. You may have heard of one, but not the other. Is there a difference? In terms of what they do there is no difference that you need to care about at the moment: both allow you to build images of virtual machines, share them with other users, and run any image you find as a container. All of these words are explained below. The point being, they aim to do the same thing.\nHowever, how they do this is quite different. Docker is/was the market leader in this kind of image/container sharing system and is very stable and well-supported; however, to be installed and run Docker requires administrator access (also known as root).\nPodman is a more recent, open source tool: it is slightly less stable and widely supported than Docker, and to make the most of it you need to change quite a few things. However, the biggest benefit is that you can run it as a user: this makes installation and execution much easier.\nWe are trying to move over to Podman because it will make everyone’s life easier, but sometimes we run into issues that cause us to fall back to using Docker instead. Hope that makes sense!\n\n\n\n\n\n\nTL;DR\n\n\n\nChoose Podman unless you have a specific reason to choose Docker instead or you have been unable to successfully install and run Podman."
  },
  {
    "objectID": "setup/virtualisation.html#what-is-virtualisation",
    "href": "setup/virtualisation.html#what-is-virtualisation",
    "title": "Understanding Virtualisation",
    "section": "What is Virtualisation?",
    "text": "What is Virtualisation?\nPodman and Docker are ‘virtualisation’ tools that allows you to run ‘virtual machines’ on your computer’s ‘host’ operating system. That’s a lot of new, probably meaningless words. If you’re one of those people who (understandably) likes to understand what’s going on then here’s how some people define it:\n\nGoogle on What is a virtual machine?\nVMWare on What is a virtual machine?\nMicrosoft on What is a virtual machine (VM)?"
  },
  {
    "objectID": "setup/virtualisation.html#podmandocker-in-a-nutshell",
    "href": "setup/virtualisation.html#podmandocker-in-a-nutshell",
    "title": "Understanding Virtualisation",
    "section": "Podman/Docker in a Nutshell",
    "text": "Podman/Docker in a Nutshell\nSo in order to make use of Podman/Docker (and understand what’s happening when you get errors), it’s helpful to have some sense of what’s going on behind the scenes. You can click on the image below to make it larger, or you can download and print out a PDF version.\n\n\n\nSketch of Podman/Docker Usage\n\n\nHere’s what’s happening:\n\nStep 1. podman pull/docker pull\nYou issue the podman pull jreades/sparc:2025 (or docker pull jreades/sparc:2025) command to your computer, which turns around and asks the Hub for a copy of this image. The Hub responds by transferring a copy of the jreades/sparc:2025 image to your computer. You now have a file containing all the instructions to set up and run a virtual machine on your computer.1\n\n\nStep 2, docker run\nYou will issue the podman run ... jreades/sparc:2025 ... command (or docker run ...) from the command line (or terminal) of your computer, and this tells Docker/Podman to use the jreades/sparc:2025 image as a template for creating a container called sparc2. sparc will do whatever it was told to do by its creator. This could be wait to run Python code, start up a database, serve web pages, the list is practically endless. But sparc is read-only, although you can make changes to the container while it’s running, as soon as you shut it down those changes are lost. So you cannot break an image, only a container. And if you do that, you delete the container and start a new one from the image… we can cover this if you ever do it.\nAs part of the run command, we also told Docker/Podman what resources the container could access. There are two main types of resources for our purposes:\n\nA mount point which is a part of your computer’s hard drive that we can use to write things down permanently. We use $(pwd), which is short-hand for print working directory and refers to the ‘place’ on your computer where we issued the docker run command. We tell the platform to connect this to a directory called work (which resides in /home/jovyan/) on the sparc container. This allows you to share data between the container and your computer, and for changes to be saved when you shut down.\nOne or more ports which are like channels on a radio where the container can talk to other computers (including yours). In this case, we connect port 8888 on sparc to port 8888 on your computer. And that is why you have to tell your browser to go to localhost:8888 to access Jupyter Lab.\n\n\n\nStep 3. Interacting with the Container\nNow when you type things into the browser and tell code to ‘run’, what’s actually happening is that your computer is forwarding the request to the container, which does its thing, updates the web page, and this change is then forwarded back to you via the browser.\n\n\nStep 4. Anatomy of run\nAt the bottom of the Figure above you can see a full run command, here we just want to focus on the most important options (each -X is an option) for most users:\n\n-v: this specified the point on your hard drive that the sparc can use. By default we use $(pwd) which means ‘use the location where the docker run command was executed. You can also ’hard code’ this to something like /Users/&lt;your username&gt;/Documents/casa/ if you always want to use the same location.\n-p: this specified the channel (or port) on which the web browser can talk to the sparc.\njreades/sparc:2025: this specified the image we wanted to use"
  },
  {
    "objectID": "setup/virtualisation.html#footnotes",
    "href": "setup/virtualisation.html#footnotes",
    "title": "Understanding Virtualisation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA virtual machine is just a computer that runs on your computer. So it ‘borrows’ resources like hard drive space, memory, and processor in order to behave like an independent computer that you can interact with in various ways.↩︎\nA container is the name Podman/Docker uses to refer to a running virtual machine. The image on its own does nothing until you tell docker to run it, at which point it becomes a container!↩︎"
  },
  {
    "objectID": "setup/moving_to_d.html",
    "href": "setup/moving_to_d.html",
    "title": "Moving Docker’s Storage Location on Windows 10 Pro",
    "section": "",
    "text": "To change the default location of Docker’s storage drive (which is typically on the C: drive) on Windows 10 Pro, you’ll need to adjust the configuration in Docker Desktop to move the virtual machine’s disk image. Here’s how to do it:\nSteps:\nSteps to Change Docker’s Storage Location on Windows 10 Pro:"
  },
  {
    "objectID": "setup/moving_to_d.html#last-steps",
    "href": "setup/moving_to_d.html#last-steps",
    "title": "Moving Docker’s Storage Location on Windows 10 Pro",
    "section": "Last Steps!",
    "text": "Last Steps!\nThe following should be done as the last migration steps to move Docker Disk Image Location:\n\nOpen Docker Desktop and go to Settings.\nNavigate to Resources &gt; Advanced.\nIn the Disk Image Location section, click Browse and select the new location, such as D:or another folder on the D: drive.\nAfter selecting the new location, apply the changes and restart Docker Desktop.\n\nThis should relocate Docker’s WSL2 disk image from C: to your D: drive. Screenshot is attached below for reference.\n\n\n\nDocker Disk Image Location"
  },
  {
    "objectID": "setup/health.html",
    "href": "setup/health.html",
    "title": "Requirements",
    "section": "",
    "text": "This guide will help you to perform a basic ‘health check’ on your computer to see if you’re likely to run into problems running the programming environment. When answering the questions below, ‘your computer’ is the machine on which you plan to do the programming.\nWe try to support as many different configurations as possible, but there is no programming environment that installs and runs seamlessly on all computers so if you run into issues please come back to this guide.",
    "crumbs": [
      "Requirements"
    ]
  },
  {
    "objectID": "setup/health.html#hardware-requirements",
    "href": "setup/health.html#hardware-requirements",
    "title": "Requirements",
    "section": "Hardware Requirements",
    "text": "Hardware Requirements\nIn our experience the students most likely to encounter problems share one or more of the following:\n\nYour computer 8GB or less of RAM.\nYour computer has less than 20GB of free disk space remaining.\n\nRead on below to check what specification you have…\n\nMacOSWindowsLinux\n\n\nYou will need to look up:\n\nHow much RAM does your computer have? Help for Mac.\nHow much free disk space does your computer have? Help for Mac.\n\n\n\nYou will need to look up:\n\nHow much RAM does your computer have? Help for Windows.\nHow much free disk space does your computer have? Help for Windows.\n\n\n\nWe’re going to assume that you know what you’re doing. If you want a recommendation, we’d probably go with the latest Ubuntu desktop release.\n\n\n\nIf your computer has less than 8GB of RAM and/or less than 20GB of free disk space you are likely to have issues with virutalisation and (on Windows) WSL2. Your options are: 1) add more RAM (possible on Windows); 2) back up and remove unnecessary files (Movies, Photos, and Applications are particularly big ‘hogs’).",
    "crumbs": [
      "Requirements"
    ]
  },
  {
    "objectID": "setup/health.html#software-requirements",
    "href": "setup/health.html#software-requirements",
    "title": "Requirements",
    "section": "Software Requirements",
    "text": "Software Requirements\nIn our experience the students most likely to encounter problems share one or more of the following:\n\nYour computer runs Windows 10 Home or older, or\nYour computer runs MacOS 10.13 (High Sierra) or older.\n\nAs long as your computer is running one of the last two major releases of the Operating System you should encounter few issues.\n\nMacOSWindows\n\n\nYou will need to look up:\n\nWhat Operating System and Version is your computer running? Help for Mac.\n\n\n\nYou will need to look up:\n\nWhat Operating System and Version is your computer running? Help for Windows.\n\n\n\n\n\nRecommendations\nIf you are using a Mac then your system should have the option to update to the latest version of the MacOS at no charge. If you are unable to update then it is likely that you have an older machine that is not fully supported by the most recent Operating System and, in all probability, you will also encounter issues running the programming environment.\nIf you are using a Windows PC then try to update to either Windows 11 or to Windows 10 Pro as this will ‘unlock’ additional features that are useful for supporting the programming environment. As a student you are likely to qualify for significantly cheaper/free updates, so make sure you do this when you have access to a discount.",
    "crumbs": [
      "Requirements"
    ]
  },
  {
    "objectID": "setup/env.html",
    "href": "setup/env.html",
    "title": "Installation",
    "section": "",
    "text": "Over the years, we have experimented with a range of approaches to setting you up with a programming environment, and we have come to the conclusion that Podman1 is the most robust way to ensure a consistent experience. This guarantees that all students end up with the same versions of each library, that difficult-to-diagnose hardware/OS issues are minimised, and that running/recovery is the most straightforward.",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "setup/env.html#what-kind-of-computer",
    "href": "setup/env.html#what-kind-of-computer",
    "title": "Installation",
    "section": "What Kind of Computer?",
    "text": "What Kind of Computer?\n\nApple Intel or Silicon/M-chip?\nApple has stopped using Intel computer chips and now uses its own ‘Silicon’ (aka ‘M’) chips instead. Confusingly, these are often also referred to by their technical specification as AMD64 (Intel) and ARM64 (Apple), respectively. To find out which you have, click on the Apple icon () and select About this Mac. Look at the Processor section and you should see which type of computer you have. If you see the word Intel then you have an Intel computer, if you see M1, M2, M3, etc. then you have an Apple Silicon computer.\n\n\nWindows 32- or 64-bit?\nIt is highly unlikely that you have a 32-bit machine, so if in doubt download the 64-bit version. It is also unlikely that you have an ARM device if you have an off-the-shelf Windows computer.\nPlease download all of the Essential packages for your type of computer (Apple Intel, Apple Silicon, Windows). The rest are listed for your convenience and you can install them as you see fit.\n\nApple IntelApple Silicon (M-chips)Windows PCs\n\n\n\nEssential:\n\nPodman for AMD64\niTerm2\n\nOptional:\n\nQGIS Long-Term Release (LTR)\nGitHub Desktop\n\nOnly if Podman doesn’t work:\n\nDocker Desktop for Mac with Intel chip\n\n\n\n\n\nEssential:\n\nPodman for ARM64\niTerm2\n\nOptional:\n\nQGIS Long-Term Release (LTR)\nGitHub Desktop\n\nOnly if Podman doesn’t work:\n\nDocker Desktop for Mac with Apple silicon\n\n\n\n\n\nEssential:\n\nWindows Terminal installed via either Windows Store or winget.\nPodman for AMD64 (it’s unlikely that you need the ARM64 installer)\n\nOptional:\n\nQGIS Long-Term Release (LTR)\nGit\nGitHub Desktop\n\nOnly if Podman doesn’t work:\n\nDocker Desktop for Windows",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "setup/env.html#podman-or-docker",
    "href": "setup/env.html#podman-or-docker",
    "title": "Installation",
    "section": "Podman or Docker?",
    "text": "Podman or Docker?\nYou’ll see below that we talk about both Podman and Docker interchangeably. You may have heard of one, but not the other. Is there a difference? In terms of what they do there is no difference that you need to care about at the moment: both allow you to build images of virtual machines, share them with other users, and run any image you find as a container. All of these words are explained below. The point being, they aim to do the same thing.\nHowever, how they do this is quite different. Docker is/was the market leader in this kind of image/container sharing system and is very stable and well-supported; however, to be installed and run Docker requires administrator access (also known as root).\nPodman is a more recent, open source tool: it is slightly less stable and widely supported than Docker, and to make the most of it you need to change quite a few things. However, the biggest benefit is that you can run it as a user: this makes installation and execution much easier.\nWe are trying to move over to Podman because it will make everyone’s life easier, but sometimes we run into issues that cause us to fall back to using Docker instead. Hope that makes sense!\n\n\n\n\n\n\nTL;DR\n\n\n\nChoose Podman unless you have a specific reason to choose Docker instead or you have been unable to successfully install and run Podman.",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "setup/env.html#installing",
    "href": "setup/env.html#installing",
    "title": "Installation",
    "section": "Installing",
    "text": "Installing\n\n\n\n\n\n\nHow They Work\n\n\n\nOne way to think of Podman/Docker is as a ‘library’ of ready-made virtual computers that you can copy and use free-of-charge. If you’d like to know more about what Podman/Docker are and how they work, you can read more in the Understanding Virtualisation section.\n\n\n\n\n\n\n\n\nWindows Users\n\n\n\nPlease ensure that you have installed WSL2 before installing or Podman or Docker! If you cannot install WSL2 then please have a look at the ‘dealing with errors’ section.\n\n\nAfter you’ve downloaded Podman (first choice) or Docker (second choice), you need to:\n\nInstall it – usually this will mean opening the image and either dragging it your Application folder (Mac) or running the installer (Windows)\nStart it up – double-click the Podman/Docker icon in your Applications folder to start the application running.\nFinish setup – you may see a login screen like the one below, but you do not need to create an account (notice Continue without signing in)\nOn all the subsequent questions you can Skip (upper-right corner) answering as well.\n\n\n\n\nDocker trying to trick you into creating an account\n\n\nYou must finish setting up before proceeding to the next step. You’ll know that you’re ready to move on when you see either the ‘Podman Desktop’ or ‘Docker Desktop’ window appear listing ‘downloaded images’ and ‘running containers’:\n\n\n\nPodman Desktop\n\n\n\n\n\nDocker Desktop\n\n\nIf you didn’t see this then you will need to have a look at the ‘dealing with errors’ section.\n\n\n\n\n\n\nImportant\n\n\n\nInstallation on a Mac should be fairly straightforward (as evidenced by the dearth of documentation), but for Windows there is quite a bit more detail.",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "setup/env.html#configuration",
    "href": "setup/env.html#configuration",
    "title": "Installation",
    "section": "Configuration",
    "text": "Configuration\nPodman requires you to create a virtual machine with user-specified characteristics before you can run an image. You can think of this as specifying the ‘hardware’ of the computer before you buy it (How much RAM? How big a hard drive? And so on.). In principle, this means that you can have more than one valid configuration at a time which would allow you to tune each virtual machine to your needs (one machine could have a lot of memory, another a lot of storage).\nIn contrast, Docker uses a single configuration for all machines. If you want to change the configuration you need to shut everything down and then change the system setup in Docker.\n\nConsiderations\nYou should create a virtual machine that has enough resources to do the job, but not so much that it affects your ability to do other work on the computer. Since every computer is unique, you need to look at your computer’s configuration and work it out from there but below is some basic guidance. You will find it easiest to configure this using Podman Desktop (next section) and clicking on the Settings and Create Podman machine.\n\n\n\nTable 1: Memory (RAM)\n\n\n\n\n\nIf your computer has\n8GB\n16GB\n32GB\n64GB\n\n\n\n\nWe’d recommend\n1-2GB\n2-3GB\n4–8GB\nLucky you!\n\n\n\n\n\n\n\n\n\nTable 2: Processing Power (CPUs)\n\n\n\n\n\nIf your computer has\n2CPUs\n8CPUs\n16CPU\n32CPUs\n\n\n\n\nWe’d recommend\n1CPU\n2-4CPUs\n4–8CPUs\nLucky you!\n\n\n\n\n\n\n\n\n\nTable 3: Free Disk Space (Disk size)\n\n\n\n\n\n\n\n\n\n\n\n\nIf your computer has\n10GB Free\n20GB Free\n100GB Free\n&gt; 150GB Free\n\n\n\n\nWe’d recommend\nTidying up\n10GB\n30GB\nLucky you!\n\n\n\n\n\n\nNote that some Windows machines come configured with two hard drives (usually called C and D); in those cases D is intended for data and often doesn’t have much on on it! In those cases, you can change the Image Path option in Podman desktop to store the image on the D drive and avoid problems with filling up your C drive.\n\n\nInitialising a Podman Machine\nThe easiest way to configure your Podman machine is to use Podman Desktop. When you launch the Desktop, at the lower-left you’ll see a Settings button. Click this and then (if it’s not already selected) choose the Resources tab.\n\n\n\nPodman Resources\n\n\nSelect Create new and Podman will pick some sensible defaults for you; however, you might want to fine-tune the settings in line with the options set out above in Tables Table 1 and Table 2 especially. Here’s one setup for a fairly recent Apple MacBookPro M2.\n\n\n\nPodman Machine Initalisation\n\n\nClick on Create and you are now ready to run your first container from an image!",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "setup/env.html#testing",
    "href": "setup/env.html#testing",
    "title": "Installation",
    "section": "Testing",
    "text": "Testing\nTo test if the application (Docker or Podman) is installed correctly, you will need either the Terminal (macOS) or the Windows Terminal (Windows).\n\n\n\n\n\n\nCopy Code to Clipboard\n\n\n\nWhenever you see a ‘code block’ below, you will also see a ‘clipboard’ icon () in the right. Click that, and the code will be copied to your computer’s ‘clipboard’ so that you can then paste it into the Terminal or Windows Terminal (see Base Utilities. That will save you a lot of time and effort.\n\n\nBoth Podman and Docker have provided a simple way to test if your installation is working correctly. You can run the following command:\n\nPodmanDocker\n\n\npodman run hello-world\nThis should output something like:\nResolved \"hello-world\" as an alias (/etc/containers/registries.conf.d/000-shortnames.conf)\nTrying to pull quay.io/podman/hello:latest...\nGetting image source signatures\nCopying blob sha256:1ff9adeff4443b503b304e7aa4c37bb90762947125f4a522b370162a7492ff47\nCopying config sha256:83fc7ce1224f5ed3885f6aaec0bb001c0bbb2a308e3250d7408804a720c72a32\nWriting manifest to image destination\n!... Hello Podman World ...!\n\n         .--\"--.\n       / -     - \\\n      / (O)   (O) \\\n   ~~~| -=(,Y,)=- |\n    .---. /`  \\   |~~\n ~/  o  o \\~~~~.----. ~~\n  | =(X)= |~  / (O (O) \\\n   ~~~~~~~  ~| =(Y_)=-  |\n  ~~~~    ~~~|   U      |~~\n\nProject:   https://github.com/containers/podman\nWebsite:   https://podman.io\nDesktop:   https://podman-desktop.io\nDocuments: https://docs.podman.io\nYouTube:   https://youtube.com/@Podman\nX/Twitter: @Podman_io\nMastodon:  @Podman_io@fosstodon.org\n\n\ndocker run hello-world\nThis should output something like:\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\n478afc919002: Pull complete\nDigest: sha256:91fb4b041da273d5a3273b6d587d62d518300a6ad268b28628f74997b93171b2\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (arm64v8)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://hub.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/get-started/\n\n\n\nIf you see these messages, then Podman/Docker are installed correctly and you can move on to the next step. If you see an error message, then you will need to have a look at the ‘dealing with errors’ section. Notice how there are several things happening:\n\nTrying to pull... or Pulling from library/hello-world – this is starting the process of downloading the hello-world image from the internet.\nPull complete or Writing manifest to image destination – this is telling you that it has finished downloading the image.\nHello from Docker! or Hello Podman World – this is the hello-world image running and telling you that the image now running as a container.\n\nThere’s obviously a lot more to that message, but that’s the basic idea.",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "setup/env.html#running",
    "href": "setup/env.html#running",
    "title": "Installation",
    "section": "Running",
    "text": "Running\nWe now need to download and run the sparc image that we created for the workshop. The image is the ‘template’ for running containers (i.e. virtual computers) on our ‘host’ computer and it comes complete with all of the Python libraries and other tools that you’ll need to complete the module (and a good deal more besides!). Installing the image will take a while as it’s quite large (1-5GB) and so will depend on the speed of your internet connection.\nYou can use a single command to download and start the sparc container by copying the following command into the Terminal or Windows Terminal (this may work in Power Shell, but not in the Command Prompt*/cmd):\n\nFor Intel MacsFor M-Chip MacsFor Windows\n\n\npodman run --rm -d --name sparc -p 8888:8888 \\\n   -v \"$(pwd):/home/jovyan/work\" \\\n  jreades/sparc:2025-amd start.sh jupyter lab \\\n  --LabApp.password='' --ServerApp.password='' --NotebookApp.token=''\n\n\n\n\n\n\nTip\n\n\n\nIf you were using Docker instead of Podman then the command is the same except that you need to replace podman with docker. This is on purpose: open source podman is intended to replace the proprietary docker.\n\n\n\n\npodman run --rm -d --name sparc -p 8888:8888 \\\n   -v \"$(pwd):/home/jovyan/work\" \\\n  jreades/sparc:2025-arm start.sh jupyter lab \\\n  --LabApp.password='' --ServerApp.password='' --NotebookApp.token=''\n\n\n\n\n\n\nTip\n\n\n\nIf you were using Docker instead of Podman then the command is the same except that you need to replace podman with docker. This is on purpose: open source podman is intended to replace the proprietary docker.\n\n\n\n\npodman run --rm -d --name sparc -p 8888:8888 -v \"$(pwd):/home/jovyan/work\" jreades/sparc:2025-amd start.sh jupyter lab --LabApp.password='' --ServerApp.password='' --NotebookApp.token=''\n\n\n\n\n\n\nTip\n\n\n\nIf you were using Docker instead of Podman then the command is the same except that you need to replace podman with docker. This is on purpose: open source podman is intended to replace the proprietary docker.",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "setup/env.html#if-things-go-wrong",
    "href": "setup/env.html#if-things-go-wrong",
    "title": "Installation",
    "section": "If Things Go Wrong",
    "text": "If Things Go Wrong\nIf you encounter any problems with the installation, please have a look at the Dealing with Problems page.",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "setup/env.html#success",
    "href": "setup/env.html#success",
    "title": "Installation",
    "section": "Success!",
    "text": "Success!\nHowever, most of you should now be able to connect to the virtual machine by pointing your browser at: localhost:8888 where you should see something like this:\n\n\n\nJupyter Lab Success",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "setup/env.html#footnotes",
    "href": "setup/env.html#footnotes",
    "title": "Installation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd Docker, which is basically the same thing.↩︎",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "setup/base.html",
    "href": "setup/base.html",
    "title": "Updates",
    "section": "",
    "text": "In order to get the machine ready to do virtualisation you need to install a few updates to the Operating System as well as some tools that help to interact with the programming platform. Things are a lot more complicated for Windows users here than for Mac (or Linux) users.",
    "crumbs": [
      "Updates"
    ]
  },
  {
    "objectID": "setup/base.html#system-updates",
    "href": "setup/base.html#system-updates",
    "title": "Updates",
    "section": "System Updates",
    "text": "System Updates\nBefore going any further, please ensure that your computer is fully up-to-date with all Operating System and application updates before following any of the other steps in this brief guide to getting started.",
    "crumbs": [
      "Updates"
    ]
  },
  {
    "objectID": "setup/base.html#base-utilities",
    "href": "setup/base.html#base-utilities",
    "title": "Updates",
    "section": "Base Utilities",
    "text": "Base Utilities\nIn order to access the majority of the features that this module uses, you will need to install several ‘base’ utilities:\n\nMacOSWindows\n\n\nYou need the Xcode Command Line Tools installed on your Mac. In some cases this may have already been installed, but it’s the same process to check as to install:\n\nOpen the Terminal application (which can be found under Utilities in your Applications folder).\nType the following: xcode-select --install and then hit Enter (⏎)\n\nUnless you get an outright error you can proceed to the next step:\n\nAlthough not strictly necessary, you’ll eventually want the Homebrew package manager, which can also be installed directly from the Terminal.\n\n\n\nYou will need to install WSL2 but, strangely, you do not actually need to install a full Linux O/S, so as far as we can tell this means you only need to run the following commands in the Windows PowerShell:\n\nwsl --install should quickly install the subsystem that we need.\nwsl --update will ensure that the most up-to-date version is available.\n\nThat should be it: Podman and Docker should now run without complaint.\n\n\n\n\n\n\nIf You Need More Help\n\n\n\n\nThere is good guidance from Microsoft for Windows 11 and more recent ‘builds’ of Windows 10.\nThere are also older instructions for Windows 10.\n\nCommon installation errors are covered here.\nFinally, if you are still being told that you can’t install WSL2 and Linux, then the most likely cause of this issue is a setting in the BIOS of the computer itself. There are a couple of settings that need changing at a very low level to enable Hyper-V virtualisation. I would start by following this MS guide and, if there is still a problem, check this blog post.",
    "crumbs": [
      "Updates"
    ]
  },
  {
    "objectID": "setup/base.html#windows-only",
    "href": "setup/base.html#windows-only",
    "title": "Updates",
    "section": "Windows Only",
    "text": "Windows Only\n\n\n\n\n\n\nCreating a Linux User\n\n\n\nIf you want to install a full Linux distribution (e.g. because you want to do more than just run Docker) then make sure you set up a new Linux user and do not end up running everything as root.\n\n\nTo make full use of WSL2 and Linux (if you want to do more than just use Docker) you will need to familiarise yourself with how having two operating systems that can talk to each other works. That is covered in the next section. This will actually be useful for understanding how Docker works, as it can be profoundly confusing.\nThe behaviour of the ‘shell’ (command line) is quite confusing because, with WSL2, you effectively end up with two ‘home’ directories: one for your Windows user, and one for your new Ubuntu user. Starting a Linux shell/command line puts you in your Linux home directory (the username may be completely different from your Windows username). Starting a Windows shell/command line puts you in your Windows home directory (again, the username may be completely different from your Linux username).\nSo the first, and perhaps most important, thing is understanding where ‘data’ is being stored:\n\nUnder Linux the user directory is apparently something like: \\\\wsl$\\&lt;DistroName&gt;\\home\\&lt;UserName&gt;\\ but you can usually get it by simply typing cd $HOME and then pwd when starting a new Linux shell.\nUnder Windows the user directory is: C:\\Users\\&lt;UserName&gt;\\ or /mnt/c/Users/&lt;UserName&gt;/, and you can often simply type pwd when opening a new Windows shell.\n\nSo these are different locations on your computer’s hard drive, and you will not see your Linux files in your Windows Home Directory and vice versa. To make it easy to switch between the two, I found this page on Microsoft’s web site that goes through some of the post-WSL2 installation steps, but see especially the additional page on Windows Terminal configuration.\nSlightly confusingly, you can run Linux commands directly from Windows, usually be adding wsl in front of the Linux command (e.g. wsl ls *.csv from Windows, where in Linux the command would be ls *.csv).\nTo make it easy to move from the Linux side of your machine to the Windows machine you can also do this:\n\nWork out where your CASA files are stored on the Windows side (see above: /mnt/c/Users/&lt;UserName&gt;/My\\ Documents/CASA/... where &lt;UserName&gt; is your Windows user name).\ncd to this location and type pwd to output the full path to the CASA directory.\nCopy this.\nNow start a Linux shell and run the following command once (you do not need to do it ever again): echo \"export WIN_HOME=\\\"/mnt/c/Users/.../CASA/\\\"\" &gt;&gt; $HOME/.bashrc. Replace the ... with the rest of the actual path! The \\\" is very important, don’t try to change those!\nNow run source .bashrc and you should see no errors.\n\nOnce you have done this you will have added a single line to your .bashrc file in Linux. This will create a ‘shortcut’ for bash in Linux. From here on out you should be able to type cd $WIN_HOME and move immediately over to the CASA directory on the Windows side. This will save having to remember (and type) the Windows path each time.",
    "crumbs": [
      "Updates"
    ]
  },
  {
    "objectID": "sessions/day3.html",
    "href": "sessions/day3.html",
    "title": "Day 3: Reproducible Analysis",
    "section": "",
    "text": "Goal\n\n\n\nHow tools like Quarto and Python can make report-writing ‘easy’.\n\n\nIntroductory session:\n\nWhat is reproducible analysis and why care about it?\nA brief introduction to version control and github.io (Git / GitHub)\nAn introduction to Quarto\n\nApplied session:\n\nLinking data\nCreating a report in Quarto starting from Report.qmd",
    "crumbs": [
      "Day 3: Reproducible Analysis"
    ]
  },
  {
    "objectID": "sessions/day1.html",
    "href": "sessions/day1.html",
    "title": "Day 1: Getting Started",
    "section": "",
    "text": "Goal\n\n\n\nA basic understanding of core concepts in reproducible analysis and the advantages of moving to open data and open source code.\n\n\n\nPoll: how many people have used GIS? If so, which one?\nPoll: how many people have learned a programming language? If so, which one?\n\nIntroductory session:\n\n1.1 Getting Started\n1.2 What Do (Spatial) Data Scientists Do\n1.3 Our Principles\n1.4 Our Tools\n\nApplied session:\n\nCheck your computer is up to date\nSetting up Podman\nRunning Podman\nIf you’ve never programmed before:\n\nKey Concepts (Ctrl+Click to download file)",
    "crumbs": [
      "Day 1: Getting Started"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Over the course of the week we will be exploring a little bit of what is possible using code (Python, in this case) to process and analyse spatial data. Along the way we’ll touch both on the practical benefits of coding (instead of using only point-and-click software like ArcGIS or QGIS) as well as the more subtle philosophical advantages of using open source software and code.\nEach day we will have about 90 minutes of lecture and discussion, then a short break, and then about 90 minutes of an ‘applied’ workshop using the techniques and principles discussed earlier. Because we only have a week, we will be focussing on understanding the why we do something in a particular way, and a general understanding of what is happening, rather than digging into how to code.\nThese days, AI can help you a lot with the basics of coding, but it is important to recognise that AI is only as good as the problems it was trained on, and you’ll likely find that it knows a lot less about Indian geo-data than it does British or American data. Treat the AI as a personal tutor that is endlessly patient and can offer you 1:1 time, but it cannot answer questions of ethics or purpose.\nNow,in order to get you started on your spatial and data science ‘journey’, you will need to follow the guidance provided on the pages we’ve linked to below to check that you’re able to run the programming environment and get everything (or as much as you can!) set up and ready to go on Day 1.."
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "Welcome",
    "section": "Requirements",
    "text": "Requirements\nBefore trying to install the software, please complete the basic health check to ensure that your computer is up-to-date and able to run the software we use."
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "Welcome",
    "section": "Updates",
    "text": "Updates\nOnce you know that your machine and operating system are up-to-date, you should install the basic utilities that will enable you to complete installation of the programming environment."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Welcome",
    "section": "Installation",
    "text": "Installation\nThe Installation Guide section will help you to get your computer ready for the workshop sessions."
  },
  {
    "objectID": "index.html#sessions",
    "href": "index.html#sessions",
    "title": "Welcome",
    "section": "Sessions",
    "text": "Sessions\nThere are four sessions planned for the week (see Sessions drop-down menu!). The sessions will make use of the tools installed above, so please try to install Podman on your laptop before the start of the first session as this will save quite a bit of time and also allow us to help you more quickly if there are any issues."
  },
  {
    "objectID": "index.html#citing",
    "href": "index.html#citing",
    "title": "Welcome",
    "section": "Citing",
    "text": "Citing\n@software{jaipur:2025,\n  author = {Reades, Jon},\n  title = {\\texttt{jaipur_env}: A containerised platform for Urban Spatial Science},\n  url = {https://github.com/jreades/jaipur/},\n  version = {2025},\n  date = {2025-08-11},\n}"
  },
  {
    "objectID": "sessions/day2.html",
    "href": "sessions/day2.html",
    "title": "Day 2: Exploring Code",
    "section": "",
    "text": "Goal\n\n\n\nLearning to manage and link data.\n\n\nIntroductory session:\n\nThink Maps\nThink Data\nReading Tables with Pandas\nReading Geographies with GeoPandas\n\nApplied session:\n\nPandas (Ctrl+Click to download file)\nGeopandas (Ctrl+Click to download file)",
    "crumbs": [
      "Day 2: Exploring Code"
    ]
  },
  {
    "objectID": "sessions/day4.html",
    "href": "sessions/day4.html",
    "title": "Day 4: Visualising Data",
    "section": "",
    "text": "Goal\n\n\n\nTo get you using the tools taught to create maps and plots using data.\n\n\nApplied session:\n\nApplying what has been learned over the preceding days.\nAdding new data to your map\nChanging colours, line widths, …, choropleths and maps-in-maps.\nWorking on a report — choosing a topic of interest (continuation of Day 3)",
    "crumbs": [
      "Day 4: Visualising Data"
    ]
  },
  {
    "objectID": "setup/code_camp.html",
    "href": "setup/code_camp.html",
    "title": "Code Camp",
    "section": "",
    "text": "Code Camp provides a gentle introduction to the basics of programming in Python. Across about twelve sessions you will learn about syntax, variables, ‘simple’ data structures such as lists and dictionaries, and about the fundamentals of writing functions for reusable code. The sessions are entirely self-led: you should follow along at your own pace and remember that ‘it’s a marathon, not a sprint’.\nWe’ve tried to keep Code Camp as simple as possible so that you can get started as quickly as possible. We recommend that you simply run Code Camp in your browser since that will allow you to learn (and run) Python anywhere and any time. However, you are welcome to run Code Camp code on your own computer (see: install options).",
    "crumbs": [
      "Code Camp"
    ]
  },
  {
    "objectID": "setup/code_camp.html#tldr",
    "href": "setup/code_camp.html#tldr",
    "title": "Code Camp",
    "section": "TL;DR",
    "text": "TL;DR\nJust point your browser to Lesson 1 and get started because there’s nothing to install.",
    "crumbs": [
      "Code Camp"
    ]
  },
  {
    "objectID": "setup/git.html",
    "href": "setup/git.html",
    "title": "Git & GitHub",
    "section": "",
    "text": "Git is a ‘version control system’, which is a fancy way of saying that it has the potential to store a complete, line-by-line history of your work… so long as it’s in a plain-text format like Markdown or Python/R code files. Git updates all of this history on your computer every time you ‘commit’ the changes you’ve told it about by ‘adding’ a file. With GitHub you can then synchronise these changes so that you have a full backup (code and content are no longer just on your computer), publish a web site (via github.io), or collaborate with other people (via public, shared ‘repositories’).\nWhile it is not necessary that you learn all about how GitHub works now, it will be really helpful if you can get yourself set up with a GitHub account and install Git locally so that you’re ready to get when we start the term."
  },
  {
    "objectID": "setup/git.html#get-a-github-account",
    "href": "setup/git.html#get-a-github-account",
    "title": "Git & GitHub",
    "section": "Get a GitHub Account",
    "text": "Get a GitHub Account\nYou will need to go to GitHub.com and then click the Sign up button at the top right. Follow the instructions for creating a new account from there.\n\n\n\n\n\n\nUse Your Personal Account\n\n\n\nAlthough you can always change it later (so if you’ve opened your GitHub account using your UCL email don’t panic!), you may find it easier to:\n\nOpen a GitHub account using a personal email address to which you expect to always have access.\nThen associate your UCL email address to this GitHub account so that you gain from any educational benefits offered by GitHub (there are some around private respositories and so on)."
  },
  {
    "objectID": "setup/git.html#install-git-locally",
    "href": "setup/git.html#install-git-locally",
    "title": "Git & GitHub",
    "section": "Install Git Locally",
    "text": "Install Git Locally\nYou can follow along with GitHub’s own instructions for installing Git on your own computer. In many cases (especially if you use a Mac) you may find that Git is already installed, you just didn’t know it."
  },
  {
    "objectID": "setup/markdown.html",
    "href": "setup/markdown.html",
    "title": "Markdown",
    "section": "",
    "text": "Markdown is a ‘markup language’ for documents that is compatible with a lot of different tools (including GitHub and Jupyter) that we use day-in and day-out for doing our research and teaching. Many of us have now largely stopped using Word (and even LaTeX) except for the final polishing of a document. Why? Because Markdown is faster, simpler, and just gets out of the way when we’re writing. So rather than fiddling about with Word’s styles (or, worse, discovering that you should have been using styles all along) or with LaTeX’s painful table layout, you can just get on with writing in Markdown and then export to Word or LaTeX for the final steps. Best of both worlds!\nIn fact, Markdown is so handy that it’s become the default for writing content for the web. This web site was actually written in Markdown and then ‘rendered’ (i.e. turned into a functional web page) using Quarto. We’ve included Quarto on the SDS2022 Docker image that we’ve recommended you use."
  },
  {
    "objectID": "setup/markdown.html#markdown-editors",
    "href": "setup/markdown.html#markdown-editors",
    "title": "Markdown",
    "section": "Markdown Editors",
    "text": "Markdown Editors\nOver time you will undoubtedly learn how to write markdwon without need to think much (if at all) about how to type the formatting ‘codes’, but a simple markdown editor can make your life much, much easier. Even when you’re highly experienced!\n\nFor MacOSFor WindowsFor Linux\n\n\nThere’s a good overview of ‘free’1 editors for the Mac which highlights a few in particular:\n\nMacDown – I’ve not used this, but it seems determinedly FOSS so is probably a good choice.\nHaroopad – this looks like a more powerful, but less immediatley user-friendly editor.\nAtom – I’ve used Atom for editing Python code but believe it’s largely plugin-based so it clearly supports markdown too.\n\nTypora was free while in beta, but I thought it was worth the modest amount of money asked for something that was super-fast and gave me a tool with which to write up my research, not just my code and teaching content. iA Writer is another good (paid for) option because it runs on iPad and iPhone as well! Together with the Working Copy Git client for iOS I’ve used iA Writer to draft articles, make notes directly in my codebase, and correct errors found at the last minute in my teaching materials. Like Typora, iA Writer is probably intended more for writing text, not writing complex Reveal.js presentations or non-standard markdown.\n\n\nI don’t have access to a Windows machine to test this out, but there are plenty of opinions to be found by Googling ‘best Windows Markdown editors’ or ‘best free Windows Markdown editors’. There’s one for writers and a more generic set of recommendations. Have a look around and see what you like!\n\n\nIf you use Linux already do you really need a recommendation? How about vim or vi?"
  },
  {
    "objectID": "setup/markdown.html#using-markdown",
    "href": "setup/markdown.html#using-markdown",
    "title": "Markdown",
    "section": "Using Markdown",
    "text": "Using Markdown\nFor Markdown to be useful as more than just a lightweight way to write notes, we want to be able to render it into new output formats/contexts. Here are three…"
  },
  {
    "objectID": "setup/markdown.html#markdown-github",
    "href": "setup/markdown.html#markdown-github",
    "title": "Markdown",
    "section": "Markdown & GitHub",
    "text": "Markdown & GitHub\nMarkdown is the ‘default’ language of GitHub, which means it’s worth your while to familiarise yourself with how it works. However, there are different ‘flavours’ of markdown, which also means that just because something works on GitHub it will work everywhere else in the same way. This is particularly common when dealing with optional parameters that try to give the ‘renderer’ (the thing that converts markdown to HTML, or LaTeX, or any other format) hints about how the content should look."
  },
  {
    "objectID": "setup/markdown.html#markdown-notebooks",
    "href": "setup/markdown.html#markdown-notebooks",
    "title": "Markdown",
    "section": "Markdown & Notebooks",
    "text": "Markdown & Notebooks\nMarkdown is also the language of plain-text cells in Jupyter notebooks, which makes it doubly worth your while to familiarise yourself with how it works."
  },
  {
    "objectID": "setup/markdown.html#quarto",
    "href": "setup/markdown.html#quarto",
    "title": "Markdown",
    "section": "Quarto",
    "text": "Quarto\nQuarto builds on RMarkdown to make the power of Markdown+Code available to other languages than R. In our Foundations of Spatial Data Science module we use Quarto to do submissions but that’s largely because it demonstrates how we can Quarto to write whole articles or dissertation!\nIn many cases, it is as straightforward as installing Quarto and then running quarto render ..., but there are some tricks. In particular, to ‘render’ Markdown files to PDF, you will need to have some flabour to TeX installed. The default suggested by Quarto is TinyTex, and this can be installed using:\nquarto install tool tinytex\nBut on some platforms there are additional issues:\n\nMacWindows\n\n\nYou will probably be able to successfully install tinytex, but then be told that no TeX installation could be found when trying to render. The issue relates to the $PATH where Quarto searches for a valid TeX installation and when completing the installation you may have seen a warning to the effect of “To complete the installation, please run the following…”\nSo that’s basically what we need to do. The first thing you need to do is find the TinyTeX binaries, in the cases that I’ve been able to fix these were found under something like $HOME/Library/TinyTex/bin/, but you may need to look further under $HOME/Library/ to find the TinyTex directory.\nOnce you know where TinyTeX is (adjust the cd command below as necessary), you can then follow the suggestion given:\ncd $HOME/Library/TinyTeX/bin/\n./universal-darwin/tlmgr option sys_bin $HOME/Library/TinyTeX/bin\n./universal-darwin/tlmgr path add\nThat should do it, but in case you are still getting errors, then the following might be necessary (again, adjust the path if necessary):\necho \"export PATH=\\\"/Users/$(whoami)/Library/TinyTeX/bin:\\$PATH\\\"\" &gt;&gt; $HOME/.zshrc\nThe command above uses whoami to set the username and assumes that’s how things should be set up, but don’t blindly copy+paste and assume this will work! You could, for instance, check this location exists first using: ls /Users/$(whoami)/Library/TinyTeX/bin/ That will show if the path exists! The above command then updates the $PATH variable used by your Terminal to look for binaries, enabling Quarto to find TinyTeX once you close and then re-open a new Shell.\n\n\nMost commands seem to require replacing quarto with quarto.exe to run correctly. So quarto render Template.qmd becaomes quarto.exe render Template.qmd.\nHowever, if you are still getting errors to the effect that Quarto cannot be found then you may need to make additional edites to your .bashrc file (this is the configuration file for bash):\n\nCheck where Quarto is installed, it is most likely under /mnt/c/Users/&lt;Your Username&gt;/AppData/Local/Programs/Quarto/bin (replace &lt;Your Username&gt; as appropriate.\nEdit the .bashrc file in your $HOME directory. On Windows I would Google search for the best way to do this: “edit .bashrc file Windows”.\nAdd the following line at the end of the file: export PATH=/mnt/c/Users/&lt;Your Username&gt;/AppData/Local/Programs/Quarto/bin:$PATH\nSave the file and run source .bashrc in the Terminal.\nYou should now be able to run the following command successfully: quarto.exe --help"
  },
  {
    "objectID": "setup/markdown.html#footnotes",
    "href": "setup/markdown.html#footnotes",
    "title": "Markdown",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNot all of these are still free.↩︎"
  },
  {
    "objectID": "setup/problems.html",
    "href": "setup/problems.html",
    "title": "Dealing with Problems",
    "section": "",
    "text": "There are two types of problems that you’re likely to encounter when trying to use Podman or Docker:\nThe vast majority of people encountering these issues will be on Windows machines."
  },
  {
    "objectID": "setup/problems.html#errors-installing",
    "href": "setup/problems.html#errors-installing",
    "title": "Dealing with Problems",
    "section": "Errors Installing",
    "text": "Errors Installing\n\nUnsupported Version\nThe only problem that is common to both Windows and macOS machines is the ‘unsupported version’ (or similar wording) error. This happens when you are running an older version of the Operating System that is no longer supported by the most recent installer.\nIf you are not able to update your computer’s operating system to a more recent release, then your only option is to use an older version of the application. You can find older versions of [Podman]() and Docker to install, though for very old machines this may still not resolve the ‘unsupported version’ problem.\n\n\nProblems with WSL2\nOn Windows, if you have problems that mention WSL/WSL2 (this could be errors installing or updating this system) and you are sure that you’ve installed and updated WSL, then you may need to try using ‘Hyper-V’ instead. This is a ‘second-best’ workaround that has worked for some people.\n\nEnabling Hyper-V\nMicrosoft has instructions here on how to enable Hyper-V on your Windows machine. You will need to restart your computer after enabling Hyper-V. These instructions may also help.\nYou will then need to look at the Podman or Docker settings and change the Use the WSL2 based engine to Use the Hyper-V based engine.\n\n\n\nProblems with Admin Rights\nIssues with administrator rights are the main reason that we are trying to swich from Docker to Podman. Podman does not require you to be an administrator of your machine. So if you are unable to install Docker on your computer (e.g. because you are using a secure corporate laptop), you may wish to try using Podman instead. Podman is runs in ‘user space’, which means that it does not need admin rights to run. You can find out more about Podman here. Our limited testing suggests that it works well with the sparc environment.\nYou will need to:\n\nDownload and install the Podman CLI and Desktop from here.\nRun the following command in the PowerShell or Terminal: podman machine initpodman machine start\nAnd then run this command in PowerShell or Terminal to download the sparc image with podman pull jreades/sparc:2025-amd (or jreades/sparc:2025-arm for M-chip Macs)"
  },
  {
    "objectID": "setup/problems.html#errors-running",
    "href": "setup/problems.html#errors-running",
    "title": "Dealing with Problems",
    "section": "Errors Running",
    "text": "Errors Running\nIf you have managed to install Docker (or Podman) and have managed to ‘pull’ a disk image then the last remaining obstacle to running Podman or Docker is, typically, running out of disk space.\n\nDealing with Disk Space\nSome Windows PCs (especially those sold with Windows10) have two drives (C and D). The C drive may fill up very quickly once you start installing and running software or processing data. In that case you may want to store your images on the D drive.\nThis is a complex process, and you may need assistance, so there is a Moving to the D Drive page to try to talk you through the process.\n\n\nOther Disk Space Issues\nIf you don’t have separate C and D drives but still have very little space left on your hard drive then it’s worth remembering that all of the images you’ve used and containers that you have run are saved on your computer. You can see how much space Docker (there’s a similar command for Podman) is using by running the following command in the PowerShell or Terminal:\ndocker system df\n\nDeleting an Image\nThe most ‘intuitive’ way is to delete each image by it’s unique ID:\ndocker ps -aq # Get list of running processes and work out container IDs to remove\ndocker rm -f &lt;list of container IDs&gt;\ndocker images # Get list of available images and work out image IDs to remove\ndocker rmi -f &lt;list of image IDs&gt;\n\n\nDeleting by Filter\nA more ‘direct’ approach that should be used with some care is to looking for the name of the image and delete that way:\ndocker ps -aqf \"name=sds\" --format=\"{{.Image}} {{.Names}} {{.ID}}\" | grep \"2025\" | cut -d' ' -f3 | xargs docker rm -f\ndocker images --format=\"{{.Repository}} {{.Tag}} {{.ID}}\" | grep \"sds\" | cut -d' ' -f3 | xargs docker rmi"
  },
  {
    "objectID": "setup/problems.html#the-last-resort",
    "href": "setup/problems.html#the-last-resort",
    "title": "Dealing with Problems",
    "section": "The Last Resort",
    "text": "The Last Resort\nA very small number of students are unable to run Docker or Podman at all on computers running Windows 10 Home (in 23/24 there were none), in which case Anaconda Python can be used with a configuration based on the Podman build file. We don’t recommend this as Podman and Docker isolate the programming environment from your computer, ensuring that nothing is clobbered by accident, and guaranteeing that you are working with the same version of every Python library as the rest of the class (and the versions for which the practicals are tested).\nAnaconda is only supported as a last resort.\nHowever you are always free to install Anaconda Python and to use our YAML configuration script to install the sparc environment, but you should do this in your own time: in previous years students have encountered difficult-to-diagnose bugs in their code (and lost marks in the Assessments!) because they had installed an older or more recent version of a Python library than the one configured and tested in the sparc environment.\nWe believe that the replication advantages of virtualisation outweigh the disbenefits in terms of performance. It also means that you will spend less time installing libraries and more time running code, which is where your attention should really be when you are familiarising yourself with the foundations of data science.\nEventually you will, of course, want to install and manage your own programming environment (possibly even by building and sharing Podman/Docker images!) but this can be left for later when you have developed an appreciation of how and when virtualisation is (or is not) an appropriate solution to your needs."
  },
  {
    "objectID": "lectures/1.1-the_week.html#useful-information",
    "href": "lectures/1.1-the_week.html#useful-information",
    "title": "The Week Ahead",
    "section": "Useful Information",
    "text": "Useful Information\nAll of the content will remain available to you after our workshop:\n\nThe main micro-site: jreades.github.io/jaipur/ – talks, applied sessions, and some suggested readings.\nA quick introduction to Python: jreades.github.io/code-camp/ – 12 self-paced sessions focussing on the basics of Python.\n\nThere are plenty of other resources online (Khan Academy, MIT Open Courseware, etc, etc) if you want to go further. And, of course, our Urban Spatial Science MSc and Connected Environments MSc."
  },
  {
    "objectID": "lectures/1.1-the_week.html#what-are-we-trying-to-do",
    "href": "lectures/1.1-the_week.html#what-are-we-trying-to-do",
    "title": "The Week Ahead",
    "section": "What Are We Trying to Do?",
    "text": "What Are We Trying to Do?\nThis workshop hopes to achieve four things:\n\nTo expose you to the power of spatial analysis in Python.\nTo expose you to a set of tools that support reproducible analysis.\nTo (hopefully) convince you that going further with this is worth your time.\nTo signpost resources that will support further learning.\n\nThis knowledge is intended to be transferrable."
  },
  {
    "objectID": "lectures/1.1-the_week.html#overall-structure",
    "href": "lectures/1.1-the_week.html#overall-structure",
    "title": "The Week Ahead",
    "section": "Overall Structure",
    "text": "Overall Structure\n\nDay 1: Getting Started: introducing open source, code and data.\nDay 2: Preparing: setting up, managing, and linking data.\nDay 3: Reproducing: making outputs and analysis reproducible.\nDay 4: Reinforcing: making maps and visualisations."
  },
  {
    "objectID": "lectures/1.1-the_week.html#week-to-week",
    "href": "lectures/1.1-the_week.html#week-to-week",
    "title": "The Week Ahead",
    "section": "Week-to-Week",
    "text": "Week-to-Week\nThe specific activities for each week can be found on the microsite: jreades.github.io/fsds/. These include:\n\nPreparation: readings, pre-recorded lectures, quizzes/feedback.\nIn-Person: discussing readings and lectures; responding to assessment requirements; discussing issues arising from the previous week’s practical, and some ‘live coding’.\nPracticals: working through a weekly ‘programming notebook’ with support from your PGTAs.\n\n\n\n\n\n\n\nBring Your Computer\n\n\nPlease remember to bring your own computer to the practical sessions! The tools we use are not installed on cluster systems."
  },
  {
    "objectID": "lectures/1.1-the_week.html#assessments",
    "href": "lectures/1.1-the_week.html#assessments",
    "title": "The Week Ahead",
    "section": "Assessments",
    "text": "Assessments\n\nTimed, Open Book Exam (?var:assess.quiz-weight of module grade): A quiz requiring a mix of numeric and textual answers to short data analysis questions for which you must write the code.\nGroup Report (?var:assess.group-weight of module grade; 2,500 words max): A structured, small-group submission in which students respond to set questions and develop an exploratory analysis of the assigned data set.\nSelf-Evaluation (?var:assess.peer-weight of module grade): A structured individual reflection combined with numerical scoring of peers on their contribution to the group’s outcomes.\n\n\nAssessment logic:\n\nTeach and test the most challenging aspects of data science ‘work’ without mastery of Python.\nDiscover transferrability of skills and tools across projects, disciplines, and industries.\nBuild on content from QM (e.g. setting quantitative research questions) and GIS (e.g. spatial statistics).\nDevelop experience with non-academic research formats and writing.\n\nSo,\n\nIs a Moodule quiz due ?var:assess.quiz-date (after Reading Week) and it will focus on the effective use of the pandas library.\nIs a Quarto document due ?var:assess.group-date (immediately after the end of term) that combines the analysis and outputs in one document with a set of specified questions upon which randomly-selected groups will receive feedback throughout the term.\nIs a reflection and ranking exercise due ?var:assess.peer-date (the day after the Quarto submission).\n\nWe will talk more about these over the course of the term."
  },
  {
    "objectID": "lectures/1.1-the_week.html#consequences",
    "href": "lectures/1.1-the_week.html#consequences",
    "title": "The Week Ahead",
    "section": "Consequences…",
    "text": "Consequences…\nSo…\n\nIf you only code during the practical session then you will not learn how to code.\nIf you cram the night before then you will not learn how to code.\nIf you practice for 45 minutes a day then you will learn how to code.\n\nDon’t take my word for it, @prat:2020 in Nature link language learning to programming language learning!\n\nThis said, we do hope to convince you that:\n\nAnyone—and this includes you—can code.\nLearning to code does not require mathematical ability.\nLearning to code does not require linguistic ability.\nLearning to code does require practice. And more practice. And more again."
  },
  {
    "objectID": "lectures/1.1-the_week.html#actual-feedback",
    "href": "lectures/1.1-the_week.html#actual-feedback",
    "title": "The Week Ahead",
    "section": "Actual Feedback…",
    "text": "Actual Feedback…\n\nI was really struggling with the concepts of lists, dictionaries and iterations (I basically could not do any of Practical 3 without panicking) and I was telling  that it felt like Workshop 3 was all in a foreign language - I was so lost. \n But both yesterday and today, I have been over all the content, recordings and even code camp again and I’ve just had a penny drop moment, I could cry woohooo!!!!!! \nI really appreciate all the effort you’ve put into recording the concepts ahead of lectures and the way you’ve structured the module, although it is very fast-moving you have provided all the resources for us to do well."
  },
  {
    "objectID": "lectures/1.1-the_week.html#more-feedback",
    "href": "lectures/1.1-the_week.html#more-feedback",
    "title": "The Week Ahead",
    "section": "More Feedback",
    "text": "More Feedback\n\nI just wanted to update you on my progress. Since flipping the content round following your advice, I have been feeling much much better. I followed what you were doing in the workshop and also have completed the practical in about half the time than I usually do. Thanks so much for responding and for your effort with this module."
  },
  {
    "objectID": "lectures/1.1-the_week.html#the-old-challenges",
    "href": "lectures/1.1-the_week.html#the-old-challenges",
    "title": "The Week Ahead",
    "section": "The (Old) Challenges",
    "text": "The (Old) Challenges\n\nDifferent style of learning from what you might be used to (“I didn’t anticipate, or rather factor into my schedule, the amount of out-of-hours practice that was required to stay up to date.”).\nDoing stats and programming at the same time and connecting this all back to the bigger picture.\nDelayed gratification (you have to walk before you can run).\nEasy to fall behind, but hard to catch up (“the pace is relentless”)."
  },
  {
    "objectID": "lectures/1.1-the_week.html#the-new-challenges",
    "href": "lectures/1.1-the_week.html#the-new-challenges",
    "title": "The Week Ahead",
    "section": "The (New) Challenges",
    "text": "The (New) Challenges\n\nChatGPT (you’re going to learn when not to trust it)\nClose reading 101 (you’re going to be asked to really read)\nPublic speaking (yes, you’re going to have do a bit of this too)\n\n\nThis is a new one for us too. We don’t want to pretend that ChatGPT doesn’t exist. It’s how you will do your work. Unquestionably. But it is also a trap. This year we’re hoping to show you that."
  },
  {
    "objectID": "lectures/1.1-the_week.html#the-rewards",
    "href": "lectures/1.1-the_week.html#the-rewards",
    "title": "The Week Ahead",
    "section": "The Rewards",
    "text": "The Rewards\n\nSkills that are highly transferrable and highly sought-after professionally.\nProblem-solving and practical skills that are valued by the private and public sectors.\nA whole new way of seeing the world and interacting with it.\nLots of support along the way… if you remember to ask for it!"
  },
  {
    "objectID": "lectures/1.1-the_week.html#lots-of-help-out-there",
    "href": "lectures/1.1-the_week.html#lots-of-help-out-there",
    "title": "The Week Ahead",
    "section": "Lots of Help ‘Out There’",
    "text": "Lots of Help ‘Out There’\nYou will learn cope best if you treat this like learning a new language:\n\nStart simple and work up.\nGoogle is your friend (really).\nTalk with your friends (i.e. in-person or on Slack).\nImmerse yourself and practice regularly.\nPrint out the readings and annotate them.\nSubscribe to a ‘magazine’ or two (e.g. Medium or Pocket).\nLearn how to ask questions (i.e. Search Stack Overflow)."
  },
  {
    "objectID": "lectures/1.1-the_week.html#study-aids",
    "href": "lectures/1.1-the_week.html#study-aids",
    "title": "The Week Ahead",
    "section": "Study Aids",
    "text": "Study Aids\nWhen you need an answer right now:\n\nGoogle\nStack Overflow\nSlack\n\nWhen you want to learn more:\n\nMedium\nPocket\n\n\nGoogle will become more useful as you learn more and this is definitely one class in which “I Googled it” is a good answer.\nAs of early September 2020, Stack Overflow contains over 1.5 million Python questions alone! Chances are someone else has had your question before.\nWhere’s ChatGPT in this? We’ll get to that!"
  },
  {
    "objectID": "lectures/1.1-the_week.html#before-you-ask-for-help",
    "href": "lectures/1.1-the_week.html#before-you-ask-for-help",
    "title": "The Week Ahead",
    "section": "Before You Ask for Help",
    "text": "Before You Ask for Help\nFrom the Computer Science Wiki:\n\nDraw a picture of the problem\nExplain the problem to a toy or inanimate object (really!)\nForget about a computer; how would you solve this with a pencil and paper?\nTalk it through out loud\nExplain the problem to a friend\n\nTo which we would add:\n\nUse print(variable) statements liberally in your code!\n\n\nWe’ll cover this last bit as we get more used to coding!"
  },
  {
    "objectID": "lectures/1.1-the_week.html#where-to-ask-for-help",
    "href": "lectures/1.1-the_week.html#where-to-ask-for-help",
    "title": "The Week Ahead",
    "section": "Where to Ask for Help",
    "text": "Where to Ask for Help\nThere is no shame in asking for help. None. We are here to support your learning and we have chosen a range of tools to support that:\n\nSlack: use public #fsds channel for help with coding, practical, and related course questions.\nDrop-in Hours: use Booking Form\nOut-of-Hours: use email to raise personal circumstances and related issues for focussed support.\nEmergencies: contact Professional Services for support as-needed to preserve privacy.\n\n\nWe’ll talk about Slack more later, but we think that this is the best way to get help when you need it. Slack enables us to support you as a community of learners across computer / tablet / phone.\nI’ve tried to throw together some ideas on how you can study effectively that covers things relating to managing distractions when you’ve only got limited time, as well as how to read and how to think."
  },
  {
    "objectID": "lectures/1.1-the_week.html#when-to-ask-for-help",
    "href": "lectures/1.1-the_week.html#when-to-ask-for-help",
    "title": "The Week Ahead",
    "section": "When to Ask for Help",
    "text": "When to Ask for Help\n\nWhen you get warning messages from your computer’s Operating System.\nWhen you cannot get the coding environment to run at all.\nWhen even simple commands return line after line of error code.\nWhen you have no clue what is going on or why.\nWhen you have been wrestling with a coding question for more than 20 minutes (but see: How to Ask for Help!)\n\n\nIn order to learn you do need to struggle, but only up to a point! So we don’t think that giving you the answer to a coding question as soon as you get stuck is a good way for you to learn. At the same time, I remain sad to this day that one of the most insightful students I’ve ever taught in a lecture context dropped out of our module because they were having trouble with their computer and thought it was their fault nothing was working right. By we had realised what was going on it was too late: they were so far behind that they didn’t feel able to catch up. We’d rather that you asked and we said “Close, but try it again” than you didn’t ask and checked out thinking that you couldn’t ‘do’ programming."
  },
  {
    "objectID": "lectures/1.1-the_week.html#how-to-ask-for-help",
    "href": "lectures/1.1-the_week.html#how-to-ask-for-help",
    "title": "The Week Ahead",
    "section": "How to Ask for Help",
    "text": "How to Ask for Help\nIn addition to what we have provided, we like the “How to ask programming questions” page provided by ProPublica:\n\nDo some research first.\nBe specific.\nRepeat.\nDocument and share.\n\nIf you find yourself wanting to ask a question on Stack Exchange then they also have a guide, and there are plenty of checklists.\n\nThere’s also useful ideas on how to get help that covers things like ‘how to get a reply from your Prof’ and ‘where to look for help’."
  },
  {
    "objectID": "lectures/1.1-the_week.html#learn-from-your-mistakes",
    "href": "lectures/1.1-the_week.html#learn-from-your-mistakes",
    "title": "The Week Ahead",
    "section": "Learn from Your Mistakes",
    "text": "Learn from Your Mistakes"
  },
  {
    "objectID": "lectures/1.1-the_week.html#one-more-thing",
    "href": "lectures/1.1-the_week.html#one-more-thing",
    "title": "The Week Ahead",
    "section": "One More Thing…",
    "text": "One More Thing…\n\n\nYou will get things wrong.\nWe will get things wrong.\nWe will assume that you are trying your best.\nPlease assume the same about us!\nIt’s going to be messy, but we’re really excited about it!"
  },
  {
    "objectID": "lectures/1.1-the_week.html#and-finally",
    "href": "lectures/1.1-the_week.html#and-finally",
    "title": "The Week Ahead",
    "section": "And Finally…",
    "text": "And Finally…\n\n\n\n\n\n\nAuto-Updates\n\n\nDo not allow your computer to auto-update during term. Inevitably, major upgrades will break developer tools. Do this by choice only when you have time. MacOS Sonoma is out 26 September, do not install it!\n\n\n\n\nMany students allowed their computer to update to Big Sur last year and it broke their entire computing environment. Some did this shortly before a submission was due. Do not do this!"
  },
  {
    "objectID": "lectures/1.1-the_week.html#additional-resources",
    "href": "lectures/1.1-the_week.html#additional-resources",
    "title": "The Week Ahead",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nSee the GIS&T Body of Knowledge (BoK) for quick overview of concepts, techniques, and tools: gistbok.ucgis.org.\nA degree of ‘plagiarism’ is acceptable in code since that’s how we learn; however, mindless copy+pasting of Stack Overflow code leads to sphagetti and, often, incorrect results or difficult-to-squash bugs. Think of it like paraphrasing.\nTo distinguish between plagiarism and paraphrasing here’s a nice tutorial that you can also use to help you with your ‘regular’ writing."
  },
  {
    "objectID": "lectures/1.1-the_week.html#principles",
    "href": "lectures/1.1-the_week.html#principles",
    "title": "The Week Ahead",
    "section": "Principles",
    "text": "Principles\n\nSoftware should be free (as far as practicable).\nSoftware should be open (as far as practicable).\nSoftware should run on all platforms (you get the idea).\nSoftware should reflect what you will encounter in the ‘real world’."
  },
  {
    "objectID": "lectures/1.1-the_week.html#tools-to-make-your-life-easier",
    "href": "lectures/1.1-the_week.html#tools-to-make-your-life-easier",
    "title": "The Week Ahead",
    "section": "Tools to Make Your Life Easier",
    "text": "Tools to Make Your Life Easier\n\nOneDrive (or Dropbox): keep your stuff backed up in the cloud.\nSlack: get help (or just tips and tricks) from peers and staff\nDocker: virtualisation platforms to ensure you don’t ‘hose’ your computer.\nPython: how we do ‘data science’.\nGitHub: manage your code, your data, even your essays/reports.\nMarkdown: focus on the right things while you write and treat your essays like code!\nQuarto: convert Markdown+Python to pretty documents/web pages.\nLLMs: assistance in sketching out code snippets/validation."
  },
  {
    "objectID": "lectures/1.1-the_week.html#onedrivedropbox",
    "href": "lectures/1.1-the_week.html#onedrivedropbox",
    "title": "The Week Ahead",
    "section": "OneDrive/Dropbox",
    "text": "OneDrive/Dropbox\n\n\n\n\n\n\nOneDrive and Dropbox are ‘cloud-based file synchronisation tools’: files placed in the special folder are automatically uploaded to servers, and automatically downloaded to any other computer on which you have set up the service. Changes are also synchronised ‘every time’ you save the file."
  },
  {
    "objectID": "lectures/1.1-the_week.html#why-use-it",
    "href": "lectures/1.1-the_week.html#why-use-it",
    "title": "The Week Ahead",
    "section": "Why Use It?",
    "text": "Why Use It?\nMany programmers are starting to use LLMs as part of their coding for three reasons:\n\nThey can help to spot bugs, redundancy, and other issues that impact the performance of large applications (i.e. feedback).\nThey can provide information about different libraries and strategies the developer can use, as well as completing code begun by the developer (i.e. guidance or training).\nThey can help to ‘translate’ code and design patterns between languages (i.e. re-use).\n\nThis is very much a ‘brave new world’ and we are all trying to figure it out on the fly."
  },
  {
    "objectID": "lectures/1.1-the_week.html#we-recommend",
    "href": "lectures/1.1-the_week.html#we-recommend",
    "title": "The Week Ahead",
    "section": "We Recommend…",
    "text": "We Recommend…\nLLMs like ChatGPT can help you to learn to be a better coder by providing guidance and feedback, but for many applications a competent human being will be faster and have a better grasp of the purpose of the code.\n\n\n\n\n\n\nLLMs as co-authors\n\n\nUsing ChatGPT as your co-pilot is not the same as using ChatGPT as your co-author."
  },
  {
    "objectID": "lectures/1.1-the_week.html#slack",
    "href": "lectures/1.1-the_week.html#slack",
    "title": "The Week Ahead",
    "section": "Slack",
    "text": "Slack\n\nSlack is a “messaging app for teams” that is designed to reduce email, organise conversations & topics of discussion, and pull in relevant data from a host of other services in a flexible, fully-searchable way."
  },
  {
    "objectID": "lectures/1.1-the_week.html#why-use-it-1",
    "href": "lectures/1.1-the_week.html#why-use-it-1",
    "title": "The Week Ahead",
    "section": "Why Use It?",
    "text": "Why Use It?\nWe want you to use Slack for four reasons:\n\nMoodle is clunky and formal—it works well for one-to-many communication, but not so much for ‘chat’.\nSlack offers a searchable history1—you will have access to this archive for as long as you need it.\nYou (and we) can access Slack on every major OS (OSX, Windows, iOS, Android, and Windows Phone) and via a browser quickly.\nSlack is used in the ‘real world’ by everyone from Apple to PayPal and the JPL. This is how developers work.\n\nUp to a point, we don’t pay for the permanent history."
  },
  {
    "objectID": "lectures/1.1-the_week.html#we-recommend-1",
    "href": "lectures/1.1-the_week.html#we-recommend-1",
    "title": "The Week Ahead",
    "section": "We Recommend…",
    "text": "We Recommend…\nInstall the Slack client on your phone and on your personal computer and start using it as the way to ask questions, share answers, and generally keep ‘up to date’ on things across the entire MSc.\nWorkspace: ?var:module.slack\nP.S. Unless a question is personal it should normally be asked in the appropriate module channel."
  },
  {
    "objectID": "lectures/1.1-the_week.html#docker",
    "href": "lectures/1.1-the_week.html#docker",
    "title": "The Week Ahead",
    "section": "Docker",
    "text": "Docker\n\nDocker “makes development efficient and predictable” because it is “used through the development lifecycle for fast, easy and portable application development”."
  },
  {
    "objectID": "lectures/1.1-the_week.html#why-use-it-2",
    "href": "lectures/1.1-the_week.html#why-use-it-2",
    "title": "The Week Ahead",
    "section": "Why Use It?",
    "text": "Why Use It?\nDocker is a ‘virtualisation platform’ that allows you to run a second (virtual) computer on your personal computer. We use it for four reasons:\n\nEasier installation than Anaconda Python and everyone has the same versions of every library.\nNo spillover effects since each container is isolated.\nEasy to tidy up when you’re done or add new containers when you start something new (e.g. PostgreSQL).\nUsed in the ‘real world’ by many companies (JP Morgan Chase, GSK, PayPal, Twitter, Spotify, Uber…)."
  },
  {
    "objectID": "lectures/1.1-the_week.html#we-recommend-2",
    "href": "lectures/1.1-the_week.html#we-recommend-2",
    "title": "The Week Ahead",
    "section": "We Recommend…",
    "text": "We Recommend…\nUsing Docker because configuring a development machine is hard, this makes it simple. If a Docker image works for us then we know 1 it works for you.\nDocker Desktop with either:\n\njreades/sparc:2025-amd (Windows and Older Macs)\njreades/sparc:2025-arm (Newer Macs)\n\n\nNot always true, alas."
  },
  {
    "objectID": "lectures/1.1-the_week.html#large-language-models-llms",
    "href": "lectures/1.1-the_week.html#large-language-models-llms",
    "title": "The Week Ahead",
    "section": "Large Language Models (LLMs)",
    "text": "Large Language Models (LLMs)\n\nChatGPT from OpenAI (an increasingly ‘ironic’ name) is simply the most famous of a growing number of Large Language Models that draw on information found on the web and in open texts to perform sophisticated summarisation tasks."
  },
  {
    "objectID": "lectures/1.1-the_week.html#why-use-it-3",
    "href": "lectures/1.1-the_week.html#why-use-it-3",
    "title": "The Week Ahead",
    "section": "Why Use It?",
    "text": "Why Use It?\nMany programmers are starting to use LLMs as part of their coding for three reasons:\n\nThey can help to spot bugs, redundancy, and other issues that impact the performance of large applications (i.e. feedback).\nThey can provide information about different libraries and strategies the developer can use, as well as completing code begun by the developer (i.e. guidance or training).\nThey can help to ‘translate’ code and design patterns between languages (i.e. re-use).\n\nThis is very much a ‘brave new world’ and we are all trying to figure it out on the fly."
  },
  {
    "objectID": "lectures/1.1-the_week.html#danger-will-robinson",
    "href": "lectures/1.1-the_week.html#danger-will-robinson",
    "title": "The Week Ahead",
    "section": "Danger, Will Robinson!",
    "text": "Danger, Will Robinson!\nHere’s what we’ve noticed about ChatGPT’s output so far:\n\nOver-use of flowery language (e.g. ‘intricate methodologies’, ‘exhaustive reviews’, …).\nLack of overall structure/coherence (i.e. individual paragraphs seem reasonable but the whole doesn’t ‘work’ as a single output).\nPoor referencing, including the use of non-existent or irrelevant references (i.e. papers that don’t actually exist, or that seem superficially useful but are not relevant in practice).\nThe more specific the information, the less likely it is to be correct (e.g. Who is Jon Reades? vs. What are the main areas where NLP is useful in built environment research?)"
  },
  {
    "objectID": "lectures/1.1-the_week.html#over-confidence-in-action",
    "href": "lectures/1.1-the_week.html#over-confidence-in-action",
    "title": "The Week Ahead",
    "section": "Over-Confidence in Action",
    "text": "Over-Confidence in Action\nIn fact, the more specific the information, the less likely it is to be correct: who is Jon Reades?1\n\nJon Reades is a Senior Lecturer in the Department of Geography at University College London (UCL). He is also the director of the MSc in Smart Cities and Urban Analytics program at UCL.\nReades’ research focuses on the intersection of cities, data, and technology, with a particular emphasis on the use of spatial data and analytics to understand urban processes and support decision-making in cities. He has published widely on topics such as spatial analysis, urban data science, and the use of smart technologies in urban planning and governance.\nIn addition to his academic work, Reades is actively involved in a number of professional organizations and initiatives related to urban analytics and smart cities. He is a co-founder of the Urban Dynamics Institute at UCL and has served on the editorial board of several academic journals in the fields of geography and urban studies.\n\nAnd, by the way, there’s only one of me!"
  },
  {
    "objectID": "lectures/1.1-the_week.html#read-the-fine-print",
    "href": "lectures/1.1-the_week.html#read-the-fine-print",
    "title": "The Week Ahead",
    "section": "Read the Fine Print",
    "text": "Read the Fine Print"
  },
  {
    "objectID": "lectures/1.1-the_week.html#we-recommend-3",
    "href": "lectures/1.1-the_week.html#we-recommend-3",
    "title": "The Week Ahead",
    "section": "We Recommend…",
    "text": "We Recommend…\nLLMs like ChatGPT can help you to learn to be a better coder by providing guidance and feedback, but for many applications a competent human being will be faster and have a better grasp of the purpose of the code.\n\n\n\n\n\n\nLLMs as co-authors\n\n\nUsing ChatGPT as your co-pilot is not the same as using ChatGPT as your co-author. In this module the latter is still considered plagiarism.\n\n\n\nThe people making the best use of LLMs are people who already know how to code or write."
  },
  {
    "objectID": "lectures/1.1-the_week.html#recap",
    "href": "lectures/1.1-the_week.html#recap",
    "title": "The Week Ahead",
    "section": "Recap",
    "text": "Recap\n\nWith Docker we have a way to create a coding environment that is isolated from the computer and highly portable across machines.\nWith OneDrive and/or Dropbox we have a place to store, backup, and share files (size limits apply).\nWith Slack we have a place to ask for/provide help.\nWith LLMs we have a personal ‘tutor’ who can help us to learn more quickly and effectively.\n\nLet’s turn to the rest in part 2: Writing Code!"
  },
  {
    "objectID": "lectures/1.1-the_week.html#resources",
    "href": "lectures/1.1-the_week.html#resources",
    "title": "The Week Ahead",
    "section": "Resources",
    "text": "Resources\n\nWhat is Python?\nWhy Python?\nProgramming Foundations: Fundamentals\nPython is eating the world\nWhat can you do with Python?\nProgram-Aided Language Models\nChain of Thought Prompting\nChatGPT is a blurry JPEG of the Internet 1\nWhy Meta’s latest large language model survived only three days online 2\nGit for Decolonisation3\n\n\n\n\n\nPrat, Chantel S, Tara M Madhyastha, Malayka J Mottarella, and Chu-Hsuan Kuo. 2020. “Relating Natural Language Aptitude to Individual Differences in Learning Programming Languages.” Scientific Reports 10 (1). Nature Publishing Group UK London:3817.\n\n\nProbably the best ‘lay person’s’ explanation of how LLMs work/fall apart you’ll ever read.And this one was trained on scientific articles!Part art, part activism, part tech!"
  },
  {
    "objectID": "lectures/1.1-the_week.html#study-like-youre-learning-a-new-language.-do-the-readings.-talk-to-other-students-especially-in-your-group.-ask-for-help-when-you-need-it",
    "href": "lectures/1.1-the_week.html#study-like-youre-learning-a-new-language.-do-the-readings.-talk-to-other-students-especially-in-your-group.-ask-for-help-when-you-need-it",
    "title": "The Week Ahead",
    "section": "> Study like you’re learning a new language. Do the readings. Talk to other students (especially in your group). Ask for help when you need it!1",
    "text": "&gt; Study like you’re learning a new language. Do the readings. Talk to other students (especially in your group). Ask for help when you need it!1\n\nSo…\n\nIf you only code during the practical session then you will not learn how to code.\nIf you cram the night before then you will not learn how to code.\nIf you practice for 45 minutes a day then you will learn how to code.\n\nThis said, we do hope to convince you that:\n\nAnyone—and this includes you—can code.\nLearning to code does not require mathematical ability.\nLearning to code does not require linguistic ability.\nLearning to code does require practice. And more practice. And more again.\n\n\nDon’t take my word for it, Prat et al. (2020) in Nature link language learning to programming language learning!"
  },
  {
    "objectID": "lectures/1.1-the_week.html#study-like-youre-learning-a-new-language.-recognise-you-are-all-starting-at-different-levels.-talk-to-other-students.-ask-for-help-when-you-need-it",
    "href": "lectures/1.1-the_week.html#study-like-youre-learning-a-new-language.-recognise-you-are-all-starting-at-different-levels.-talk-to-other-students.-ask-for-help-when-you-need-it",
    "title": "The Week Ahead",
    "section": "Study like you’re learning a new language. Recognise you are all starting at different levels. Talk to other students. Ask for help when you need it!1",
    "text": "Study like you’re learning a new language. Recognise you are all starting at different levels. Talk to other students. Ask for help when you need it!1\n\nSo… we do hope to convince you that:\n\nAnyone—and this includes you—can code.\nLearning to code does not require mathematical ability.\nLearning to code does not require linguistic ability.\nLearning to code does require practice. And more practice. And more again.\n\n\nDon’t take my word for it, Prat et al. (2020) in Nature link language learning to programming language learning!"
  },
  {
    "objectID": "lectures/1.1-the_week.html#the-challenges",
    "href": "lectures/1.1-the_week.html#the-challenges",
    "title": "The Week Ahead",
    "section": "The Challenges",
    "text": "The Challenges\n\nTo learn a bit of programming and to connect it to the bigger picture.\nTo be ok with learning to walk before you run.\nTo learn not to rely (too much) on ChatGPT.\nTo communicate your thoughts through code and text.\n\n\nThis is a new one for us too. We don’t want to pretend that ChatGPT doesn’t exist. It’s how you will do your work. Unquestionably. But it is also a trap. This year we’re hoping to show you that."
  },
  {
    "objectID": "lectures/1.1-the_week.html#when-to-use-ai",
    "href": "lectures/1.1-the_week.html#when-to-use-ai",
    "title": "The Week Ahead",
    "section": "When to use AI",
    "text": "When to use AI\n\n\nDo not use AI like the font of all knowledge.\nUse AI like a patient, but scatty mentor.\nAsk it to explain. Then ask it again."
  },
  {
    "objectID": "lectures/1.1-the_week.html#references",
    "href": "lectures/1.1-the_week.html#references",
    "title": "The Week Ahead",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\nChiang, T. 2023. “ChatGPT Is a Blurry JPEG of the Web.” The New Yorker. https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web.\n\n\nHeaven, W. D. 2022. “Why Meta’s Latest Large Language Model Survived Only Three Days Online.” MIT Technology Review. https://www.technologyreview.com/2022/11/18/1063487/meta-large-language-model-ai-only-survived-three-days-gpt-3-science/.\n\n\nPrat, Chantel S, Tara M Madhyastha, Malayka J Mottarella, and Chu-Hsuan Kuo. 2020. “Relating Natural Language Aptitude to Individual Differences in Learning Programming Languages.” Scientific Reports 10 (1). Nature Publishing Group UK London:3817. https://www.nature.com/articles/s41598-020-60661-8.\n\n\nWolfe, C. R. 2023b. “Program-Aided Language Models.” https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms.\n\n\n———. 2023a. “Program-Aided Language Models.” https://medium.com/data-science/program-aided-language-models-93d226c7d9a0."
  },
  {
    "objectID": "lectures/1.2-principles.html#principles",
    "href": "lectures/1.2-principles.html#principles",
    "title": "Our Principles",
    "section": "Principles",
    "text": "Principles\n\nSoftware should be free (as far as practicable).\nSoftware should be open (as far as practicable).\nSoftware should run on all platforms (you get the idea).\nSoftware should reflect what you will encounter in the ‘real world’."
  },
  {
    "objectID": "lectures/1.2-principles.html#tools-to-make-your-life-easier",
    "href": "lectures/1.2-principles.html#tools-to-make-your-life-easier",
    "title": "Opening Up",
    "section": "Tools to Make Your Life Easier",
    "text": "Tools to Make Your Life Easier\n\nPodman: separate your computer from your coding environment.\nPython: how we do ‘data science’.\nGitHub: manage your code, your data, even your reports.\nMarkdown: focus on the right things while you write and treat your essays like code!\nQuarto: convert Markdown+Python to pretty documents/web pages."
  },
  {
    "objectID": "lectures/1.2-principles.html#onedrivedropbox",
    "href": "lectures/1.2-principles.html#onedrivedropbox",
    "title": "Our Principles",
    "section": "OneDrive/Dropbox",
    "text": "OneDrive/Dropbox\n\n\n\n\n\n\nOneDrive and Dropbox are ‘cloud-based file synchronisation tools’: files placed in the special folder are automatically uploaded to servers, and automatically downloaded to any other computer on which you have set up the service. Changes are also synchronised ‘every time’ you save the file."
  },
  {
    "objectID": "lectures/1.2-principles.html#why-use-it",
    "href": "lectures/1.2-principles.html#why-use-it",
    "title": "Opening Up",
    "section": "Why Use It?",
    "text": "Why Use It?\nPodman is a ‘virtualisation platform’ that allows you to run one more more virtual computers on your personal computer. We use it for four reasons:\n\nEasier installation and everyone has the same versions of the code.\nEach container is isolated and read-only.\nEasy to tidy up when you’re done.\nUsed in the ‘real world’ by many companies (JP Morgan Chase, GSK, PayPal, Twitter, Spotify, Uber…)."
  },
  {
    "objectID": "lectures/1.2-principles.html#we-recommend",
    "href": "lectures/1.2-principles.html#we-recommend",
    "title": "Opening Up",
    "section": "We Recommend…",
    "text": "We Recommend…\nUsing Podman because configuring a development machine is hard and this makes it (fairly) simple. If a Podman image works for us then we know1 it works for you.\nUse either:\n\njreades/sparc:2025-amd (Windows and Older Macs)\njreades/sparc:2025-arm (Newer Macs)\n\nNot always true, alas."
  },
  {
    "objectID": "lectures/1.2-principles.html#slack",
    "href": "lectures/1.2-principles.html#slack",
    "title": "Our Principles",
    "section": "Slack",
    "text": "Slack\n\nSlack is a “messaging app for teams” that is designed to reduce email, organise conversations & topics of discussion, and pull in relevant data from a host of other services in a flexible, fully-searchable way."
  },
  {
    "objectID": "lectures/1.2-principles.html#why-use-it-1",
    "href": "lectures/1.2-principles.html#why-use-it-1",
    "title": "Our Principles",
    "section": "Why Use It?",
    "text": "Why Use It?\nMany programmers are starting to use LLMs as part of their coding for three reasons:\n\nThey can help to spot bugs, redundancy, and other issues that impact the performance of large applications (i.e. feedback).\nThey can provide information about different libraries and strategies the developer can use, as well as completing code begun by the developer (i.e. guidance or training).\nThey can help to ‘translate’ code and design patterns between languages (i.e. re-use).\n\nThis is very much a ‘brave new world’ and we are all trying to figure it out on the fly."
  },
  {
    "objectID": "lectures/1.2-principles.html#we-recommend-1",
    "href": "lectures/1.2-principles.html#we-recommend-1",
    "title": "Our Principles",
    "section": "We Recommend…",
    "text": "We Recommend…\nLLMs like ChatGPT can help you to learn to be a better coder by providing guidance and feedback, but for many applications a competent human being will be faster and have a better grasp of the purpose of the code.\n\n\n\n\n\n\nLLMs as co-authors\n\n\nUsing ChatGPT as your co-pilot is not the same as using ChatGPT as your co-author. In this module the latter is still considered plagiarism.\n\n\n\nThe people making the best use of LLMs are people who already know how to code or write.\nLet’s turn to the rest in part 2: Writing Code!"
  },
  {
    "objectID": "lectures/1.2-principles.html#docker",
    "href": "lectures/1.2-principles.html#docker",
    "title": "Our Principles",
    "section": "Docker",
    "text": "Docker\n\nDocker “makes development efficient and predictable” because it is “used through the development lifecycle for fast, easy and portable application development”."
  },
  {
    "objectID": "lectures/1.2-principles.html#why-use-it-2",
    "href": "lectures/1.2-principles.html#why-use-it-2",
    "title": "Our Principles",
    "section": "Why Use It?",
    "text": "Why Use It?\nDocker is a ‘virtualisation platform’ that allows you to run a second (virtual) computer on your personal computer. We use it for four reasons:\n\nEasier installation than Anaconda Python and everyone has the same versions of every library.\nNo spillover effects since each container is isolated.\nEasy to tidy up when you’re done or add new containers when you start something new (e.g. PostgreSQL).\nUsed in the ‘real world’ by many companies (JP Morgan Chase, GSK, PayPal, Twitter, Spotify, Uber…)."
  },
  {
    "objectID": "lectures/1.2-principles.html#we-recommend-2",
    "href": "lectures/1.2-principles.html#we-recommend-2",
    "title": "Our Principles",
    "section": "We Recommend…",
    "text": "We Recommend…\nUsing Docker because configuring a development machine is hard, this makes it simple. If a Docker image works for us then we know 1 it works for you.\nDocker Desktop with either:\n\njreades/sparc:2025-amd (Windows and Older Macs)\njreades/sparc:2025-arm (Newer Macs)\n\n\nNot always true, alas."
  },
  {
    "objectID": "lectures/1.2-principles.html#large-language-models-llms",
    "href": "lectures/1.2-principles.html#large-language-models-llms",
    "title": "Our Principles",
    "section": "Large Language Models (LLMs)",
    "text": "Large Language Models (LLMs)\n\nChatGPT from OpenAI (an increasingly ‘ironic’ name) is simply the most famous of a growing number of Large Language Models that draw on information found on the web and in open texts to perform sophisticated summarisation tasks."
  },
  {
    "objectID": "lectures/1.2-principles.html#why-use-it-3",
    "href": "lectures/1.2-principles.html#why-use-it-3",
    "title": "Our Principles",
    "section": "Why Use It?",
    "text": "Why Use It?\nMany programmers are starting to use LLMs as part of their coding for three reasons:\n\nThey can help to spot bugs, redundancy, and other issues that impact the performance of large applications (i.e. feedback).\nThey can provide information about different libraries and strategies the developer can use, as well as completing code begun by the developer (i.e. guidance or training).\nThey can help to ‘translate’ code and design patterns between languages (i.e. re-use).\n\nThis is very much a ‘brave new world’ and we are all trying to figure it out on the fly."
  },
  {
    "objectID": "lectures/1.2-principles.html#danger-will-robinson",
    "href": "lectures/1.2-principles.html#danger-will-robinson",
    "title": "Our Principles",
    "section": "Danger, Will Robinson!",
    "text": "Danger, Will Robinson!\nHere’s what we’ve noticed about ChatGPT’s output so far:\n\nOver-use of flowery language (e.g. ‘intricate methodologies’, ‘exhaustive reviews’, …).\nLack of overall structure/coherence (i.e. individual paragraphs seem reasonable but the whole doesn’t ‘work’ as a single output).\nPoor referencing, including the use of non-existent or irrelevant references (i.e. papers that don’t actually exist, or that seem superficially useful but are not relevant in practice).\nThe more specific the information, the less likely it is to be correct (e.g. Who is Jon Reades? vs. What are the main areas where NLP is useful in built environment research?)"
  },
  {
    "objectID": "lectures/1.2-principles.html#over-confidence-in-action",
    "href": "lectures/1.2-principles.html#over-confidence-in-action",
    "title": "Our Principles",
    "section": "Over-Confidence in Action",
    "text": "Over-Confidence in Action\nWho is Jon Reades?1\n\nJon Reades is a Senior Lecturer in the Department of Geography at University College London (UCL). He is also the director of the MSc in Smart Cities and Urban Analytics program at UCL.\nReades’ research focuses on the intersection of cities, data, and technology, with a particular emphasis on the use of spatial data and analytics to understand urban processes and support decision-making in cities. He has published widely on topics such as spatial analysis, urban data science, and the use of smart technologies in urban planning and governance.\nIn addition to his academic work, Reades is actively involved in a number of professional organizations and initiatives related to urban analytics and smart cities. He is a co-founder of the Urban Dynamics Institute at UCL and has served on the editorial board of several academic journals in the fields of geography and urban studies.\n\nAnd, by the way, there’s only one of me!"
  },
  {
    "objectID": "lectures/1.2-principles.html#read-the-fine-print",
    "href": "lectures/1.2-principles.html#read-the-fine-print",
    "title": "Our Principles",
    "section": "Read the Fine Print",
    "text": "Read the Fine Print"
  },
  {
    "objectID": "lectures/1.2-principles.html#we-recommend-3",
    "href": "lectures/1.2-principles.html#we-recommend-3",
    "title": "Our Principles",
    "section": "We Recommend…",
    "text": "We Recommend…\nLLMs like ChatGPT can help you to learn to be a better coder by providing guidance and feedback, but for many applications a competent human being will be faster and have a better grasp of the purpose of the code.\n\n\n\n\n\n\nLLMs as co-authors\n\n\nUsing ChatGPT as your co-pilot is not the same as using ChatGPT as your co-author. In this module the latter is still considered plagiarism.\n\n\n\nThe people making the best use of LLMs are people who already know how to code or write."
  },
  {
    "objectID": "lectures/1.2-principles.html#recap",
    "href": "lectures/1.2-principles.html#recap",
    "title": "Our Principles",
    "section": "Recap",
    "text": "Recap\n\nWith Docker we have a way to create a coding environment that is isolated from the computer and highly portable across machines.\nWith OneDrive and/or Dropbox we have a place to store, backup, and share files (size limits apply).\nWith Slack we have a place to ask for/provide help.\nWith LLMs we have a personal ‘tutor’ who can help us to learn more quickly and effectively.\n\nLet’s turn to the rest in part 2: Writing Code!"
  },
  {
    "objectID": "lectures/1.2-principles.html#resources",
    "href": "lectures/1.2-principles.html#resources",
    "title": "Opening Up",
    "section": "Resources",
    "text": "Resources\n\nWhat is Python?\nWhy Python?\nProgramming Foundations: Fundamentals\nPython is eating the world\nWhat can you do with Python?\nGit for Decolonisation1\n\nPart art, part activism, part tech!"
  },
  {
    "objectID": "lectures/1.2-principles.html#references",
    "href": "lectures/1.2-principles.html#references",
    "title": "Opening Up",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\nOpen Data Institute. n.d. “SOD0009 - Evidence on Statistics and Open Data.” https://committees.parliament.uk/writtenevidence/45220/pdf/.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1):160018. https://doi.org/10.1038/sdata.2016.18."
  },
  {
    "objectID": "lectures/1.2-principles.html#podman",
    "href": "lectures/1.2-principles.html#podman",
    "title": "Opening Up",
    "section": "Podman",
    "text": "Podman\n\n\nPodman is an open source container, pod, and container image management engine. Podman makes it easy to find, run, build, and share containers."
  },
  {
    "objectID": "lectures/1.1-the_week.html#so-study-like-youre-learning-a-new-language.-recognise-you-are-all-starting-at-different-levels.-talk-to-other-students.-ask-for-help-when-you-need-it",
    "href": "lectures/1.1-the_week.html#so-study-like-youre-learning-a-new-language.-recognise-you-are-all-starting-at-different-levels.-talk-to-other-students.-ask-for-help-when-you-need-it",
    "title": "The Week Ahead",
    "section": "So… Study like you’re learning a new language. Recognise you are all starting at different levels. Talk to other students. Ask for help when you need it!1",
    "text": "So… Study like you’re learning a new language. Recognise you are all starting at different levels. Talk to other students. Ask for help when you need it!1\n\nSo… we do hope to convince you that:\n\nAnyone—and this includes you—can code.\nLearning to code does not require mathematical ability.\nLearning to code does not require linguistic ability.\nLearning to code does require practice. And more practice. And more again.\n\n\nDon’t take my word for it, Prat et al. (2020) in Nature link language learning to programming language learning!"
  },
  {
    "objectID": "lectures/1.1-the_week.html#why-use-them",
    "href": "lectures/1.1-the_week.html#why-use-them",
    "title": "The Week Ahead",
    "section": "Why Use Them?",
    "text": "Why Use Them?\nMany programmers use LLMs as part of their coding for three reasons:\n\nThey can help to spot bugs, redundancy, and other issues that impact the performance of large applications (i.e. feedback).\nThey can provide information about different libraries and strategies the developer can use, as well as completing code begun by the developer (i.e. guidance or training).\nThey can help to ‘translate’ code and design patterns between languages (i.e. re-use).\n\nThis is very much a ‘brave new world’ and we are all trying to figure it out on the fly."
  },
  {
    "objectID": "lectures/1.1-the_week.html#how-not-to-use-them",
    "href": "lectures/1.1-the_week.html#how-not-to-use-them",
    "title": "The Week Ahead",
    "section": "How (Not) to Use Them",
    "text": "How (Not) to Use Them\nHere’s what we’ve noticed so far:\n\nDon’t delegate learning to the LLM, it doesn’t work.\nDon’t let it write paragraphs for you because LLMs like flowery language (e.g. ‘intricate methodologies’, ‘exhaustive reviews’, …).\nDon’t expect LLMs to come up with an argument for you because they strugle with structure/coherence.\nDon’t expect LLMs to get the details right (they don’t ‘like’ referencing!)."
  },
  {
    "objectID": "lectures/1.1-the_week.html#how-to-use-them",
    "href": "lectures/1.1-the_week.html#how-to-use-them",
    "title": "The Week Ahead",
    "section": "How to Use THem",
    "text": "How to Use THem\nLLMs like ChatGPT can help you to learn to be a better coder by providing guidance and feedback:\n\nUse AI like a patient, but scatty tutor.\nAsk it to explain. Then ask it again.\nAlways test its suggestions against reality and logic.\n\n\n\n\n\n\n\nLLMs as co-authors\n\n\nUsing ChatGPT as your co-pilot is not the same as using ChatGPT as your co-author."
  },
  {
    "objectID": "lectures/1.1-the_week.html#so",
    "href": "lectures/1.1-the_week.html#so",
    "title": "The Week Ahead",
    "section": "So…",
    "text": "So…\n\n\nDo not use AI like the font of all knowledge.\nUse AI like a patient, but scatty mentor.\nAsk it to explain. Then ask it again."
  },
  {
    "objectID": "lectures/1.1-the_week.html#own-your-mistakes",
    "href": "lectures/1.1-the_week.html#own-your-mistakes",
    "title": "The Week Ahead",
    "section": "Own Your Mistakes",
    "text": "Own Your Mistakes"
  },
  {
    "objectID": "lectures/1.2-principles.html#what-is-open-source",
    "href": "lectures/1.2-principles.html#what-is-open-source",
    "title": "Opening Up",
    "section": "What is Open Source?",
    "text": "What is Open Source?\n\nGenerally, open source refers to a computer program in which the source code is available to the general public for usage, modification from its original design, and publication of their version (fork) back to the community.\nSource: Wikipedia"
  },
  {
    "objectID": "lectures/1.2-principles.html#how-is-this-helpful",
    "href": "lectures/1.2-principles.html#how-is-this-helpful",
    "title": "Opening Up",
    "section": "How is this Helpful?",
    "text": "How is this Helpful?\n\n\nIf you can think of it, someone has probably built it.\nMany hands make light work / With many eyes all bugs are shallow.\n‘Free as in speech’.\n‘Free as in beer’.1\n\n\n\n\nBy this I mean that, if you use Microsoft to do everything then you are dependent on their tools, even when they aren’t appropriate. There are many small, open source applications that do really useful but narrow things.\nWhen you have many people involved and they are working in the open\n\n\nOr chai, thalis, etc."
  },
  {
    "objectID": "lectures/1.2-principles.html#how-is-open-source-helpful",
    "href": "lectures/1.2-principles.html#how-is-open-source-helpful",
    "title": "Opening Up",
    "section": "How is Open Source Helpful?",
    "text": "How is Open Source Helpful?\n\n\nIf you can think of it, someone has probably built it.\nGiven enough eyeballs all bugs are shallow.1\n‘Free as in speech’ (always).\n‘Free as in beer’ (often).2\n\n\n\nAll of the tools used this week are open source and free.\n\n\nBy this I mean that, if you use Microsoft to do everything then you are dependent on their tools, even when they aren’t appropriate. There are many small, open source applications that do really useful but narrow things.\nWhen you have many people involved and they are working in an open environment then issues are often spotted spotted and solved more quickly. While this claim is open to some debate, there’s certainly no evidence that closed source code is any better than open source.\nA more interesting claim has to do with the potential for interested people to get involved in making code better – for instance, for some languages there may be very few translators available, leading private companies to simply ignore localisation; whereas for open source you can contribute the translation yourself! Or if you really think something isn’t working the way it should then you can offer up a solution that will make it better and, if the people running the project don’t agree, you can still do it and launch your own, competing project.\nFinally, many (though by no means all) open source projects are also free as free food, beer, or rides. If you have been taught to use ESRI’s ArcGIS or Microsoft Excel this seems crazy: how can you have free competition to those? Well, why not give QGIS and LibreOffice a try!\n\n\n\nAlso known as Linus’ law via Eric Raymond’s The Cathedral and the Bazaar.Or chai, thalis, etc."
  },
  {
    "objectID": "lectures/1.3-tools.html#tools-to-make-your-life-easier",
    "href": "lectures/1.3-tools.html#tools-to-make-your-life-easier",
    "title": "Our Tools",
    "section": "Tools to Make Your Life Easier",
    "text": "Tools to Make Your Life Easier\n\nVirtualisation: separate your computer from your coding environment.\nVersion Control: manage your code, your data, and even your reports.\nCode: how we do ‘data science’.\nMarkup: focus on the structure while you write!\nRender: creating documents and web pages from code and markup."
  },
  {
    "objectID": "lectures/1.3-tools.html#podman",
    "href": "lectures/1.3-tools.html#podman",
    "title": "Our Tools",
    "section": "Podman",
    "text": "Podman\n\n\nPodman is an open source container and image management engine. Podman makes it easy to find, run, build, and share containers."
  },
  {
    "objectID": "lectures/1.3-tools.html#why-use-it",
    "href": "lectures/1.3-tools.html#why-use-it",
    "title": "Our Tools",
    "section": "Why Use It?",
    "text": "Why Use It?\nPodman is a ‘virtualisation platform’ that allows you to run one more more virtual computers on your personal computer. We use it for four reasons:\n\nEasier installation and everyone has the same versions of the code.\nEach container is isolated and read-only.\nEasy to tidy up when you’re done.\nUsed in the ‘real world’ by many companies (JP Morgan Chase, GSK, PayPal, Twitter, Spotify, Uber…)."
  },
  {
    "objectID": "lectures/1.3-tools.html#we-recommend",
    "href": "lectures/1.3-tools.html#we-recommend",
    "title": "Our Tools",
    "section": "We Recommend…",
    "text": "We Recommend…\nUsing Podman because configuring a development machine is hard and this makes it (fairly) simple. If a Podman image works for us then we know1 it works for you.\nUse either:\n\njreades/sparc:2025-amd (Windows and Older Macs)\njreades/sparc:2025-arm (Newer Macs)\n\nUsually, but not always, true."
  },
  {
    "objectID": "lectures/1.3-tools.html#resources",
    "href": "lectures/1.3-tools.html#resources",
    "title": "Our Tools",
    "section": "Resources",
    "text": "Resources\n\nWhat is Python?\nWhy Python?\nProgramming Foundations: Fundamentals\nPython is eating the world\nWhat can you do with Python?\nGit for Decolonisation1\n\nPart art, part activism, part tech!"
  },
  {
    "objectID": "lectures/1.3-tools.html#references",
    "href": "lectures/1.3-tools.html#references",
    "title": "Our Tools",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\nKnuth, Donald Ervin. 1984. “Literate Programming.” The Computer Journal 27 (2). Oxford University Press:97–111.\n\n\nSusnjara, S., and I. Smalley. 2025. “What Is Virtualization?” 2025. https://www.ibm.com/think/topics/virtualization."
  },
  {
    "objectID": "lectures/1.2-principles.html#fair-play",
    "href": "lectures/1.2-principles.html#fair-play",
    "title": "Opening Up",
    "section": "FAIR Play",
    "text": "FAIR Play\nWilkinson et al. (2016) set out the following principles:\n\nFindable: data and metadata should be easy to find for computers and humans.\nAccessible: it should be clear how the data found can be accessed.\nInteroperable: data should work for range of analyses, storage, and processing needs.1\nReusable: metadata and data should be well-described so they can be used/combined easily.\n\n\nThere are strict and loose versions of these principles. Many governments truggle with the ‘R’ part because of licensing restrictions.\n\nShape files fail this test."
  },
  {
    "objectID": "lectures/1.2-principles.html#how-is-open-data-helpful",
    "href": "lectures/1.2-principles.html#how-is-open-data-helpful",
    "title": "Opening Up",
    "section": "How is Open Data Helpful?",
    "text": "How is Open Data Helpful?\nAccording to Open Data Institute (n.d.) (amongst others):\n\n\nPromotes transparency and accountability in government and services.\nPromotes efficiency and service delivery.\nPromotes innovation and economic growth.\nEmpowers citizens.\nReduces costs.\n\n\n\nOpen data is valuable!\n\n\nThis can be scary for many, especially those in government; however, making data open can help to debunk conspiracies and demonstrate that government is not corrupt! It can also help government to spot where there are problems.\nWe get increased efficiency and better service delivery if governments can look across departments or states to compare performance or enable others to turn up opportunities for improvement.\nCompanies can innovate off of government data: Land Registry and EPC data in the UK, for instance, give companies certainty that they won’t have to pay for this data and can build commercial products using it. The biggest gain here is in making government mapping data available.\nThere’s a strong assumption here that citizens are data literate, which I think doesn’t always hold; however, crime mapping, bus route mapping, and so on can help citizens to advocate for resources and support.\nGovernments can also save by not having to constantly respond to FOI requests or rebut other access requests from citizens and companies."
  },
  {
    "objectID": "lectures/1.2-principles.html#how-is-open-code-helpful",
    "href": "lectures/1.2-principles.html#how-is-open-code-helpful",
    "title": "Opening Up",
    "section": "How is Open Code Helpful?",
    "text": "How is Open Code Helpful?\n\n\nAllow others to build on your work (reuse, collaboration).\nAllow others to learn from your work (speed, bug detection).\nA way to attract contributors to your project (visibility, collaboration).\nA mechanism for perpetuating a potlatch ecosystem (community building, recognition, visibility).1\n\n\n\nThere are many ways to share code, including contributing questions and answers to public fora like Stack Overflow,\n\nWhere ‘knowledge is power’, not ‘power is right’."
  },
  {
    "objectID": "lectures/1.3-tools.html#two-basic-flavours",
    "href": "lectures/1.3-tools.html#two-basic-flavours",
    "title": "Our Tools",
    "section": "Two Basic ‘Flavours’",
    "text": "Two Basic ‘Flavours’\nBoth do the same thing: separate the platform from the hardware, but they do this in defferent ways for different reasons.\n\nA ‘full’ Virtual Machine (VM) includes the Operating System and behaves like a separate computer even though it may share hardware with other VMs.\nA ‘container’ is a ‘lightweight’ VM running only the application and its dependencies; everything else is managed by the host Operating SYstem so the resulting ‘image’ is small and easy to distribute.\n\nShort version: if you have to install an Operating System you are using a full VM; otherwise you are probably using a containerisation tool/\nMany things, including storage, networks, CPUs, GPUs, etc. can be virtualised."
  },
  {
    "objectID": "lectures/1.3-tools.html#why-containerise",
    "href": "lectures/1.3-tools.html#why-containerise",
    "title": "Our Tools",
    "section": "Why Containerise?",
    "text": "Why Containerise?\nWe gain quite a few benefits:\n\nEasier installation and ‘everyone’ has the same versions of the code.\nEach container is isolated and read-only.\nEasy to tidy up when you’re done.\nEasy to scale up and scale down, or to link them together via ‘microservices’.\nUsed in the ‘real world’ by many companies (JP Morgan Chase, GSK, PayPal, Twitter, Spotify, Uber…)."
  },
  {
    "objectID": "lectures/1.3-tools.html#change-of-view",
    "href": "lectures/1.3-tools.html#change-of-view",
    "title": "Our Tools",
    "section": "Change of View",
    "text": "Change of View\n\nRather than having one environment for every project, we have one environment for each project.\n\n\n‘Computing contexts’ are disposable, while data and code are persistent when I need them.\nI don’t care where my code and data are, so long as they’re accessible when I need them.\nI don’t care if containers are created or destroyed, so long as they’re available when I need them.\nI rebuild or update the computing context when I am ready to do so."
  },
  {
    "objectID": "lectures/1.3-tools.html#why-version-control",
    "href": "lectures/1.3-tools.html#why-version-control",
    "title": "Our Tools",
    "section": "Why Version Control?",
    "text": "Why Version Control?"
  },
  {
    "objectID": "lectures/1.3-tools.html#change-of-view-1",
    "href": "lectures/1.3-tools.html#change-of-view-1",
    "title": "Our Tools",
    "section": "Change of View",
    "text": "Change of View\n\nIn open source projects there may be no one view of what the ‘right’ solution/version of a project is, so differences need to be negotiated.\n\n\nEvery computer with version control might have the ‘right’ version of the code for a given user, so there is no ‘master’ view of a project.\nWe need to be able to choose whether to merge other people’s changes with our changes, rather than having everything forced on us.\nWe still want to be able to share our version of the code / outputs of the code with other people, and a web site is a good way to do that."
  },
  {
    "objectID": "lectures/1.3-tools.html#gitgithub",
    "href": "lectures/1.3-tools.html#gitgithub",
    "title": "Our Tools",
    "section": "Git/GitHub",
    "text": "Git/GitHub"
  },
  {
    "objectID": "lectures/1.3-tools.html#we-recommend-1",
    "href": "lectures/1.3-tools.html#we-recommend-1",
    "title": "Our Tools",
    "section": "We recommend…",
    "text": "We recommend…"
  },
  {
    "objectID": "lectures/1.3-tools.html#why-code",
    "href": "lectures/1.3-tools.html#why-code",
    "title": "Our Tools",
    "section": "Why Code?",
    "text": "Why Code?\nCoding has a number of advantages over ‘point-and-click’:\n\n\nThe computer requires our instructions to be unambiguous and logical.1\nComputers are infinitely patient so a process can be re-run as many times as necessary to get it ‘right’.\nWe can add complexity iteratively, but successful programming requires us to break a problem down into small steps.\n\n\nThis does not guarantee that they’ll be correct."
  },
  {
    "objectID": "lectures/1.3-tools.html#change-of-view-2",
    "href": "lectures/1.3-tools.html#change-of-view-2",
    "title": "Our Tools",
    "section": "Change of View",
    "text": "Change of View\n\nIf we can’t explain it simply enough that a computer can do it, perhaps we don’t actually understand it? Conversely, perhaps the real value of humans over AI lies in what cannot be explained to the computer.\n\n\nIt’s easy to forget how you obtained a particular result when you are clicking around inside software like ArcGIS; this is much harder when using code.\nIn analysing the problem so that we can submit it to the computer we often develop a better understanding of the problem ourselves!\nWhy spend your time doing the boring stuff???"
  },
  {
    "objectID": "lectures/1.3-tools.html#python",
    "href": "lectures/1.3-tools.html#python",
    "title": "Our Tools",
    "section": "Python",
    "text": "Python"
  },
  {
    "objectID": "lectures/1.3-tools.html#we-recommend-2",
    "href": "lectures/1.3-tools.html#we-recommend-2",
    "title": "Our Tools",
    "section": "We recommend…",
    "text": "We recommend…"
  },
  {
    "objectID": "lectures/1.3-tools.html#why-use-markup",
    "href": "lectures/1.3-tools.html#why-use-markup",
    "title": "Our Tools",
    "section": "Why use Markup?",
    "text": "Why use Markup?\n\n\nQuickly sketch out the structure of a document.\nWorks well with version control (line-by-line changes + GitHub.io web site).\nWorks well with JupyterLab and other coding environments."
  },
  {
    "objectID": "lectures/1.3-tools.html#change-of-view-3",
    "href": "lectures/1.3-tools.html#change-of-view-3",
    "title": "Our Tools",
    "section": "Change of View",
    "text": "Change of View\n\nThe way you interact with your code can be the same whether you are running it on your computer or on a cluster half-way round the world!\n\n\nTogether with VMs and Version Control, this allows us to largely stop worrying about where code is running.\nThe web browser has become the ‘app’ of choice for many developers, though it’s not without challenges."
  },
  {
    "objectID": "lectures/1.3-tools.html#markdown",
    "href": "lectures/1.3-tools.html#markdown",
    "title": "Our Tools",
    "section": "Markdown",
    "text": "Markdown"
  },
  {
    "objectID": "lectures/1.3-tools.html#we-recommend-3",
    "href": "lectures/1.3-tools.html#we-recommend-3",
    "title": "Our Tools",
    "section": "We recommend…",
    "text": "We recommend…"
  },
  {
    "objectID": "lectures/1.3-tools.html#why-render",
    "href": "lectures/1.3-tools.html#why-render",
    "title": "Our Tools",
    "section": "Why Render?",
    "text": "Why Render?\n\n\nOutputs can be: web pages, Jupyter notebooks, Word documents, PDFs, presentations…\nIt can be really useful to have a single input and multiple outputs because requirements and needs always change.\nIt teaches you to focus on the process, not the minutiae."
  },
  {
    "objectID": "lectures/1.3-tools.html#change-of-view-4",
    "href": "lectures/1.3-tools.html#change-of-view-4",
    "title": "Our Tools",
    "section": "Change of View",
    "text": "Change of View\n\nI spend a lot less time ‘faffing’ when writing than I used to. Spend more time on what you want to say and worry about the how later."
  },
  {
    "objectID": "lectures/1.3-tools.html#quarto",
    "href": "lectures/1.3-tools.html#quarto",
    "title": "Our Tools",
    "section": "Quarto",
    "text": "Quarto"
  },
  {
    "objectID": "lectures/1.3-tools.html#we-recommend-4",
    "href": "lectures/1.3-tools.html#we-recommend-4",
    "title": "Our Tools",
    "section": "We recommend…",
    "text": "We recommend…"
  },
  {
    "objectID": "lectures/1.3-tools.html#why-use-containers",
    "href": "lectures/1.3-tools.html#why-use-containers",
    "title": "Our Tools",
    "section": "Why Use Containers?",
    "text": "Why Use Containers?\nWe gain quite a few benefits:\n\nEasier installation and ‘everyone’ has the same versions of the code.\nEach container is isolated and read-only.\nEasy to tidy up when you’re done.\nEasy to scale up and scale down, or to link them together via ‘microservices’.\nUsed in the ‘real world’ by many companies (JP Morgan Chase, GSK, PayPal, Twitter, Spotify, Uber…)."
  },
  {
    "objectID": "lectures/1.3-tools.html#why-use-version-control",
    "href": "lectures/1.3-tools.html#why-use-version-control",
    "title": "Our Tools",
    "section": "Why use Version Control?",
    "text": "Why use Version Control?\n\n… If a mistake is made, developers can turn back the clock and compare earlier versions of the code to help fix the mistake while minimizing disruption to all team members.\nSource: Altassian\n\nIn addition:\n\n\nWe can share code with others (directly) as source code or (indirectly) as the product of compiling that source code.\nWe can rewind, fast forward, and combine changes by different people working on different features.\nWe gain detailed, incremental backups that help us tro track down the changes that introduced a bug when something goes wrong.\n\n\n\nDiscuss comparison with Dropbox or OneDrive. How are they similar, how are they different? File level vs. row-level views."
  },
  {
    "objectID": "lectures/1.3-tools.html#git",
    "href": "lectures/1.3-tools.html#git",
    "title": "Our Tools",
    "section": "Git",
    "text": "Git\n\n\n\n\nVersion control allows us to:\n\nTrack changes to files with a high level of detail using commit.\npush these changes out to others.\npull down changes made by others.\nmerge and resolve conflicting changes.\nCreate a tag when a ‘milestones’ is reached.\nCreate a branch to add a feature.\nRetrieve specific versions or branches with a checkout."
  },
  {
    "objectID": "lectures/1.3-tools.html#github",
    "href": "lectures/1.3-tools.html#github",
    "title": "Our Tools",
    "section": "GitHub",
    "text": "GitHub\n\n\n\n\nGit is distributed, meaning that every computer is a potential server and a potential authority. Result: commits on a plane!\nBut how do people find and access your code if your ‘server’ is a home machine that goes to sleep at night? Result: GitHub.\nGitHub is ‘just’ a very large Git server with a lot of nice web-friendly features tacked on: create a web site, issue/bug tracking, promote your project…"
  },
  {
    "objectID": "lectures/1.3-tools.html#gitgithub-is-for-anything",
    "href": "lectures/1.3-tools.html#gitgithub-is-for-anything",
    "title": "Our Tools",
    "section": "Git+GitHub is for… anything!",
    "text": "Git+GitHub is for… anything!\n\n\nThis whole course is on GitHub."
  },
  {
    "objectID": "lectures/1.3-tools.html#oh-my-git",
    "href": "lectures/1.3-tools.html#oh-my-git",
    "title": "Our Tools",
    "section": "Oh My Git!",
    "text": "Oh My Git!\n\n\nSource: OhMyGit"
  },
  {
    "objectID": "lectures/1.3-tools.html#jupyterlab",
    "href": "lectures/1.3-tools.html#jupyterlab",
    "title": "Our Tools",
    "section": "JupyterLab",
    "text": "JupyterLab"
  },
  {
    "objectID": "lectures/1.3-tools.html#why-use-jupyterlab",
    "href": "lectures/1.3-tools.html#why-use-jupyterlab",
    "title": "Our Tools",
    "section": "Why use JupyterLab?",
    "text": "Why use JupyterLab?\n\nNothing to install (runs in your web browser).\nRun code from anywhere (runs in your web browser).\nNatural way to interact with containers/VMs.\nLarge ecosystem of plugins/extensions.\nJupyter notebooks are compatible with VSCode."
  },
  {
    "objectID": "lectures/1.3-tools.html#markdown-1",
    "href": "lectures/1.3-tools.html#markdown-1",
    "title": "Our Tools",
    "section": "Markdown",
    "text": "Markdown"
  },
  {
    "objectID": "lectures/1.3-tools.html#change-of-view-5",
    "href": "lectures/1.3-tools.html#change-of-view-5",
    "title": "Our Tools",
    "section": "Change of View",
    "text": "Change of View\n\nEverything we are seeing and doing this week passed through Quarto. It has transformed the way I teach, do research, and write! It embodies ‘literate programming’ (Knuth 1984)."
  },
  {
    "objectID": "lectures/1.2-what-do-we-do.html#the-pipeline",
    "href": "lectures/1.2-what-do-we-do.html#the-pipeline",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "The ‘Pipeline’",
    "text": "The ‘Pipeline’\n\nCode\nAnalyse\nUnderstand\nCommunicate\nReport\n\n\nHere are five things that we might imagine any data scientists, spatial or otherwise, does.\nDo we think they’re in the right order?\nI can tell you right now that this sequence of steps is how to get a mark of between 45 and 63 in your dissertation."
  },
  {
    "objectID": "lectures/1.2-what-do-we-do.html#the-pipeline-1",
    "href": "lectures/1.2-what-do-we-do.html#the-pipeline-1",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "The ‘Pipeline’",
    "text": "The ‘Pipeline’\n\nUnderstand\nCode\nReport\nAnalyse\nCommunicate\n\n\nHere’s a slightly better order, here’s why:\n\nYou can’t answer a question – from your boss, from an academic, from your friends even – if you don’t understand it. So before you run off and start writing some code, the first thing you need to do is understand the problem you’re trying to solve. That problem should not be technical, it should be practical.\nOnce you understand the problem you can start trying to code a solution.\nThe code will allow you to produce reports. These reports might be to do with data quality, they might be diagnostics from a Random Forest Machine Learning algorithm. At each stage in the development of your results you should be generating reports that help you to better-understand your problem and work out if your code is working.\nOnce the reports have given you confidence in your findings now you can actually write the analysis. Your analysis might lead you to realise that you need to go back and write more code and produce more reports, but that’s normal.\nFinally, you need to work out how to communicate your analysis. If you understand the problem then you’ll find this process rewarding. If you don’t then you’ll find it frustrating and want to brush it off quickly.\n\nSo the fact these are in a list is still rather misleading because at each point you get feedback effects, and it’s also a loop."
  },
  {
    "objectID": "lectures/1.2-what-do-we-do.html#another-way",
    "href": "lectures/1.2-what-do-we-do.html#another-way",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "Another Way…",
    "text": "Another Way…\nAnother way to think about all this is to write backwards:"
  },
  {
    "objectID": "lectures/1.2-what-do-we-do.html#how-to-understand",
    "href": "lectures/1.2-what-do-we-do.html#how-to-understand",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Understand?",
    "text": "How to Understand?\n\nHow should we do this?"
  },
  {
    "objectID": "lectures/1.2-what-do-we-do.html#how-to-understand-1",
    "href": "lectures/1.2-what-do-we-do.html#how-to-understand-1",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Understand?",
    "text": "How to Understand?\n\nHow should we do this?\n\n\nWhy are we doing this?"
  },
  {
    "objectID": "lectures/1.2-what-do-we-do.html#how-to-report",
    "href": "lectures/1.2-what-do-we-do.html#how-to-report",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Report?",
    "text": "How to Report?\n\nThis is the number."
  },
  {
    "objectID": "lectures/1.2-what-do-we-do.html#how-to-report-1",
    "href": "lectures/1.2-what-do-we-do.html#how-to-report-1",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Report?",
    "text": "How to Report?\n\nThis is the number.\n\n\nThese are the takeaways."
  },
  {
    "objectID": "lectures/1.2-what-do-we-do.html#how-to-analyse",
    "href": "lectures/1.2-what-do-we-do.html#how-to-analyse",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Analyse?",
    "text": "How to Analyse?\n\nThese are the methods we can use."
  },
  {
    "objectID": "lectures/1.2-what-do-we-do.html#how-to-analyse-1",
    "href": "lectures/1.2-what-do-we-do.html#how-to-analyse-1",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Analyse?",
    "text": "How to Analyse?\n\nThese are the methods I can use.\n\n\nThis is the method that matches the need."
  },
  {
    "objectID": "lectures/1.2-what-do-we-do.html#how-to-communicate",
    "href": "lectures/1.2-what-do-we-do.html#how-to-communicate",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Communicate?",
    "text": "How to Communicate?\n\nWhat do I need to say?"
  },
  {
    "objectID": "lectures/1.2-what-do-we-do.html#how-to-communicate-1",
    "href": "lectures/1.2-what-do-we-do.html#how-to-communicate-1",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "How to Communicate?",
    "text": "How to Communicate?\n\nWhat do I need to say?\n\n\nWho needs to know?"
  },
  {
    "objectID": "lectures/1.2-what-do-we-do.html#writing-thinking",
    "href": "lectures/1.2-what-do-we-do.html#writing-thinking",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "Writing == Thinking",
    "text": "Writing == Thinking\n\nIf writing down your ideas always makes them more precise and more complete, then no one who hasn’t written about a topic has fully formed ideas about it. And someone who never writes has no fully formed ideas about anything nontrivial. ~ Graham (2022)"
  },
  {
    "objectID": "lectures/1.2-what-do-we-do.html#clarity-understanding",
    "href": "lectures/1.2-what-do-we-do.html#clarity-understanding",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "Clarity == Understanding",
    "text": "Clarity == Understanding\n\nWriting is thinking. To write well is to think clearly. That’s why it’s so hard. ~ McCullough (2002)"
  },
  {
    "objectID": "lectures/1.2-what-do-we-do.html#i-dont-care",
    "href": "lectures/1.2-what-do-we-do.html#i-dont-care",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "I Don’t Care",
    "text": "I Don’t Care\n\nFocus on the ‘so what’."
  },
  {
    "objectID": "lectures/1.2-what-do-we-do.html#tell-me-a-story",
    "href": "lectures/1.2-what-do-we-do.html#tell-me-a-story",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "Tell Me a Story",
    "text": "Tell Me a Story\n\nAll data science is, ultimately, a story.\n\n\nA story of struggle. Frustration. Discovery. Learning. But you need to tell that story the right way.\nMany of you will have learned some ‘system’ for writing in school. The inverted pyramid or something like that. In university, in my literary theory class I picked up the pyramid approach: taking a single sentence and unpacking that into the themes of the entire book.\nThere are mystery novels. Romance novels. Economist articles. Teen Vogue articles. They are all telling stories. They all do this in different ways."
  },
  {
    "objectID": "lectures/1.2-what-do-we-do.html#resources",
    "href": "lectures/1.2-what-do-we-do.html#resources",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "Resources",
    "text": "Resources\n\nThe Most Undervalued Skill for Data Scientists\nOne Mindset Shift That Will Make You a Better Data Scientist\nSword, H. (2017). Air & light & time & space: How successful academics write. Harvard University Press.1\nGreene, A. E. (2013). Writing science in plain English. University of Chicago Press.2\n\n\n\n\n\nGraham, P. 2022. “Putting Ideas into Words.” 2022. https://paulgraham.com/words.html.\n\n\nXie, Tessa. 2024. “The Most Undervalued Skill for Data Scientists.” 2024. https://towardsdatascience.com/the-most-undervalued-skill-for-data-scientists-e0e0d7709321/.\n\n\nAvailable for free via JStor.Not seemingly available for free, but I found a nice little summary (with typos) here."
  },
  {
    "objectID": "lectures/1.4-tools.html#tools-to-make-your-life-easier",
    "href": "lectures/1.4-tools.html#tools-to-make-your-life-easier",
    "title": "Our Tools",
    "section": "Tools to Make Your Life Easier",
    "text": "Tools to Make Your Life Easier\n\nVirtualisation: separate your computer from your coding environment.\nVersion Control: manage your code, your data, and even your reports.\nCode: how we do ‘data science’.\nMarkup: focus on the structure while you write!\nRender: creating documents and web pages from code and markup."
  },
  {
    "objectID": "lectures/1.4-tools.html#two-basic-flavours",
    "href": "lectures/1.4-tools.html#two-basic-flavours",
    "title": "Our Tools",
    "section": "Two Basic ‘Flavours’",
    "text": "Two Basic ‘Flavours’\nBoth do the same thing: separate the platform from the hardware, but they do this in defferent ways for different reasons.\n\nA ‘full’ Virtual Machine (VM) includes the Operating System and behaves like a separate computer even though it may share hardware with other VMs.\nA ‘container’ is a ‘lightweight’ VM running only the application and its dependencies; everything else is managed by the host Operating System so the resulting ‘image’ is small and easy to distribute.\n\nShort version: if you have to install an Operating System you are using a full VM; otherwise you are probably using a containerisation tool/\nMany things, including storage, networks, CPUs, GPUs, etc. can be virtualised."
  },
  {
    "objectID": "lectures/1.4-tools.html#why-use-containers",
    "href": "lectures/1.4-tools.html#why-use-containers",
    "title": "Our Tools",
    "section": "Why Use Containers?",
    "text": "Why Use Containers?\nWe gain quite a few benefits:\n\nEasier installation and ‘everyone’ has the same versions of the code.\nEach container is isolated and read-only.\nEasy to tidy up when you’re done.\nEasy to scale up and scale down, or to link them together via ‘microservices’.\nUsed in the ‘real world’ by many companies (JP Morgan Chase, GSK, PayPal, Twitter, Spotify, Uber…)."
  },
  {
    "objectID": "lectures/1.4-tools.html#change-of-view",
    "href": "lectures/1.4-tools.html#change-of-view",
    "title": "Our Tools",
    "section": "Change of View",
    "text": "Change of View\n\nIf we can’t explain it simply enough that a computer can do it, perhaps we don’t actually understand it?\n\n\n\nTogether with the other tools in this talk, you can largely stop worrying about where code is running.\nIt’s easy to forget how you obtained a particular result when you are clicking around inside software like ArcGIS; this is much harder when using code.\nIn analysing the problem so that we can submit it to the computer we often develop a better understanding of the problem ourselves!\nWhy spend your time doing the boring stuff???\n\n\n\nConversely, perhaps the real value of humans over AI lies in what cannot be explained to the computer."
  },
  {
    "objectID": "lectures/1.4-tools.html#podman",
    "href": "lectures/1.4-tools.html#podman",
    "title": "Our Tools",
    "section": "Podman",
    "text": "Podman\n\n\nPodman is an open source container and image management engine. Podman makes it easy to find, run, build, and share containers."
  },
  {
    "objectID": "lectures/1.4-tools.html#we-recommend",
    "href": "lectures/1.4-tools.html#we-recommend",
    "title": "Our Tools",
    "section": "We Recommend…",
    "text": "We Recommend…\nUsing Podman because configuring a development machine is hard and this makes it (fairly) simple. If a Podman image works for us then we know1 it works for you.\nUse either:\n\njreades/sparc:2025-amd (Windows and Older Macs)\njreades/sparc:2025-arm (Newer Macs)\n\nUsually, but not always, true."
  },
  {
    "objectID": "lectures/1.4-tools.html#why-use-version-control",
    "href": "lectures/1.4-tools.html#why-use-version-control",
    "title": "Our Tools",
    "section": "Why use Version Control?",
    "text": "Why use Version Control?\n\n… If a mistake is made, developers can turn back the clock and compare earlier versions of the code to help fix the mistake while minimizing disruption to all team members.\nSource: Altassian\n\nIn addition:\n\n\nWe can share code with others (directly) as source code or (indirectly) as the product of compiling that source code.\nWe can rewind, fast forward, and combine changes by different people working on different features.\nWe gain detailed, incremental backups that help us tro track down the changes that introduced a bug when something goes wrong.\n\n\n\nDiscuss comparison with Dropbox or OneDrive. How are they similar, how are they different? File level vs. row-level views."
  },
  {
    "objectID": "lectures/1.4-tools.html#change-of-view-1",
    "href": "lectures/1.4-tools.html#change-of-view-1",
    "title": "Our Tools",
    "section": "Change of View",
    "text": "Change of View\n\nRather than having one environment for every project, we have one environment for each project.\n\n\n‘Computing contexts’ are disposable, while data and code are persistent when I need them.\nI don’t care where my code and data are, so long as they’re accessible when I need them.\nI don’t care if containers are created or destroyed, so long as they’re available when I need them.\nI rebuild or update the computing context when I am ready to do so."
  },
  {
    "objectID": "lectures/1.4-tools.html#git",
    "href": "lectures/1.4-tools.html#git",
    "title": "Our Tools",
    "section": "Git",
    "text": "Git\n\n\n\n\nVersion control allows us to:\n\nTrack changes to files with a high level of detail using commit.\npush these changes out to others.\npull down changes made by others.\nmerge and resolve conflicting changes.\nCreate a tag when a ‘milestones’ is reached.\nCreate a branch to add a feature.\nRetrieve specific versions or branches with a checkout."
  },
  {
    "objectID": "lectures/1.4-tools.html#github",
    "href": "lectures/1.4-tools.html#github",
    "title": "Our Tools",
    "section": "GitHub",
    "text": "GitHub\n\n\n\n\nGit is distributed, meaning that every computer is a potential server and a potential authority. Result: commits on a plane!\nBut how do people find and access your code if your ‘server’ is a home machine that goes to sleep at night? Result: GitHub.\nGitHub is ‘just’ a very large Git server with a lot of nice web-friendly features tacked on: create a web site, issue/bug tracking, promote your project…"
  },
  {
    "objectID": "lectures/1.4-tools.html#gitgithub-is-for-anything",
    "href": "lectures/1.4-tools.html#gitgithub-is-for-anything",
    "title": "Our Tools",
    "section": "Git+GitHub is for… anything!",
    "text": "Git+GitHub is for… anything!\n\n\nThis whole course is on GitHub."
  },
  {
    "objectID": "lectures/1.4-tools.html#oh-my-git",
    "href": "lectures/1.4-tools.html#oh-my-git",
    "title": "Our Tools",
    "section": "Oh My Git!",
    "text": "Oh My Git!\n\n\nSource: OhMyGit"
  },
  {
    "objectID": "lectures/1.4-tools.html#why-code",
    "href": "lectures/1.4-tools.html#why-code",
    "title": "Our Tools",
    "section": "Why Code?",
    "text": "Why Code?\nCoding has a number of advantages over ‘point-and-click’:\n\n\nThe computer requires our instructions to be unambiguous and logical.1\nComputers are infinitely patient so a process can be re-run as many times as necessary to get it ‘right’.\nWe can add complexity iteratively, but successful programming requires us to break a problem down into small steps.\n\n\nThis does not guarantee that they’ll be correct."
  },
  {
    "objectID": "lectures/1.4-tools.html#change-of-view-2",
    "href": "lectures/1.4-tools.html#change-of-view-2",
    "title": "Our Tools",
    "section": "Change of View",
    "text": "Change of View\n\nIn open source projects there may be no one view of what the ‘right’ solution/version of a project is, so differences need to be negotiated.\n\n\nEvery computer with version control might have the ‘right’ version of the code for a given user, so there is no ‘master’ view of a project.\nWe need to be able to choose whether to merge other people’s changes with our changes, rather than having everything forced on us.\nWe still want to be able to share our version of the code / outputs of the code with other people, and a web site is a good way to do that."
  },
  {
    "objectID": "lectures/1.4-tools.html#why-use-jupyterlab",
    "href": "lectures/1.4-tools.html#why-use-jupyterlab",
    "title": "Our Tools",
    "section": "Why Use JupyterLab?",
    "text": "Why Use JupyterLab?\nCoding in JupyterLab has a number of advantages over ‘point-and-click’:\n\n\nCoding requires our instructions to be unambiguous and logical.1\nComputers are infinitely patient so we can re-run as many times as necessary to get it ‘right’.\nThere is nothing to install (runs in your web browser).\nYou can run code from anywhere (runs in your web browser).\n\n\nThis does not guarantee that they’ll be correct."
  },
  {
    "objectID": "lectures/1.4-tools.html#change-of-view-3",
    "href": "lectures/1.4-tools.html#change-of-view-3",
    "title": "Our Tools",
    "section": "Change of View",
    "text": "Change of View\n\nI spend a lot less time ‘faffing’ writing in Markdown than I used to. Spend more time on what you want to say and worry about the how later.\n\n\n\n### A Subtitle\n\nSome text in **bold** and *italics* with a [link](https://jreades.github.io/).\n\n&gt; A blockquote\n\nA Subtitle\nSome text in bold and italics with a link.\n\nA blockquote"
  },
  {
    "objectID": "lectures/1.4-tools.html#why-use-markup",
    "href": "lectures/1.4-tools.html#why-use-markup",
    "title": "Our Tools",
    "section": "Why use Markup?",
    "text": "Why use Markup?\n\n\nQuickly sketch out the structure of a document.\nWorks well with version control (line-by-line changes + GitHub.io web site).\nWorks well with JupyterLab and other coding environments."
  },
  {
    "objectID": "lectures/1.4-tools.html#change-of-view-4",
    "href": "lectures/1.4-tools.html#change-of-view-4",
    "title": "Our Tools",
    "section": "Change of View",
    "text": "Change of View\n\nEverything this week was created using these basic tools and techniques. It has transformed the way I teach, do research, and write! It embodies the potential of ‘literate programming’ (Knuth 1984)."
  },
  {
    "objectID": "lectures/1.4-tools.html#why-render",
    "href": "lectures/1.4-tools.html#why-render",
    "title": "Our Tools",
    "section": "Why Render?",
    "text": "Why Render?\n\n\nOutputs can be: web pages, Jupyter notebooks, Word documents, PDFs, presentations…\nIt can be really useful to have a single input and multiple outputs because requirements and needs always change.\nIt teaches you to focus on the process, not the minutiae."
  },
  {
    "objectID": "lectures/1.4-tools.html#change-of-view-5",
    "href": "lectures/1.4-tools.html#change-of-view-5",
    "title": "Our Tools",
    "section": "Change of View",
    "text": "Change of View\n\nEverything this week was created using these basic tools and techniques. It has transformed the way I teach, do research, and write! It embodies the potential of ‘literate programming’ (Knuth 1984)."
  },
  {
    "objectID": "lectures/1.4-tools.html#references",
    "href": "lectures/1.4-tools.html#references",
    "title": "Our Tools",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\nKnuth, D. E. 1984. “Literate Programming.” The Computer Journal 27 (2). Oxford University Press:97–111.\n\n\n———. 1996. Selected Papers on Computer Science. Cambridge University Press.\n\n\nSusnjara, S., and I. Smalley. 2025. “What Is Virtualization?” 2025. https://www.ibm.com/think/topics/virtualization."
  },
  {
    "objectID": "lectures/1.2-what-do-we-do.html#references",
    "href": "lectures/1.2-what-do-we-do.html#references",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\nGraham, P. 2022. “Putting Ideas into Words.” 2022. https://paulgraham.com/words.html.\n\n\nMcCullough, D. 2002. “Interview with NEH chairman Bruce Cole.” Humanities Magazine.\n\n\nXie, Tessa. 2024. “The Most Undervalued Skill for Data Scientists.” 2024. https://towardsdatascience.com/the-most-undervalued-skill-for-data-scientists-e0e0d7709321/."
  },
  {
    "objectID": "lectures/1.2-what-do-we-do.html#writing-coding-thinking",
    "href": "lectures/1.2-what-do-we-do.html#writing-coding-thinking",
    "title": "What Does a (Spatial) Data Scientist Do?",
    "section": "Writing & Coding == Thinking",
    "text": "Writing & Coding == Thinking\n\nIf writing down your ideas always makes them more precise and more complete, then no one who hasn’t written about a topic has fully formed ideas about it. And someone who never writes has no fully formed ideas about anything nontrivial. ~ Graham (2022)"
  },
  {
    "objectID": "lectures/1.4-tools.html#markdown-examples",
    "href": "lectures/1.4-tools.html#markdown-examples",
    "title": "Our Tools",
    "section": "Markdown Examples",
    "text": "Markdown Examples\nSee CommonMark and the Markdown Guide for more:\n\n\n\n\n\n\n\nFormat\nOutput\n\n\nPlain text...\nPlain text\n\n\n## A Large Heading\nA Large Heading\n\n\n### A Medium Heading\nA Medium Heading\n\n\n- A list\n- More list\n\nA list\nMore list\n\n\n\n1. An ordered list\n2. More ordered list\n\nAn ordered list\nMore ordered list\n\n\n\n[A link](http://casa.ucl.ac.uk)\nA link\n\n\n\n\nThis guide is good for HTML entities, though Google will also give you them pretty easily if you type HTML entity code for copyright…"
  },
  {
    "objectID": "lectures/1.4-tools.html#section",
    "href": "lectures/1.4-tools.html#section",
    "title": "Our Tools",
    "section": "",
    "text": "But why would we want this?"
  },
  {
    "objectID": "lectures/1.4-tools.html#additional-resources",
    "href": "lectures/1.4-tools.html#additional-resources",
    "title": "Our Tools",
    "section": "Additional Resources",
    "text": "Additional Resources\nMarkdown:\n\nGetting Started\nAn online interactive tutorial\nCheatsheet\n\nAnd once you’re ready to get ‘serious’, check out this tutorial on Sustainable Authorship in Plain Text using Pandoc and Markdown from The Programming Historian! That’s what actually underpins Quarto, but you can do so much more…"
  },
  {
    "objectID": "lectures/1.4-tools.html#literate-programming",
    "href": "lectures/1.4-tools.html#literate-programming",
    "title": "Our Tools",
    "section": "Literate Programming",
    "text": "Literate Programming\nIdeally, we want to ‘do’ data science in ways that are ‘literate’.\n\nThe best programs are written so that computing machines can perform them quickly and so that human beings can understand them clearly. A programmer is ideally an essayist who works with traditional aesthetic and literary forms as well as mathematical concepts, to communicate the way that an algorithm works and to convince a reader that the results will be correct. ~ Knuth (1996)"
  },
  {
    "objectID": "lectures/1.4-tools.html#key-tenets",
    "href": "lectures/1.4-tools.html#key-tenets",
    "title": "Our Tools",
    "section": "Key Tenets",
    "text": "Key Tenets\nWhat we want:\n\nWeaving: the code and its documentation are together.\nTangling: the code can be run directly.\n\nIn an ideal world, these are the same file…"
  },
  {
    "objectID": "lectures/1.4-tools.html#but-why-do-we-want-this",
    "href": "lectures/1.4-tools.html#but-why-do-we-want-this",
    "title": "Our Tools",
    "section": "But why do we want this?",
    "text": "But why do we want this?"
  },
  {
    "objectID": "lectures/1.4-tools.html#and-how-do-we-do-this",
    "href": "lectures/1.4-tools.html#and-how-do-we-do-this",
    "title": "Our Tools",
    "section": "And how do we do this?",
    "text": "And how do we do this?\nHint: it’s more than just one thing…\n\n\nJupyterLab: how we do ‘data science’.\nVirtualisation: separate your computer from your coding environment.\nVersion Control: manage your code, your data, and even your reports.\nMarkup: focus on the structure while you write!\nRender: creating documents and web pages from code and markup."
  },
  {
    "objectID": "lectures/1.4-tools.html#but-why-would-we-want-this",
    "href": "lectures/1.4-tools.html#but-why-would-we-want-this",
    "title": "Our Tools",
    "section": "But why would we want this?",
    "text": "But why would we want this?"
  },
  {
    "objectID": "lectures/1.4-tools.html#section-1",
    "href": "lectures/1.4-tools.html#section-1",
    "title": "Our Tools",
    "section": "",
    "text": "Format\n\n\nOutput\n\n\n\n\n![Alt Text](casa_logo.jpg)\n\n\n\n\n\n\n\n`A code snippet`\n\n\nA code snippet\n\n\n\n\n```{python} A block of Python code ```\n\n\nA block of Python code\n\n\n\n\n$$\nf(a) = \\frac{1}{2\\pi i}     \\oint_{\\gamma} \\frac{f(z)}{z-a} dz\n$$\n\n\n\\[\nf(a) = \\frac{1}{2\\pi i} \\oint_{\\gamma} \\frac{f(z)}{z-a} dz\n\\]"
  },
  {
    "objectID": "lectures/1.4-tools.html#jupyterlab",
    "href": "lectures/1.4-tools.html#jupyterlab",
    "title": "Our Tools",
    "section": "JupyterLab",
    "text": "JupyterLab\n\nJupyterLab is the latest web-based interactive development environment for notebooks, code, and data. Its flexible interface allows users to configure and arrange workflows in data science, scientific computing, computational journalism, and machine learning.\nSource: Project Jupyter"
  },
  {
    "objectID": "lectures/1.4-tools.html#using-podman",
    "href": "lectures/1.4-tools.html#using-podman",
    "title": "Our Tools",
    "section": "Using Podman",
    "text": "Using Podman\nPodman makes configuring a development environment (fairly) simple. If a Podman image works for us then we know1 it works for you.\nUse either:\n\njreades/sparc:2025-amd (Windows and Older Macs)\njreades/sparc:2025-arm (Newer Macs)\n\nUsually, but not always, true."
  },
  {
    "objectID": "lectures/1.3-principles.html#how-is-open-source-helpful",
    "href": "lectures/1.3-principles.html#how-is-open-source-helpful",
    "title": "Our Principles",
    "section": "How is Open Source Helpful?",
    "text": "How is Open Source Helpful?\n\n\nIf you can think of it, someone has probably built it.\nGiven enough eyeballs all bugs are shallow.1\n‘Free as in speech’ (always).\n‘Free as in beer’ (often).2\n\n\n\nAll of the tools used this week are open source and free.\n\n\nBy this I mean that, if you use Microsoft to do everything then you are dependent on their tools, even when they aren’t appropriate. There are many small, open source applications that do really useful but narrow things.\nWhen you have many people involved and they are working in an open environment then issues are often spotted spotted and solved more quickly. While this claim is open to some debate, there’s certainly no evidence that closed source code is any better than open source.\nA more interesting claim has to do with the potential for interested people to get involved in making code better – for instance, for some languages there may be very few translators available, leading private companies to simply ignore localisation; whereas for open source you can contribute the translation yourself! Or if you really think something isn’t working the way it should then you can offer up a solution that will make it better and, if the people running the project don’t agree, you can still do it and launch your own, competing project.\nFinally, many (though by no means all) open source projects are also free as free food, beer, or rides. If you have been taught to use ESRI’s ArcGIS or Microsoft Excel this seems crazy: how can you have free competition to those? Well, why not give QGIS and LibreOffice a try!\n\n\n\nAlso known as Linus’ law via Eric Raymond’s The Cathedral and the Bazaar.Or chai, thalis, etc."
  },
  {
    "objectID": "lectures/1.3-principles.html#fair-play",
    "href": "lectures/1.3-principles.html#fair-play",
    "title": "Our Principles",
    "section": "FAIR Play",
    "text": "FAIR Play\nWilkinson et al. (2016) set out the following principles:\n\n\nFindable: data and metadata should be easy to find for computers and humans.\nAccessible: it should be clear how the data found can be accessed.\nInteroperable: data should work for range of analyses, storage, and processing needs.1\nReusable: metadata and data should be well-described so they can be used/combined easily.\n\n\n\nThere are strict and loose versions of these principles. Many governments truggle with the ‘R’ part because of licensing restrictions.\n\nShape files fail this test."
  },
  {
    "objectID": "lectures/1.3-principles.html#how-is-open-data-helpful",
    "href": "lectures/1.3-principles.html#how-is-open-data-helpful",
    "title": "Our Principles",
    "section": "How is Open Data Helpful?",
    "text": "How is Open Data Helpful?\nAccording to Open Data Institute (n.d.) (amongst others):\n\n\nPromotes transparency and accountability in government and services.\nPromotes efficiency and service delivery.\nPromotes innovation and economic growth.\nEmpowers citizens.\nReduces costs.\n\n\n\nOpen data is valuable!\n\n\nThis can be scary for many, especially those in government; however, making data open can help to debunk conspiracies and demonstrate that government is not corrupt! It can also help government to spot where there are problems.\nWe get increased efficiency and better service delivery if governments can look across departments or states to compare performance or enable others to turn up opportunities for improvement.\nCompanies can innovate off of government data: Land Registry and EPC data in the UK, for instance, give companies certainty that they won’t have to pay for this data and can build commercial products using it. The biggest gain here is in making government mapping data available.\nThere’s a strong assumption here that citizens are data literate, which I think doesn’t always hold; however, crime mapping, bus route mapping, and so on can help citizens to advocate for resources and support.\nGovernments can also save by not having to constantly respond to FOI requests or rebut other access requests from citizens and companies."
  },
  {
    "objectID": "lectures/1.3-principles.html#how-is-open-code-helpful",
    "href": "lectures/1.3-principles.html#how-is-open-code-helpful",
    "title": "Our Principles",
    "section": "How is Open Code Helpful?",
    "text": "How is Open Code Helpful?\n\n\nAllow others to build on your work (reuse, collaboration).\nAllow others to learn from your work (speed, bug detection).\nA way to attract contributors to your project (visibility, collaboration).\nA mechanism for perpetuating a potlatch ecosystem (community building, recognition, visibility).1\n\n\n\nThere are many ways to share code, including contributing questions and answers to public fora like Stack Overflow,\n\nWhere ‘knowledge is power’, not ‘power is right’."
  },
  {
    "objectID": "lectures/1.3-principles.html#references",
    "href": "lectures/1.3-principles.html#references",
    "title": "Our Principles",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\nOpen Data Institute. n.d. “SOD0009 - Evidence on Statistics and Open Data.” https://committees.parliament.uk/writtenevidence/45220/pdf/.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1):160018. https://doi.org/10.1038/sdata.2016.18."
  },
  {
    "objectID": "bib/Bibliography.html",
    "href": "bib/Bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "Bibliography\n\n\nAlsudais, Abdulkareem. 2021. “Incorrect Data in the Widely Used Inside Airbnb Dataset.” Decision Support Systems 141:113453. https://doi.org/10.1016/j.dss.2020.113453.\n\n\nAmoore, L. 2019. “Doubt and the Algorithm: On the Partial Accounts of Machine Learning.” Theory, Culture, Society 36 (6):147–69. https://doi.org/10.1177/0263276419851846.\n\n\nAnderson, C. 2008. “The End of Theory: The Data Deluge Makes the Scientific Method Obsolete.” Wired. https://www.wired.com/2008/06/pb-theory/.\n\n\nArribas-Bel, Daniel. 2014. “Accidental, Open and Everywhere: Emerging Data Sources for the Understanding of Cities.” Applied Geography 49. Elsevier:45–53. https://doi.org/10.1016/j.apgeog.2013.09.012.\n\n\nArribas-Bel, D., and J. Reades. 2018. “Geography and Computers: Past, Present, and Future.” Geography Compass 12 (e12403). https://doi.org/10.1111/gec3.12403.\n\n\nBadger, E., Q. Bui, and R. Gebeloff. 2019. “Neighborhood Is Mostly Black. The Home Buyers Are Mostly White. New York Times.” New York Times. https://www.nytimes.com/interactive/2019/04/27/upshot/diversity-housing-maps-raleigh-gentrification.html.\n\n\nBarron, K., E. Kung, and D. Proserpio. 2018. “The Sharing Economy and Housing Affordability: Evidence from Airbnb.” https://static1.squarespace.com/static/5bb2d447a9ab951efbf6d10a/t/5bea6881562fa7934045a3f0/1542088837594/The+Sharing+Economy+and+Housing+Affordability.pdf.\n\n\nBemt, V. van den, J. Doornbos, L. Meijering, M. Plegt, and N. Theunissen. 2018. “Teaching Ethics When Working with Geocoded Data: A Novel Experiential Learning Approach.” Journal of Geography in Higher Education 42 (2):293–310. https://doi.org/10.1080/03098265.2018.1436534.\n\n\nBrockes, E. 2023. “Airbnb was wild, disruptive and cheap: we loved it. But it wasn’t a love strong enough to last.” The Guardian. https://www.theguardian.com/commentisfree/2023/mar/08/airbnb-wild-disruptive-cheap-lettings-agency.\n\n\nBunday, B. D. n.d. “A Final Tale or You Can Prove Anything with Figures.” https://www.ucl.ac.uk/~ucahhwi/AFinalTale.pdf.\n\n\nBurton, I. 1963. “The Quantitative Revolution and Theoretical Geography.” The Canadian Geographer/Le Géographe Canadien 7 (4):151–62. https://doi.org/10.1111/j.1541-0064.1963.tb00796.x.\n\n\nCheng, M., and C. Foley. 2018. “The Sharing Economy and Digital Discrimination: The Case of Airbnb.” International Journal of Hospitality Management 70:95–98. https://doi.org/10.1016/j.ijhm.2017.11.002.\n\n\nCheng, M., and X. Jin. 2018. “What Do Airbnb Users Care about? An Analysis of Online Review Comment.” International Journal of Hospitality Management, 76 (A):58–70. https://doi.org/10.1016/j.ijhm.2018.04.004.\n\n\nChiang, T. 2023. “ChatGPT Is a Blurry JPEG of the Web.” The New Yorker. https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web.\n\n\nCima, R. n.d. “The Most and Least Diverse Cities in America.” Priceonomics. https://priceonomics.com/the-most-and-least-diverse-cities-in-america/.\n\n\nClark, J. 2023. “Bidding wars: inside the super-charged fight for rental properties.” The Guardian. https://www.theguardian.com/money/2023/apr/08/bidding-wars-inside-the-super-charged-fight-for-rental-properties.\n\n\nCocola-Gant, A., and A. Gago. 2019. “Airbnb, Buy-to-Let Investment and Tourism-Driven Displacement: A Case Study in Lisbon.” Environment and Planning A: Economy and Space 0 (0):1–18. https://doi.org/10.1177/0308518X19869012.\n\n\nCox, M., and T. Slee. 2016. “How Airbnb’s Data Hid the Facts in New York City.” Inside Airbnb. http://insideairbnb.com/reports/how-airbnbs-data-hid-the-facts-in-new-york-city.pdf.\n\n\nCrawford, K., and M. Finn. 2015. “The Limits of Crisis Data: Analytical and Ethical Challenges of Using Social and Mobile Data to Understand Disasters.” GeoJournal 80 (4):491–502. https://doi.org/10.1007/s10708-014-9597-z.\n\n\nD’Ignazio, Catherine, and Lauren F. Klein. 2020d. “Data Feminism.” In. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\n———. 2020c. “Data Feminism.” In. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\n———. 2020e. “Data Feminism.” In. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\n———. 2020b. “Data Feminism.” In. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\n———. 2020a. Data Feminism. MIT Press. https://data-feminism.mitpress.mit.edu/.\n\n\nDark Matter Labs. 2019. “A Smart Commons: A New Model for INvesting in the Commons.” Medium. https://provocations.darkmatterlabs.org/a-smart-commons-528f4e53cec2.\n\n\nDelmelle, Elizabeth C, and Isabelle Nilsson. 2021. “The Language of Neighborhoods: A Predictive-Analytical Framework Based on Property Advertisement Text and Mortgage Lending Data.” Computers, Environment and Urban Systems 88. Elsevier:101658. https://doi.org/10.1016/j.compenvurbsys.2021.101658.\n\n\nDonoho, D. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4):745–66. https://doi.org/10.1007/978-3-642-23430-9_71.\n\n\nElwood, S., and A. Leszczynski. 2018. “Feminist Digital Geographies.” Gender, Place and Culture 25 (5):629–44. https://doi.org/10.1080/0966369X.2018.1465396.\n\n\nElwood, S., and M. Wilson. 2017. “Critical GIS Pedagogies Beyond ‘Week 10: Ethics‘.” International Journal of Geographical Information Science 31 (10):2098–2116. https://doi.org/10.1080/13658816.2017.1334892.\n\n\nErt, E., A. Fleischer, and N. Magen. 2016. “Trust and Reputation in the Sharing Economy: The Role of Personal Photos in Airbnb.” Tourism Management, 55:62–63. https://doi.org/10.1016/j.tourman.2016.01.013.\n\n\nEtherington, Thomas R. 2016. “Teaching introductory GIS programming to geographers using an open source Python approach.” Journal of Geography in Higher Education 40 (1). Taylor & Francis:117–30. https://doi.org/10.1080/03098265.2015.1086981.\n\n\nEugenio-Martin, J. L., J. M. Cazorla-Artiles, and C. Gonzàlez-Martel. 2019. “On the Determinants of Airbnb Location and Its Spatial Distribution.” Tourism Economics 25 (8):1224–24. https://doi.org/10.1177/1354816618825415.\n\n\nFerreri, Mara, and Romola Sanyal. 2018. “Platform Economies and Urban Planning: Airbnb and Regulated Deregulation in London.” Urban Studies 55 (15):3353–68. https://doi.org/10.1177/0042098017751982.\n\n\nFitzpatrick, B., and B. Collins-Sussman. n.d. “The Myth of the ’Genius Programmer’.” Google. https://www.youtube.com/watch?v=0SARbwvhupQ.\n\n\nFranklin, Rachel. 2024. “Quantitative methods III: Strength in numbers?” Progress in Human Geography 48 (2). SAGE Publications Sage UK: London, England:236–44. https://doi.org/10.1177/03091325231210512.\n\n\nGibbs, C., D. Guttentag, U. Gretzel, J. Morton, and A. Goodwill. 2017. “Pricing in the Sharing Economy: A Hedonic Pricing Model Applied to Airbnb Listings.” Journal of Travel & Tourism Marketing 35 (1):46–56. https://doi.org/10.1080/10548408.2017.1308292.\n\n\nGraham, P. 2022. “Putting Ideas into Words.” 2022. https://paulgraham.com/words.html.\n\n\nGurran, N., and P. Phibbs. 2017. “When Tourists Move in: How Should Urban Planners Respond to Airbnb?” Journal of the American Planning Association 83 (1):80–92. https://doi.org/10.1080/01944363.2016.1249011.\n\n\nGutiérrez, J., J. C. Garcı́a-Palomares, G. Romanillos, and M. H. Salas-Olmedo. 2017. “The Eruption of Airbnb in Tourist Cities: Comparing Spatial Patterns of Hotels and Peer-to-Peer Accommodation in Barcelona.” Tourism Management 62:278–91. https://doi.org/10.1016/j.tourman.2017.05.003.\n\n\nGuttentag, Daniel A., and Stephen L. J. Smith. 2017. “Assessing Airbnb as a Disruptive Innovation Relative to Hotels: Substitution and Comparative Performance Expectations.” International Journal of Hospitality Management 64:1–10. https://doi.org/10.1016/j.ijhm.2017.02.003.\n\n\nHarris, J. 2018. “Profiteers Make a Killing on Airbnb - and Erode Communities.” The Guardian. https://www.theguardian.com/commentisfree/2018/feb/12/profiteers-killing-airbnb-erode-communities.\n\n\nHarris, R. n.d. “The Certain Uncertainty of University Rankings.” RPubs. https://rpubs.com/profrichharris/uni-rankings.\n\n\nHeaven, W. D. 2022. “Why Meta’s Latest Large Language Model Survived Only Three Days Online.” MIT Technology Review. https://www.technologyreview.com/2022/11/18/1063487/meta-large-language-model-ai-only-survived-three-days-gpt-3-science/.\n\n\nHorn, K., and M. Merante. 2017. “Is Home Sharing Driving up Rents? Evidence from Airbnb in Boston.” Journal of Housing Economics 38:14–24. https://doi.org/10.1016/j.jhe.2017.08.002.\n\n\nIqbal, N., and A. Chakrabortty. 2023. “Why are London’s inner-city schools disappearing?” Edited by A. Bransbury. The Guardian. 2023. https://www.theguardian.com/news/audio/2023/apr/26/why-are-london-schools-disappearing-podcast.\n\n\nJolly, J. 2023. “Owners of 100,000 properties held by foreign shell companies unknown despite new UK laws.” The Guardian. https://www.theguardian.com/business/2023/sep/03/owners-of-100000-properties-held-by-foreign-shell-companies-unknown-despite-new-uk-laws.\n\n\nKitchin, R., T. P. Lauriault, and G. McArdie. 2016. “Smart Cities and the Politics of Urban Data.” In Smart Urbanism, edited by McFarlane Marvin Luque-Ayala.\n\n\nKnuth, D. E. 1984. “Literate Programming.” The Computer Journal 27 (2). Oxford University Press:97–111.\n\n\n———. 1996. Selected Papers on Computer Science. Cambridge University Press.\n\n\nLadd, John R. 2020. “Understanding and Using Common Similarity Measures for Text Analysis.” The Programming Historian, no. 9. https://doi.org/10.46430/phen0089.\n\n\nLansley, Guy. 2016. “Cars and Socio-Economics: Understanding Neighbourhood Variations in Car Characteristics from Administrative Data.” Regional Studies, Regional Science 3 (1). Taylor & Francis:264–85. https://doi.org/10.1080/21681376.2016.1177466.\n\n\nLavin, Matthew J. 2019. “Analyzing Documents with TF-IDF.” The Programming Historian, no. 8. https://doi.org/10.46430/phen0082.\n\n\nLee, D. 2016. “How Airbnb Short-Term Rentals Exacerbate Los Angeles’s Affordable Housing Crisis: Analysis and Policy Recommendations.” Harvard Law & Policy Review 10 (1):229–54. https://doi.org/https://heinonline.org/HOL/Page?handle=hein.journals/harlpolrv10&div=13&g_sent=1.\n\n\nLu, Yonggang, and Kevin SS Henning. 2013. “Are statisticians cold-blooded bosses? a new perspective on the ’old’ concept of statistical population.” Teaching Statistics 35 (1). Wiley Online Library:66–71. https://doi.org/10.1111/j.1467-9639.2012.00524.x.\n\n\nLutz, C., and G. Newlands. 2018. “Consumer Segmentation Within the Sharing Economy: The Case of Airbnb.” Journal of Business Research 88:187–96. https://doi.org/10.1016/j.jbusres.2018.03.019.\n\n\nMa, X., J. T. Hancock, K. L. Mingjie, and M. Naaman. 2017. “Self-Disclosure and Perceived Trustworthiness of Airbnb Host Profiles.” CSCW’17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computation, 2397–2409. https://doi.org/10.1145/2998181.2998269.\n\n\nMassey, Doreen. 1996. “Politicising Space and Place.” Scottish Geographical Magazine 112 (2). Routledge:117–23. https://doi.org/10.1080/14702549608554458.\n\n\nMattern, Shannon. 2015. “Mission control: A history of the urban dashboard.” Places Journal. https://doi.org/10.22269/150309.\n\n\n———. 2017. “A City Is Not a Computer.” Places Journal. https://doi.org/10.22269/170207.\n\n\nMcCullough, D. 2002. “Interview with NEH chairman Bruce Cole.” Humanities Magazine.\n\n\nMiller, Harvey J, and Michael F Goodchild. 2015. “Data-Driven Geography.” GeoJournal 80. Springer:449–61. https://doi.org/10.1007/s10708-014-9602-6.\n\n\nMinton, A. 2023. “New York is breaking free of Airbnb’s clutches. This is how the rest of the world can follow suit.” The Guardian. https://www.theguardian.com/commentisfree/2023/sep/27/new-york-airbnb-renters-cities-law-ban-properties.\n\n\nMuller, C. L., and C. Kidd. 2014. “Debugging Geographers: Teaching Programming to Non-Computer Scientists.” Journal of Geography in Higher Education 38 (2). Taylor & Francis:175–92. https://doi.org/10.1080/03098265.2014.908275.\n\n\nNeate, R. 2023. “‘This is where people with staggering wealth end up’: who will buy Britain’s most expensive house?” The Guardian. https://www.theguardian.com/money/2023/apr/08/britain-most-expensive-house-rutland-gate-mansion-london-super-rich-buyer.\n\n\nO’Sullivan, David, and Steven M Manson. 2015. “Do physicists have geography envy? And what can geographers learn from it?” Annals of the Association of American Geographers 105 (4). Taylor & Francis:704–22. https://doi.org/10.1080/00045608.2015.1039105.\n\n\nOpen Data Institute. n.d. “SOD0009 - Evidence on Statistics and Open Data.” https://committees.parliament.uk/writtenevidence/45220/pdf/.\n\n\nPrat, Chantel S, Tara M Madhyastha, Malayka J Mottarella, and Chu-Hsuan Kuo. 2020. “Relating Natural Language Aptitude to Individual Differences in Learning Programming Languages.” Scientific Reports 10 (1). Nature Publishing Group UK London:3817. https://www.nature.com/articles/s41598-020-60661-8.\n\n\nQuattrone, G., A. Greatorex, D. Quercia, L. Capra, and M. Musolesi. 2018. “Analyzing and Predicting the Spatial Penetration of Airbnb in u.s. Cities.” EPJ Data Science 7 (31). https://doi.org/10.1140/epjds/s13688-018-0156-6.\n\n\nQuattrone, Giovanni, Davide Proserpio, Daniele Quercia, Licia Capra, and Mirco Musolesi. 2016. “Who Benefits from the ‘Sharing’ Economy of Airbnb?” In Proceedings of the 25th International Conference on World Wide Web, 1385–94. WWW ’16. Republic; Canton of Geneva, CHE: International World Wide Web Conferences Steering Committee. https://doi.org/10.1145/2872427.2874815.\n\n\nReades, Jonathan, and Jennie Williams. 2023. “Clustering and Visualising Documents Using Word Embeddings.” Programming Historian. https://doi.org/10.46430/phen0111.\n\n\nReades, J., H. Yingjie, Emmanouil Tranos, and E. Delmelle. in review. “The City as Text: Understanding and Modeling Cities through the Lens of Text.” NA, in review.\n\n\nRose, Gillian. 1997. “Situating Knowledges: Positionality, Reflexivities and Other Tactics.” Progress in Human Geography 21 (3):305–20. https://doi.org/10.1191/030913297673302122.\n\n\nScheider, Simon, Enkhbold Nyamsuren, Han Kruiger, and Haiqi Xu. 2020. “Why Geographic Data Science Is Not a Science.” Geography Compass 14 (11). Wiley Online Library:e12537.\n\n\nShabrina, Z., E. Arcaute, and M. Batty. 2019. “Airbnb’s Disruption of the Housing Structure in London.” ArXiv Prepring. University College London. https://arxiv.org/pdf/1903.11205.pdf.\n\n\nShabrina, Z., Y. Zhang, E. Arcaute, and M. Batty. 2017. “Beyond Informality: The Rise of Peer-to-Peer (P2P) Renting.” CASA Working Paper 209. University College London. https://www.ucl.ac.uk/bartlett/casa/case-studies/2017/mar/casa-working-paper-209.\n\n\nShapiro, W., and M. Yavuz. 2017. “Rethinking ’distance’ in New York City.” Medium. https://medium.com/topos-ai/rethinking-distance-in-new-york-city-d17212d24919.\n\n\nSingleton, Alex, and Daniel Arribas-Bel. 2021. “Geographic Data Science.” Geographical Analysis 53 (1):61–75. https://doi.org/10.1111/gean.12194.\n\n\nSmith, D. 2010. “Valuing housing and green spaces: Understanding local amenities, the built environment and house prices in London.” GLA Economics. https://www.centreforlondon.org/wp-content/uploads/2016/08/CFLJ4292-London-Inequality-04_16_WEB_V4.pdf.\n\n\nSthapit, Erose, and Peter Björk. 2019. “Sources of Distrust: Airbnb Guests’ Perspectives.” Tourism Management Perspectives 31:245–53. https://doi.org/10.1016/j.tmp.2019.05.009.\n\n\nStrauß, Stefan. 2015. “Datafication and the Seductive Power of Uncertainty–a Critical Exploration of Big Data Enthusiasm.” Information 6 (4). MDPI:836–47.\n\n\nSusnjara, S., and I. Smalley. 2025. “What Is Virtualization?” 2025. https://www.ibm.com/think/topics/virtualization.\n\n\nTravers, Tony, Sam Sims, and Nicolas Bosetti. 2016. “Housing and Inequality in London.” Centre for London. https://www.centreforlondon.org/wp-content/uploads/2016/08/CFLJ4292-London-Inequality-04_16_WEB_V4.pdf.\n\n\nUnwin, David. 1980. “Make Your Practicals Open-Ended.” Journal of Geography in Higher Education 4 (2). Taylor & Francis:39–42. https://doi.org/10.1080/03098268008708772.\n\n\nVanderPlas, Jake. 2014. “Is Seattle Really Seeing an Uptick in Cycling?” http://jakevdp.github.io/blog/2014/06/10/is-seattle-really-seeing-an-uptick-in-cycling/.\n\n\nWachsmuth, D., D. Chaney, D. Kerrigan, A. Shillolo, and R. Basalaev-Binder. 2018. “The High Cost of Short-Term Rentals in New York City.” McGill University. https://www.mcgill.ca/newsroom/files/newsroom/channels/attach/airbnb-report.pdf.\n\n\nWachsmuth, D., and A. Weisler. 2018. “Airbnb and the Rent Gap: Gentrification Through the Sharing Economy.” Environment and Planning A: Economy and Space 50 (6):1147–70. https://doi.org/10.1177/0308518X18778038.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1):160018. https://doi.org/10.1038/sdata.2016.18.\n\n\nWolf, Levi John, Sean Fox, Rich Harris, Ron Johnston, Kelvyn Jones, David Manley, Emmanouil Tranos, and Wenfei Winnie Wang. 2021. “Quantitative Geography III: Future Challenges and Challenging Futures.” Progress in Human Geography 45 (3). SAGE Publications Sage UK: London, England:596–608. https://doi.org/10.1177/0309132520924722.\n\n\nWolfe, C. R. 2023b. “Program-Aided Language Models.” https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms.\n\n\n———. 2023a. “Program-Aided Language Models.” https://medium.com/data-science/program-aided-language-models-93d226c7d9a0.\n\n\nXiao, Ningchuan. 2016. GIS Algorithms: Theory and Applications for Geographic Information Science & Technology. Research Methods. SAGE. https://doi.org/https://dx.doi.org/10.4135/9781473921498.\n\n\nXie, Tessa. 2024a. “How to Better Communicate as a Data Scientist.” Towards Data Science. https://www.divingintodata.com/p/how-to-better-communicate-as-a-data-scientist-6fc5428d3143.\n\n\n———. 2024c. “The Most Undervalued Skill for Data Scientists.” 2024. https://towardsdatascience.com/the-most-undervalued-skill-for-data-scientists-e0e0d7709321/.\n\n\n———. 2024b. “The Most Undervalued Skill for Data Scientists.” Towards Data Science. https://towardsdatascience.com/the-most-undervalued-skill-for-data-scientists-e0e0d7709321.\n\n\nZemanek, H. 1983. “Algorithmic Perfection.” Annals of the History of Computing. AMER FED INFORM PROCESSING SOC.\n\n\nZervas, Georgios, Davide Proserpio, and John W Byers. 2021. “A First Look at Online Reputation on Airbnb, Where Every Stay Is Above Average.” Marketing Letters 32. Springer:1–16.\n\n\nZervas, G., D. Proserpio, and J. Byers. 2015. “A First Look at Online Reputation on Airbnb, Where Every Stay Is Above Average.” SSRN. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2554500."
  },
  {
    "objectID": "bib/Template.html",
    "href": "bib/Template.html",
    "title": "Summary of Reading",
    "section": "",
    "text": "Is it an academic paper, technical report, blogpost, etc.? And what kind of contribution is trying to make? Conceptual, theoretical, empirical or something else?"
  },
  {
    "objectID": "bib/Template.html#what-kind-of-reading-is-it",
    "href": "bib/Template.html#what-kind-of-reading-is-it",
    "title": "Summary of Reading",
    "section": "",
    "text": "Is it an academic paper, technical report, blogpost, etc.? And what kind of contribution is trying to make? Conceptual, theoretical, empirical or something else?"
  },
  {
    "objectID": "bib/Template.html#who-is-the-intended-audience",
    "href": "bib/Template.html#who-is-the-intended-audience",
    "title": "Summary of Reading",
    "section": "2 Who is the intended audience?",
    "text": "2 Who is the intended audience?\n\nIs it intended for academics, field experts, the general public? etc.? For example, if you think this is intended for someone who needs to learn new skills or someone who is in a policy-making position how do you know this?"
  },
  {
    "objectID": "bib/Template.html#how-is-the-piece-structured",
    "href": "bib/Template.html#how-is-the-piece-structured",
    "title": "Summary of Reading",
    "section": "3 How is the piece structured?",
    "text": "3 How is the piece structured?\n\nBriefly, how is it organised in terms of headings, sub-headings, sections, etc.? Can you explain this structure responds to the kind of reading and type of audience?"
  },
  {
    "objectID": "bib/Template.html#what-are-the-key-ideas-concepts-or-theories-discussed",
    "href": "bib/Template.html#what-are-the-key-ideas-concepts-or-theories-discussed",
    "title": "Summary of Reading",
    "section": "4 What are the key ideas, concepts, or theories discussed?",
    "text": "4 What are the key ideas, concepts, or theories discussed?\n\nBriefly identify the specific areas that the contribution engages with. How do you know this?"
  },
  {
    "objectID": "bib/Template.html#what-is-the-overall-contribution",
    "href": "bib/Template.html#what-is-the-overall-contribution",
    "title": "Summary of Reading",
    "section": "5 What is the overall contribution?",
    "text": "5 What is the overall contribution?\n\nWhat does it build on or what gap does it respond to? What are the key findings or conclusions?"
  },
  {
    "objectID": "bib/Template.html#what-issues-or-gaps-remain",
    "href": "bib/Template.html#what-issues-or-gaps-remain",
    "title": "Summary of Reading",
    "section": "6 What issues or gaps remain?",
    "text": "6 What issues or gaps remain?\n\nIf relevant, are there assumptions that might not hold in other contexts? Can you think of other case studies or contexts where the reading would apply and explain why? If you think the contribution is generally valid can you explain why? Are there areas for future work identified?"
  },
  {
    "objectID": "lectures/1.4-tools.html#jupyterlab-python",
    "href": "lectures/1.4-tools.html#jupyterlab-python",
    "title": "Our Tools",
    "section": "JupyterLab + Python",
    "text": "JupyterLab + Python"
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#from-files-to-data",
    "href": "lectures/2.1-Think_Data.html#from-files-to-data",
    "title": "Think Data",
    "section": "From Files to Data",
    "text": "From Files to Data\nIn order to read a file you need to know a few things:\n\nWhat distinguishes one record from another?\nWhat distinguishes one field from another?\nWhat ensures that a field or record is valid?\nDoes the data have row or column names?\nIs there metadata?"
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#structure-of-a-tabular-data-file",
    "href": "lectures/2.1-Think_Data.html#structure-of-a-tabular-data-file",
    "title": "Think Data",
    "section": "Structure of a Tabular Data File",
    "text": "Structure of a Tabular Data File\nRow and column names (indexes) make it a lot easier to find and refer to data but they are not data and don’t belong in the data set itself.\nOften, one record (a.k.a. observation) finishes and the next one starts with a ‘newline’ (\\n) or ‘carriage return’ (\\r) or both (\\r\\n) but it could be anything (e.g. EOR).\nOften, one field (a.k.a. attribute or value) finishes and the next one starts with a comma (,), but it could be anything (e.g. ; or | or EOF).\n\nHow would we choose a good field separator?\nPro tip: if we store column and row names separately from the data then we can access everything easily without having to factor in any ‘special’ values!\nNoice also the nd here. This is the escape sequence again that you also encountered when dealing with the Shell as well. Remember that \\ is necessary if you have a space in your file name or path."
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#most-common-formats",
    "href": "lectures/2.1-Think_Data.html#most-common-formats",
    "title": "File Formats",
    "section": "Most Common Formats",
    "text": "Most Common Formats\n\n\n\n\n\n\n\n\n\nExtension\nField Separator\nRecord Separator\nPython Package\n\n\n\n\n.csv\n, but separator can appear in fields enclosed by \".\n\\n but could be \\r or \\r\\n.\ncsv\n\n\n.tsv or .tab\n\\t and unlikely to appear in fields.\n\\n but could be \\r or \\r\\n.\ncsv (!)\n\n\n.xls or .xlsx\nBinary, you need a library to read.\nBinary, you need a library to read.\nxlrd/xlsxwriter\n\n\n.sav or .sas\nBinary, you need a library to read.\nBinary, you need a library to read.\npyreadstat\n\n\n.json, .geojson\nComplex (,, [], {}), but plain text.\nComplex (,, [], {}), but plain text\njson, geojson\n\n\n.feather\nBinary, you need a library to read.\nBinary, you need a library to read.\npyarrow, geofeather\n\n\n.parquet\nBinary, you need a library to read.\nBinary, you need a library to read.\npyarrow\n\n\n\n\nOne of the reasons we like CSV and TSV files is that they can be opened and interacted with using the Command Line (as well as Excel/Numbers/etc.) directly. As soon as you get into binary file formats you either need the original tool (and then export) or you need a tool that can read those formats. So the complexity level rises very quickly.\nOf course, sometimes you can gain (e.g. SPSS or SAS) in terms of obtaining information about variable types, levels, etc. but usually you use these when that’s all that’s available or when you want to write a file for others to use.\nThe two formats at the bottom of the table are there because they are useful: the feather format was designed for fast reads and for data interachange with R, while Parquet is a highly-compressed, column-oriented storage format for large data. So for modest-sized data sets (a few hundred MB), or situations where you are working across R and Python, then Feather cannot be beat. For ‘big data’ where you need access to parts of the data set and want to do lazy loading, then parquet is the winner."
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#mapping-data-types",
    "href": "lectures/2.1-Think_Data.html#mapping-data-types",
    "title": "Think Data",
    "section": "‘Mapping’ Data Types",
    "text": "‘Mapping’ Data Types\nYou will often hear the term ‘mapping’ used in connection to data that is not spatial, what do they mean?\nHere’s a mapping:\n\n\n\n\n\n\n\nInput (e.g. Excel)\nOutput (e.g. Python)\n\n\n\n\nNULL, N/A, “”\nNone or np.nan\n\n\n0..n\nint\n\n\n0.00…n\nfloat\n\n\nTrue/False, Y/N, 1/0\nbool\n\n\nR, G, B (etc.)\nint or str (technically a set, but hard to use with data sets)\n\n\n‘Jon Reades’, ‘Huanfa Chen’, etc.\nstr\n\n\n‘3-FEB-2020’, ‘10/25/20’, etc.\ndatetime module (date, datetime or time)\n\n\n\n\nThese would be a mapping of variables between two formats. We talk of mapping any time we are taking inputs from one data set/format/data structure as a lookup for use with another data set/format/data structure.\nHave a think about how you can use an int to represent nominal data. There are two ways: one of which will be familiar to students who have taken a stats class (with regression) and one of which is more intuitive to ‘normal’ users…"
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#testing-a-mapping",
    "href": "lectures/2.1-Think_Data.html#testing-a-mapping",
    "title": "File Formats",
    "section": "Testing a Mapping",
    "text": "Testing a Mapping\nWorking out an appropriate mapping (representation of the data) is hugely time-consuming.\n\nIt’s commonly held that 80% of data science is data cleaning.\n\nThe Unix utilities (grep, awk, tail, head) can be very useful for quickly exploring the data in order to develop a basic understanding of the data and to catch obvious errors.\nYou should never assume that the data matches the spec."
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#label-these",
    "href": "lectures/2.1-Think_Data.html#label-these",
    "title": "File Formats",
    "section": "Label These",
    "text": "Label These\n\n\nMetadata is relevant to our understanding of the data and so is important, but it’s not relevant to treating the data as data so we need to be able to skip it.\nColumn names are going to be how we access a given attribute for each observation.\nRow names are not normally data themselves, but are basically labels or identifiers for observations. Another term for this would be the data index.\nIf we store row and column names/indices separately from the data then we don’t have to treat them as ‘special’ or factor them into, for example, the calculation of summary stats.\nAlso have to consider trade-offs around mapping the full column names on to something a little faster and easier to type!"
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#things-that-can-go-wrong",
    "href": "lectures/2.1-Think_Data.html#things-that-can-go-wrong",
    "title": "Think Data",
    "section": "Things That Can Go Wrong…",
    "text": "Things That Can Go Wrong…\nA selection of real issues I’ve seen in my life:\n\nTruncation: server ran out of diskspace or memory, or a file transfer was interrupted.\nTranslation: headers don’t line up with data.\nSwapping: column order differs from spec.\nIncompleteness: range of real values differs from spec.\nCorruption: field delimitters included in field values.\nErrors: data entry errors resulted in incorrect values or the spec is downright wrong.\nIrrelevance: fields that simply aren’t relevant to your analysis.\n\n\nThese will generally require you to engage with columns and rows (via sampling) on an individual level."
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#arrow-and-parquet",
    "href": "lectures/2.1-Think_Data.html#arrow-and-parquet",
    "title": "File Formats",
    "section": "Arrow and Parquet",
    "text": "Arrow and Parquet\n\nArrow is an in-memory columnar format for data. Data is stored in a structured way in RAM making it blazingly fast for operations.\nParquet is a highly-compressed columnar file format for data. Data is stored in a structured way on your hard drive.\n\nTL;DR: for most applications Parquet will give nice, small files on disk and the benefits of columnar file storage. These are then easily loaded as Arrows."
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#what-about-the-duck",
    "href": "lectures/2.1-Think_Data.html#what-about-the-duck",
    "title": "File Formats",
    "section": "What About the Duck?",
    "text": "What About the Duck?\n\n\n\n\n\nServerless SQL queries against Parquet files\nQueries returned as Pandas data frames\nSelect and filter before loading\nFast conversion between CSV and Parquet via Arrow"
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#for-later-reference",
    "href": "lectures/2.1-Think_Data.html#for-later-reference",
    "title": "File Formats",
    "section": "For (Later) Reference",
    "text": "For (Later) Reference\n# Notice the engine and dtype_backend options\ndf = pandas.read_csv(fname, engine='pyarrow', \n                dtype_backend='pyarrow')\n\n# And for parquet files\ndf = pandas.read_parquet(fname, columns=[...])\n\n# And for DuckDB we can actually joing two\n# files before they even get to Python!\nq = f'''\n  SELECT * FROM \n    read_parquet('epc-ppd-2022-*.parquet') as ppd, \n    read_parquet('epc-ldd-2022-*.parquet') as ldd,\n  WHERE ppd.uid=ldd.uid\n'''\ndf = duckdb.query(q).df()\nP.S. There’s also a command-line tool for DuckDB so you don’t even need Python."
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#resources",
    "href": "lectures/2.1-Think_Data.html#resources",
    "title": "Think Data",
    "section": "Resources",
    "text": "Resources\n\n\n\nUnderstanding Directories and Subdirectories\nReading and writing files\nWorking with OS path utilities\nFiles and file writing\nUsing file system shell methods\nOpening files\n\n\n\nText vs. binary mode\nText files\npetl\npandas 2.0 and the Arrow revolution (Part 1)\nWhat parquet files are my preferred API for bulk open data\nDuckDB Documentation"
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#is-this-data",
    "href": "lectures/2.1-Think_Data.html#is-this-data",
    "title": "File Formats",
    "section": "Is This Data?",
    "text": "Is This Data?"
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#how-about-this",
    "href": "lectures/2.1-Think_Data.html#how-about-this",
    "title": "File Formats",
    "section": "How About This?",
    "text": "How About This?\n\n\n\n\n\n\n\n\n\ncity\nfamous_for\nsequence\n\n\n\n\n0\nJaipur\nAmber Fort\n4\n\n\n1\nDelhi\nTaj Mahal\n1\n\n\n2\nAgra\nAgra Fort\n2\n\n\n3\nRanthambore\nTigers!\n3"
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#which-of-these-is-data",
    "href": "lectures/2.1-Think_Data.html#which-of-these-is-data",
    "title": "Think Data",
    "section": "Which of These is Data?",
    "text": "Which of These is Data?\n\n\n\n\ncity,famous_for,sequence\nJaipur,Amber Fort,4\nDelhi,Taj Mahal,1\nAgra,Agra Fort,2\nRanthambore,Tigers!,3\n\n\n\n\n\n\n\n\n\n\n\n\ncity\nfamous_for\nsequence\n\n\n\n\n0\nJaipur\nAmber Fort\n4\n\n\n1\nDelhi\nTaj Mahal\n1\n\n\n2\nAgra\nAgra Fort\n2\n\n\n3\nRanthambore\nTigers!\n3"
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#with-labels",
    "href": "lectures/2.1-Think_Data.html#with-labels",
    "title": "Think Data",
    "section": "With Labels",
    "text": "With Labels\n\n\nMetadata is relevant to our understanding of the data and so is important, but it’s not relevant to treating the data as data so we need to be able to skip it.\nColumn names are going to be how we access a given attribute for each observation.\nRow names are not normally data themselves, but are basically labels or identifiers for observations. Another term for this would be the data index.\nIf we store row and column names/indices separately from the data then we don’t have to treat them as ‘special’ or factor them into, for example, the calculation of summary stats.\nAlso have to consider trade-offs around mapping the full column names on to something a little faster and easier to type!"
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#is-geo-data-any-different",
    "href": "lectures/2.1-Think_Data.html#is-geo-data-any-different",
    "title": "File Formats",
    "section": "Is Geo-Data Any Different?",
    "text": "Is Geo-Data Any Different?\n\n\nYes if you are using ESRI’s Shapefiles.\nNo if you are using any other format."
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#section",
    "href": "lectures/2.1-Think_Data.html#section",
    "title": "File Formats",
    "section": "",
    "text": "import geopandas as gpd\ngdf = gpd.read_parquet('../data/raw/Jaipur_Wards.geoparquet')\ngdf.head(3)\n\n\n\n\n\n\n\n\nId\nAREA\nWard_No\nPOP\nDENS_PPH\nNAME\ngeometry\nArea\nWard_Numbe\n\n\n\n\n0\n0\n17.0\n75.0\n12031\n708\nKISHANPOLE\nPOLYGON ((582674.835 2978151.401, 582656.683 2...\nNaN\nNaN\n\n\n1\n0\n42.0\n55.0\n13503\n322\nKISHANPOLE\nPOLYGON ((580007.419 2979496.228, 579968.156 2...\nNaN\nNaN\n\n\n2\n0\n25.0\n56.0\n13925\n557\nKISHANPOLE\nPOLYGON ((580474.073 2979166.057, 580474.245 2...\nNaN\nNaN"
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#geo-data-tables",
    "href": "lectures/2.1-Think_Data.html#geo-data-tables",
    "title": "Think Data",
    "section": "Geo-Data Tables",
    "text": "Geo-Data Tables\n\nimport geopandas as gpd\ngdf = gpd.read_parquet('../data/clean/Jaipur_Wards.geoparquet')\ngdf.head(3)\n\n\n\n\n\n\n\n\nId\nAREA\nWard_No\nPOP\nDENS_PPH\nNAME\ngeometry\nArea\nWard_Numbe\n\n\n\n\n0\n0\n781.0\n99.0\n12356\n16\nADARSH NAGAR\nPOLYGON ((588147.873 2977077.51, 588066.155 29...\nNaN\nNaN\n\n\n1\n0\n234.0\n76.0\n14120\n60\nADARSH NAGAR\nMULTIPOLYGON (((585516.609 2980880.352, 585516...\nNaN\nNaN\n\n\n2\n0\n18.0\n77.0\n13879\n771\nADARSH NAGAR\nPOLYGON ((583885.577 2978737.637, 583942.867 2...\nNaN\nNaN"
  },
  {
    "objectID": "practicals/Report.html",
    "href": "practicals/Report.html",
    "title": "A Project",
    "section": "",
    "text": "Some content here."
  },
  {
    "objectID": "practicals/Report.html#declaration-of-authorship",
    "href": "practicals/Report.html#declaration-of-authorship",
    "title": "A Project",
    "section": "",
    "text": "Some content here."
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#testing-testing",
    "href": "lectures/2.1-Think_Data.html#testing-testing",
    "title": "Think Data",
    "section": "Testing, Testing",
    "text": "Testing, Testing\n\nYou should never assume that the data matches the spec."
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#why-this-isnt-easy",
    "href": "lectures/2.1-Think_Data.html#why-this-isnt-easy",
    "title": "Think Data",
    "section": "Why This Isn’t Easy",
    "text": "Why This Isn’t Easy\n\n\nHere’s raw Excel data.\nWhat would we say the row and column names currently are?"
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#how-about-in-india",
    "href": "lectures/2.1-Think_Data.html#how-about-in-india",
    "title": "Think Data",
    "section": "How About in India?",
    "text": "How About in India?"
  },
  {
    "objectID": "lectures/2.1-Think_Data.html#some-common-formats",
    "href": "lectures/2.1-Think_Data.html#some-common-formats",
    "title": "Think Data",
    "section": "Some Common Formats",
    "text": "Some Common Formats\n\n\n\n\n\n\n\n\n\nExtension\nField Separator\nRecord Separator\nPython Package\n\n\n\n\n.csv\n, but separator can appear in fields enclosed by \".\n\\n but could be \\r or \\r\\n.\ncsv\n\n\n.tsv or .tab\n\\t and unlikely to appear in fields.\n\\n but could be \\r or \\r\\n.\ncsv (!)\n\n\n.xls or .xlsx\nBinary, you need a library to read.\nBinary, you need a library to read.\nxlrd/xlsxwriter\n\n\n.sav or .sas\nBinary, you need a library to read.\nBinary, you need a library to read.\npyreadstat\n\n\n.json, .geojson\nComplex (,, [], {}), but plain text.\nComplex (,, [], {}), but plain text\njson, geojson\n\n\n.shp\nBinary, you need a library to read. Need at least 3 parts (shp,dbf,shx)!\nBinary, you need a library to read.\ngeopandas, fiona\n\n\n.feather\nBinary, you need a library to read.\nBinary, you need a library to read.\npyarrow, geofeather\n\n\n.parquet\nBinary, you need a library to read.\nBinary, you need a library to read.\npyarrow\n\n\n\n\nOne of the reasons we like CSV and TSV files is that they can be opened and interacted with using the Command Line (as well as Excel/Numbers/etc.) directly. As soon as you get into binary file formats you either need the original tool (and then export) or you need a tool that can read those formats. So the complexity level rises very quickly.\nOf course, sometimes you can gain (e.g. SPSS or SAS) in terms of obtaining information about variable types, levels, etc. but usually you use these when that’s all that’s available or when you want to write a file for others to use.\nThe two formats at the bottom of the table are there because they are useful: the feather format was designed for fast reads and for data interachange with R, while Parquet is a highly-compressed, column-oriented storage format for large data. So for modest-sized data sets (a few hundred MB), or situations where you are working across R and Python, then Feather cannot be beat. For ‘big data’ where you need access to parts of the data set and want to do lazy loading, then parquet is the winner."
  },
  {
    "objectID": "lectures/2.2-Data.html#why-pandas",
    "href": "lectures/2.2-Data.html#why-pandas",
    "title": "Pandas",
    "section": "Why Pandas?",
    "text": "Why Pandas?\nPandas is probably (together with scipy, numpy, and sklearn) the main reason that Python has become popular for data science. According to ‘Learn Data Sci’ it accounts for 1% of all Stack Overflow question views!\nYou will want to bookmark these:\n\npandas.pydata.org\nPandas Docs\npandas tutorial for beginners"
  },
  {
    "objectID": "lectures/2.2-Data.html#pandas-terminology-data-frame",
    "href": "lectures/2.2-Data.html#pandas-terminology-data-frame",
    "title": "Pandas",
    "section": "Pandas Terminology (Data Frame)",
    "text": "Pandas Terminology (Data Frame)"
  },
  {
    "objectID": "lectures/2.2-Data.html#pandas-terminology-index",
    "href": "lectures/2.2-Data.html#pandas-terminology-index",
    "title": "Pandas",
    "section": "Pandas Terminology (Index)",
    "text": "Pandas Terminology (Index)"
  },
  {
    "objectID": "lectures/2.2-Data.html#pandas-terminology-series",
    "href": "lectures/2.2-Data.html#pandas-terminology-series",
    "title": "Pandas",
    "section": "Pandas Terminology (Series)",
    "text": "Pandas Terminology (Series)"
  },
  {
    "objectID": "lectures/2.2-Data.html#pandas-terminology-slice",
    "href": "lectures/2.2-Data.html#pandas-terminology-slice",
    "title": "Pandas",
    "section": "Pandas Terminology (Slice)",
    "text": "Pandas Terminology (Slice)"
  },
  {
    "objectID": "lectures/2.2-Data.html#using-pandas",
    "href": "lectures/2.2-Data.html#using-pandas",
    "title": "Pandas",
    "section": "Using Pandas",
    "text": "Using Pandas\nHere’s code to read a (remote) CSV file:\nimport pandas as pd         # import package\nurl='https://orca.casa.ucl.ac.uk/~jreades/jaipur/population.csv.gz'\ndf = pd.read_csv(url)       # load a (remote) CSV\nprint(type(df))             # not a 'simple' data type\nprint(df.columns.to_list()) # column names\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n['State', 'District', 'Subdistt', 'Town/Village', 'Ward', 'EB', 'Level', 'Name', 'TRU', 'No_HH', 'TOT_P', 'TOT_M', 'TOT_F', 'P_06', 'M_06', 'F_06', 'P_SC', 'M_SC', 'F_SC', 'P_ST', 'M_ST', 'F_ST', 'P_LIT', 'M_LIT', 'F_LIT', 'P_ILL', 'M_ILL', 'F_ILL', 'TOT_WORK_P', 'TOT_WORK_M', 'TOT_WORK_F', 'MAINWORK_P', 'MAINWORK_M', 'MAINWORK_F', 'MAIN_CL_P', 'MAIN_CL_M', 'MAIN_CL_F', 'MAIN_AL_P', 'MAIN_AL_M', 'MAIN_AL_F', 'MAIN_HH_P', 'MAIN_HH_M', 'MAIN_HH_F', 'MAIN_OT_P', 'MAIN_OT_M', 'MAIN_OT_F', 'MARGWORK_P', 'MARGWORK_M', 'MARGWORK_F', 'MARG_CL_P', 'MARG_CL_M', 'MARG_CL_F', 'MARG_AL_P', 'MARG_AL_M', 'MARG_AL_F', 'MARG_HH_P', 'MARG_HH_M', 'MARG_HH_F', 'MARG_OT_P', 'MARG_OT_M', 'MARG_OT_F', 'MARGWORK_3_6_P', 'MARGWORK_3_6_M', 'MARGWORK_3_6_F', 'MARG_CL_3_6_P', 'MARG_CL_3_6_M', 'MARG_CL_3_6_F', 'MARG_AL_3_6_P', 'MARG_AL_3_6_M', 'MARG_AL_3_6_F', 'MARG_HH_3_6_P', 'MARG_HH_3_6_M', 'MARG_HH_3_6_F', 'MARG_OT_3_6_P', 'MARG_OT_3_6_M', 'MARG_OT_3_6_F', 'MARGWORK_0_3_P', 'MARGWORK_0_3_M', 'MARGWORK_0_3_F', 'MARG_CL_0_3_P', 'MARG_CL_0_3_M', 'MARG_CL_0_3_F', 'MARG_AL_0_3_P', 'MARG_AL_0_3_M', 'MARG_AL_0_3_F', 'MARG_HH_0_3_P', 'MARG_HH_0_3_M', 'MARG_HH_0_3_F', 'MARG_OT_0_3_P', 'MARG_OT_0_3_M', 'MARG_OT_0_3_F', 'NON_WORK_P', 'NON_WORK_M', 'NON_WORK_F']"
  },
  {
    "objectID": "lectures/2.2-Data.html#summarise-a-data-frame",
    "href": "lectures/2.2-Data.html#summarise-a-data-frame",
    "title": "Pandas",
    "section": "Summarise a Data Frame",
    "text": "Summarise a Data Frame\n\n\nStatistical summarisation.\n\ndf.describe()\n\n\n\n\n\n\n\n\nState\nDistrict\nSubdistt\nTown/Village\nWard\nEB\nNo_HH\nTOT_P\nTOT_M\nTOT_F\n...\nMARG_AL_0_3_F\nMARG_HH_0_3_P\nMARG_HH_0_3_M\nMARG_HH_0_3_F\nMARG_OT_0_3_P\nMARG_OT_0_3_M\nMARG_OT_0_3_F\nNON_WORK_P\nNON_WORK_M\nNON_WORK_F\n\n\n\n\ncount\n2561.0\n2561.0\n2561.000000\n2561.000000\n2561.000000\n2561.0\n2.561000e+03\n2.561000e+03\n2.561000e+03\n2.561000e+03\n...\n2561.000000\n2561.000000\n2561.000000\n2561.000000\n2561.000000\n2561.000000\n2561.000000\n2.561000e+03\n2.561000e+03\n2.561000e+03\n\n\nmean\n8.0\n110.0\n543.976962\n169642.691527\n2.311207\n0.0\n2.561084e+03\n1.430145e+04\n7.489366e+03\n6.812087e+03\n...\n13.509567\n8.444748\n3.001171\n5.443577\n66.545881\n27.736431\n38.809449\n9.049970e+03\n3.776792e+03\n5.273178e+03\n\n\nstd\n0.0\n0.0\n19.015389\n239933.613990\n8.628916\n0.0\n3.293322e+04\n1.815688e+05\n9.511259e+04\n8.645723e+04\n...\n189.164285\n106.745661\n38.197017\n69.027967\n851.119683\n350.089056\n516.492442\n1.158499e+05\n4.771246e+04\n6.835303e+04\n\n\nmin\n8.0\n110.0\n0.000000\n0.000000\n0.000000\n0.0\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\n25%\n8.0\n110.0\n542.000000\n79596.000000\n0.000000\n0.0\n9.500000e+01\n6.010000e+02\n3.100000e+02\n2.870000e+02\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n3.120000e+02\n1.570000e+02\n1.520000e+02\n\n\n50%\n8.0\n110.0\n545.000000\n80236.000000\n0.000000\n0.0\n1.700000e+02\n1.064000e+03\n5.530000e+02\n5.120000e+02\n...\n0.000000\n0.000000\n0.000000\n0.000000\n2.000000\n1.000000\n1.000000\n6.050000e+02\n2.870000e+02\n3.110000e+02\n\n\n75%\n8.0\n110.0\n548.000000\n80873.000000\n0.000000\n0.0\n3.170000e+02\n1.934000e+03\n1.010000e+03\n9.200000e+02\n...\n2.000000\n1.000000\n0.000000\n0.000000\n11.000000\n4.000000\n6.000000\n1.148000e+03\n5.280000e+02\n6.230000e+02\n\n\nmax\n8.0\n110.0\n550.000000\n800523.000000\n77.000000\n0.0\n1.177096e+06\n6.626178e+06\n3.468507e+06\n3.157671e+06\n...\n6775.000000\n4002.000000\n1405.000000\n2597.000000\n32114.000000\n13016.000000\n19098.000000\n4.161285e+06\n1.753560e+06\n2.407725e+06\n\n\n\n\n8 rows × 91 columns\n\n\n\n\nData types and memory usage.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2561 entries, 0 to 2560\nData columns (total 94 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   State           2561 non-null   int64 \n 1   District        2561 non-null   int64 \n 2   Subdistt        2561 non-null   int64 \n 3   Town/Village    2561 non-null   int64 \n 4   Ward            2561 non-null   int64 \n 5   EB              2561 non-null   int64 \n 6   Level           2561 non-null   object\n 7   Name            2561 non-null   object\n 8   TRU             2561 non-null   object\n 9   No_HH           2561 non-null   int64 \n 10  TOT_P           2561 non-null   int64 \n 11  TOT_M           2561 non-null   int64 \n 12  TOT_F           2561 non-null   int64 \n 13  P_06            2561 non-null   int64 \n 14  M_06            2561 non-null   int64 \n 15  F_06            2561 non-null   int64 \n 16  P_SC            2561 non-null   int64 \n 17  M_SC            2561 non-null   int64 \n 18  F_SC            2561 non-null   int64 \n 19  P_ST            2561 non-null   int64 \n 20  M_ST            2561 non-null   int64 \n 21  F_ST            2561 non-null   int64 \n 22  P_LIT           2561 non-null   int64 \n 23  M_LIT           2561 non-null   int64 \n 24  F_LIT           2561 non-null   int64 \n 25  P_ILL           2561 non-null   int64 \n 26  M_ILL           2561 non-null   int64 \n 27  F_ILL           2561 non-null   int64 \n 28  TOT_WORK_P      2561 non-null   int64 \n 29  TOT_WORK_M      2561 non-null   int64 \n 30  TOT_WORK_F      2561 non-null   int64 \n 31  MAINWORK_P      2561 non-null   int64 \n 32  MAINWORK_M      2561 non-null   int64 \n 33  MAINWORK_F      2561 non-null   int64 \n 34  MAIN_CL_P       2561 non-null   int64 \n 35  MAIN_CL_M       2561 non-null   int64 \n 36  MAIN_CL_F       2561 non-null   int64 \n 37  MAIN_AL_P       2561 non-null   int64 \n 38  MAIN_AL_M       2561 non-null   int64 \n 39  MAIN_AL_F       2561 non-null   int64 \n 40  MAIN_HH_P       2561 non-null   int64 \n 41  MAIN_HH_M       2561 non-null   int64 \n 42  MAIN_HH_F       2561 non-null   int64 \n 43  MAIN_OT_P       2561 non-null   int64 \n 44  MAIN_OT_M       2561 non-null   int64 \n 45  MAIN_OT_F       2561 non-null   int64 \n 46  MARGWORK_P      2561 non-null   int64 \n 47  MARGWORK_M      2561 non-null   int64 \n 48  MARGWORK_F      2561 non-null   int64 \n 49  MARG_CL_P       2561 non-null   int64 \n 50  MARG_CL_M       2561 non-null   int64 \n 51  MARG_CL_F       2561 non-null   int64 \n 52  MARG_AL_P       2561 non-null   int64 \n 53  MARG_AL_M       2561 non-null   int64 \n 54  MARG_AL_F       2561 non-null   int64 \n 55  MARG_HH_P       2561 non-null   int64 \n 56  MARG_HH_M       2561 non-null   int64 \n 57  MARG_HH_F       2561 non-null   int64 \n 58  MARG_OT_P       2561 non-null   int64 \n 59  MARG_OT_M       2561 non-null   int64 \n 60  MARG_OT_F       2561 non-null   int64 \n 61  MARGWORK_3_6_P  2561 non-null   int64 \n 62  MARGWORK_3_6_M  2561 non-null   int64 \n 63  MARGWORK_3_6_F  2561 non-null   int64 \n 64  MARG_CL_3_6_P   2561 non-null   int64 \n 65  MARG_CL_3_6_M   2561 non-null   int64 \n 66  MARG_CL_3_6_F   2561 non-null   int64 \n 67  MARG_AL_3_6_P   2561 non-null   int64 \n 68  MARG_AL_3_6_M   2561 non-null   int64 \n 69  MARG_AL_3_6_F   2561 non-null   int64 \n 70  MARG_HH_3_6_P   2561 non-null   int64 \n 71  MARG_HH_3_6_M   2561 non-null   int64 \n 72  MARG_HH_3_6_F   2561 non-null   int64 \n 73  MARG_OT_3_6_P   2561 non-null   int64 \n 74  MARG_OT_3_6_M   2561 non-null   int64 \n 75  MARG_OT_3_6_F   2561 non-null   int64 \n 76  MARGWORK_0_3_P  2561 non-null   int64 \n 77  MARGWORK_0_3_M  2561 non-null   int64 \n 78  MARGWORK_0_3_F  2561 non-null   int64 \n 79  MARG_CL_0_3_P   2561 non-null   int64 \n 80  MARG_CL_0_3_M   2561 non-null   int64 \n 81  MARG_CL_0_3_F   2561 non-null   int64 \n 82  MARG_AL_0_3_P   2561 non-null   int64 \n 83  MARG_AL_0_3_M   2561 non-null   int64 \n 84  MARG_AL_0_3_F   2561 non-null   int64 \n 85  MARG_HH_0_3_P   2561 non-null   int64 \n 86  MARG_HH_0_3_M   2561 non-null   int64 \n 87  MARG_HH_0_3_F   2561 non-null   int64 \n 88  MARG_OT_0_3_P   2561 non-null   int64 \n 89  MARG_OT_0_3_M   2561 non-null   int64 \n 90  MARG_OT_0_3_F   2561 non-null   int64 \n 91  NON_WORK_P      2561 non-null   int64 \n 92  NON_WORK_M      2561 non-null   int64 \n 93  NON_WORK_F      2561 non-null   int64 \ndtypes: int64(91), object(3)\nmemory usage: 1.8+ MB"
  },
  {
    "objectID": "lectures/2.2-Data.html#familiar",
    "href": "lectures/2.2-Data.html#familiar",
    "title": "Pandas",
    "section": "Familiar?",
    "text": "Familiar?\ndf.head(3)                       # First 3 rows of df\ndf[['TOT_F','F_SC']].tail(3)     # Last 3 rows of selected columns\ndf.sample(frac=0.3)              # A random 30% sample\ndf.sample(3, random_state=42)    # A random sample of 3 with a seed\ndf.sample(3, random_state=42)    # Same sample!\n\nOn one level, this is what we’ve been building towards! We’ve got head and tail which we saw in the Command Line lecture. We’ve got random sampling with seeds which we saw in the Randomness lecture. We’ve even got LoLs, which we saw way back in the Lists of Lists lecture!"
  },
  {
    "objectID": "lectures/2.2-Data.html#jupyter-formatting",
    "href": "lectures/2.2-Data.html#jupyter-formatting",
    "title": "Pandas",
    "section": "Jupyter Formatting",
    "text": "Jupyter Formatting\nPandas is also ‘Jupyter-aware’, meaning that output can displayed directly in Jupyter and Quarto in ‘fancy’ ways:\n\ndf[['District','TOT_F','F_SC','NON_WORK_M']].sample(5)\n\n\n\n\n\n\n\n\nDistrict\nTOT_F\nF_SC\nNON_WORK_M\n\n\n\n\n1486\n110\n670\n27\n413\n\n\n941\n110\n2840\n522\n1629\n\n\n2041\n110\n458\n23\n296\n\n\n220\n110\n428\n48\n244\n\n\n720\n110\n1241\n351\n571"
  },
  {
    "objectID": "lectures/2.2-Data.html#familiar-1",
    "href": "lectures/2.2-Data.html#familiar-1",
    "title": "Pandas",
    "section": "Familiar?",
    "text": "Familiar?\ndf.head(3)                       # First 3 rows of df\ndf[['ID','Date','Year']].tail(3) # Last 3 rows of selected columns\ndf.sample(frac=0.3)              # A random 30% sample\ndf.sample(3, random_state=42)    # A random sample with a seed\ndf.sample(3, random_state=42)    # Same sample!\n\nOn one level, this is what we’ve been building towards! We’ve got head and tail which we saw in the Command Line lecture. We’ve got random sampling with seeds which we saw in the Randomness lecture. We’ve even got LoLs, which we saw way back in the Lists of Lists lecture!"
  },
  {
    "objectID": "lectures/2.2-Data.html#data-frames-vs-series",
    "href": "lectures/2.2-Data.html#data-frames-vs-series",
    "title": "Pandas",
    "section": "Data Frames vs Series",
    "text": "Data Frames vs Series\nPandas operates on two principles:\n\nAny operation on a Data Frame returns a Data Frame.\nAny operation on a Series returns a Series.\n\n\nWe’ll see in a moment why this is useful!"
  },
  {
    "objectID": "lectures/2.2-Data.html#shallow-copies",
    "href": "lectures/2.2-Data.html#shallow-copies",
    "title": "Pandas",
    "section": "‘Shallow’ Copies",
    "text": "‘Shallow’ Copies\nMore subtly, operations on a Series or Data Frame return a shallow copy, which is like a ‘view’ in a database…\n\nThe original is unchanged unless you specify inplace=True (where supported).\nAttempts to change a subset of the data frame will often trigger a SettingWithCopyWarning warning.\n\nIf you need a full copy then use the copy() method (e.g. df.copy() or df.Series.copy()).\n\n\nDataQuest has a nice overview of how SettingWithCopyWarning is triggered and what to do about it."
  },
  {
    "objectID": "lectures/2.2-Data.html#putting-these-ideas-together",
    "href": "lectures/2.2-Data.html#putting-these-ideas-together",
    "title": "Pandas",
    "section": "Putting These Ideas Together",
    "text": "Putting These Ideas Together\n\n# Returns a series but not a column\nprint(type(df.TOT_F - 1))\n# Saves returned series as a new column\ndf['smaller'] = df.TOT_F - 1\nprint(df.smaller.head(3))\n# Returns a new data frame w/o the dropped column \nprint(type(df.drop(columns=['smaller'])))\n# Modifies df directly to drop the column\nprint(type(df.drop(columns=['smaller'], inplace=True)))\n# Try to modify a view of df (triggers warning)\ndf[df.TOT_F&gt;5000].TOT_F = 5000 \n\n&lt;class 'pandas.core.series.Series'&gt;\n0    3157670\n1    1511406\n2    1646263\nName: smaller, dtype: int64\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n&lt;class 'NoneType'&gt;"
  },
  {
    "objectID": "lectures/2.2-Data.html#chaining",
    "href": "lectures/2.2-Data.html#chaining",
    "title": "Pandas",
    "section": "Chaining",
    "text": "Chaining\nOperations on a Data Frame return a DataFrame and operations on a Series return a Series, allowing us to ‘chain’ steps together:\n\ndf.sort_values(by=['TOT_P','TOT_M'], ascending=False).head(20).sample(frac=0.5).median(numeric_only=True)\n\nState                 8.0\nDistrict            110.0\nSubdistt            542.0\nTown/Village          0.0\nWard                  0.0\n                   ...   \nMARG_OT_0_3_M       907.5\nMARG_OT_0_3_F      1932.5\nNON_WORK_P       241775.5\nNON_WORK_M       112422.0\nNON_WORK_F       129353.5\nLength: 91, dtype: float64"
  },
  {
    "objectID": "lectures/2.2-Data.html#selection",
    "href": "lectures/2.2-Data.html#selection",
    "title": "Pandas",
    "section": "Selection",
    "text": "Selection\n# Returns a selection (Boolean series)\ndf['TOT_P']&gt;5000\n# Data frame of records matching selection\ndf[ df['TOT_P']&gt;5000 ]\n\n# Calculations on a slice (returns mean centroid!)\ndf[df['TOT_P']&gt;5000][['TOT_M','TOT_F']].mean()\nYou can link several conditions using & (and) and | (or).\n# Two conditions with a bit-wise AND\ndf[\n  (df['TOT_P']&gt;5000) & (df['TOT_F']&gt;5000)\n]"
  },
  {
    "objectID": "lectures/2.2-Data.html#dealing-with-types",
    "href": "lectures/2.2-Data.html#dealing-with-types",
    "title": "Pandas",
    "section": "Dealing with Types",
    "text": "Dealing with Types\nA Data Series can only be of one type:\n\n\n\n\n\n\n\n\nPandas Dtype\nPython Type\nUsage\n\n\n\n\nobject\nstr or mixed\nText or mixed columns (including arrays)\n\n\nint64\nint\nInteger columns\n\n\nfloat64\nfloat\nFloating point columns\n\n\nbool\nbool\nTrue/False columns\n\n\ndatetime64\nN/A (datetime)\nDate and time columns\n\n\ntimedelta[ns]\nN/A (datetime)\nDatetime difference columns\n\n\ncategory\nN/A (set)\nCategorical columns"
  },
  {
    "objectID": "lectures/2.2-Data.html#changing-the-type",
    "href": "lectures/2.2-Data.html#changing-the-type",
    "title": "Pandas",
    "section": "Changing the Type",
    "text": "Changing the Type\n\nprint(f\"Unique values: {df['TRU'].unique()}\")   # Find unique values\nprint(f\"Data type is: {df['TRU'].dtype.name}\")  # Confirm is 'object'\ndf['TRU'] = df['TRU'].astype('category')\nprint(f\"Data type now: {df['TRU'].dtype.name}\") # Confirm is 'category'\nprint(df['TRU'].describe()) # Category column info\n\nUnique values: ['Total' 'Rural' 'Urban']\nData type is: object\nData type now: category\ncount      2561\nunique        3\ntop       Rural\nfreq       2194\nName: TRU, dtype: object"
  },
  {
    "objectID": "lectures/2.2-Data.html#datetime-data",
    "href": "lectures/2.2-Data.html#datetime-data",
    "title": "Pandas",
    "section": "Datetime Data",
    "text": "Datetime Data\nWhat do we do here?\nprint(df.Date.dtype.name)\n# object\ndf.Date.to_list()[:3]\n# ['04/20/2019 11:00:00 PM', '12/02/2019 10:35:00 AM', '10/06/2019 04:50:00 PM']\nThis shows that Date is currently a string of dates+times.\nPandas handles date and times using a datetime type that also works as an index (more on these later):\ndf['dt'] = pd.to_datetime(df.Date.array, \n              format=\"%m/%d/%Y %H:%M:%S %p\")\nprint(df.dt.dtype.name)\n# datetime64[ns]\ndf.dt.to_list()[:3]\n# [Timestamp('2019-04-20 11:00:00'), Timestamp('2019-12-02 10:35:00'), Timestamp('2019-10-06 04:50:00')]\nThese follow the formatting conventions of strftime (string format time) for conversion."
  },
  {
    "objectID": "lectures/2.2-Data.html#datetime-formats",
    "href": "lectures/2.2-Data.html#datetime-formats",
    "title": "Pandas",
    "section": "Datetime Formats",
    "text": "Datetime Formats\nExamples of strftime conventions include:\n\n\n\nFormat\nApplies To\n\n\n\n\n%d\n2-digit day\n\n\n%m\n2-digit month\n\n\n%y\n2-digit year\n\n\n%Y\n4-digit year\n\n\n%p\nAM/PM\n\n\n\nSo that is why:\npd.to_datetime(df.Date.array, format=\"%m/%d/%Y %H:%M:%S %p\")\nNote the other things happening here:\n\npd.to_datetime(...) is not a method, it’s a function from the pandas package.\ndf.Date.array (and df.Date.to_numpy() and df.Data.tolist()) gives access to the data directly, whereas df.Date gives access to the Series."
  },
  {
    "objectID": "lectures/2.2-Data.html#deprecation-warning",
    "href": "lectures/2.2-Data.html#deprecation-warning",
    "title": "Pandas",
    "section": "Deprecation Warning!",
    "text": "Deprecation Warning!\nFrom time to time, real-world software projects will change the way things work. Pandas is just such a project!\n\n\n\n\n\n\nWarning\n\n\nWe recommend using Series.array or Series.to_numpy(), depending on whether you need a reference to the underlying data or a NumPy array. See API Documenation.\n\n\n\nSo while Series.values still works, and will continue to work for some time, you are being advised to start using Series.array or Series.to_numpy() instead. Meaning, we should consider using df.Date.array."
  },
  {
    "objectID": "lectures/2.2-Data.html#tidying-up",
    "href": "lectures/2.2-Data.html#tidying-up",
    "title": "Pandas",
    "section": "Tidying Up",
    "text": "Tidying Up\nThis is one way, there are many options and subtleties…\n\n# Fix categories\nmapping = {}\n\n# df['Primary Type'].unique().to_list() also works\nfor x in df['TRU'].cat.categories.to_list():\n  mapping[x]=x.lower()\n\n# And update\ndf['TRU'] = df['TRU'].cat.rename_categories(mapping)\n\nHow would you work out what this code does? 1\nThere are at least two ways: 1) print out mapping; 2) before running the code comment out the ‘update’ line and print out x and x.title(); 3) search for title python."
  },
  {
    "objectID": "lectures/2.2-Data.html#dropping-rows-and-columns",
    "href": "lectures/2.2-Data.html#dropping-rows-and-columns",
    "title": "Pandas",
    "section": "Dropping Rows and Columns",
    "text": "Dropping Rows and Columns\nThere are multiple ways to drop ‘stuff’:\n\ndf2 = df.copy()\nprint(f\"The data frame has {df2.shape[0]:,} rows and {df2.shape[1]:,} cols.\")\ndf2.drop(index=range(5,1000), inplace=True) # Row 'numbers' or index values\nprint(f\"The data frame has {df2.shape[0]:,} rows and {df2.shape[1]:,} cols.\")\ndf2.drop(columns=['TOT_P'], inplace=True)   # Column name(s)\nprint(f\"The data frame has {df2.shape[0]:,} rows and {df2.shape[1]:,} cols.\")\n\nThe data frame has 2,561 rows and 94 cols.\nThe data frame has 1,566 rows and 94 cols.\nThe data frame has 1,566 rows and 93 cols.\n\n\n\nWhy might you want the default to not be in_place?\nThere is also df.dropna() which can apply to rows or columns with NULL or np.nan values."
  },
  {
    "objectID": "lectures/2.2-Data.html#accessing-data-by-location",
    "href": "lectures/2.2-Data.html#accessing-data-by-location",
    "title": "Pandas",
    "section": "Accessing Data by Location",
    "text": "Accessing Data by Location\n\n\n\n\n\n\n\n\n\n\nIndex\n0\n1\n2\n3\n\n\n\n\n\nID\nCase Number\nDate\nPrimary Type\n\n\n0\n11667185\nJC237601\n04/20/2020 11:00:00PM\nBURGLARY\n\n\n1\n11998178\nJC532226\n12/02/2020 10:35:00AM\nDECEPTIVE PRACTICE\n\n\n2\n11852571\nJC462365\n10/06/2020 04:50:00PM\nBATTERY\n\n\n\nWe can interact with rows and columns by position or name:\ndf.iloc[0:2,0:2] # List selection! (':' means 'all')\ndf.loc[0:2,['ID','Case Number']] # Dict selection\nThese actually return different results because of the index:\n\ndf.loc returns the rows labeled 0, 1, and 2 ([0..2]), whereas\ndf.iloc returns the range 0..2 ([0..2))!"
  },
  {
    "objectID": "lectures/2.2-Data.html#indexes",
    "href": "lectures/2.2-Data.html#indexes",
    "title": "Pandas",
    "section": "Indexes",
    "text": "Indexes\nSo by default, pandas creates a row index index whose values are 0..n and column index whose values are the column names. You will see this if you select a sample:\n\ndf.sample(2)\n\n\n\n\n\n\n\n\nState\nDistrict\nSubdistt\nTown/Village\nWard\nEB\nLevel\nName\nTRU\nNo_HH\n...\nMARG_AL_0_3_F\nMARG_HH_0_3_P\nMARG_HH_0_3_M\nMARG_HH_0_3_F\nMARG_OT_0_3_P\nMARG_OT_0_3_M\nMARG_OT_0_3_F\nNON_WORK_P\nNON_WORK_M\nNON_WORK_F\n\n\n\n\n2516\n8\n110\n550\n81169\n0\n0\nVILLAGE\nVishanpura\nrural\n69\n...\n0\n0\n0\n0\n0\n0\n0\n207\n102\n105\n\n\n405\n8\n110\n540\n79341\n0\n0\nVILLAGE\nSurana\nrural\n518\n...\n1\n0\n0\n0\n4\n1\n3\n2039\n911\n1128\n\n\n\n\n2 rows × 94 columns\n\n\n\nThe left-most column is the index. So Name is now the index and is no longer a column: notice that the number of columns has changed.\n\ndf.set_index('Name', inplace=True)\ndf.sample(2)\n\n\n\n\n\n\n\n\nState\nDistrict\nSubdistt\nTown/Village\nWard\nEB\nLevel\nTRU\nNo_HH\nTOT_P\n...\nMARG_AL_0_3_F\nMARG_HH_0_3_P\nMARG_HH_0_3_M\nMARG_HH_0_3_F\nMARG_OT_0_3_P\nMARG_OT_0_3_M\nMARG_OT_0_3_F\nNON_WORK_P\nNON_WORK_M\nNON_WORK_F\n\n\nName\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChaturbhuj\n8\n110\n538\n79085\n0\n0\nVILLAGE\nrural\n160\n921\n...\n0\n0\n0\n0\n0\n0\n0\n504\n271\n233\n\n\nMansharampura\n8\n110\n546\n80196\n0\n0\nVILLAGE\nrural\n258\n1718\n...\n0\n0\n0\n0\n2\n0\n2\n1085\n477\n608\n\n\n\n\n2 rows × 93 columns"
  },
  {
    "objectID": "lectures/2.2-Data.html#indexes-contd",
    "href": "lectures/2.2-Data.html#indexes-contd",
    "title": "Pandas",
    "section": "Indexes (cont’d)",
    "text": "Indexes (cont’d)\nAnd now:\n\ndf.loc['Jaipur (M Corp.) (Part) WARD NO.-0002',:]\n\nState                 8\nDistrict            110\nSubdistt            546\nTown/Village     800522\nWard                  2\n                  ...  \nMARG_OT_0_3_M        64\nMARG_OT_0_3_F        59\nNON_WORK_P        45438\nNON_WORK_M        17962\nNON_WORK_F        27476\nName: Jaipur (M Corp.) (Part) WARD NO.-0002, Length: 93, dtype: object\n\n\n\ndf.loc['Jaipur (M Corp.) (Part) WARD NO.-0002':'Jaipur (M Corp.) (Part) WARD NO.-0005','TOT_P':'TOT_WORK_P']\n\n\n\n\n\n\n\n\nTOT_P\nTOT_M\nTOT_F\nP_06\nM_06\nF_06\nP_SC\nM_SC\nF_SC\nP_ST\nM_ST\nF_ST\nP_LIT\nM_LIT\nF_LIT\nP_ILL\nM_ILL\nF_ILL\nTOT_WORK_P\n\n\nName\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJaipur (M Corp.) (Part) WARD NO.-0002\n65260\n34942\n30318\n8296\n4597\n3699\n8061\n4269\n3792\n1589\n842\n747\n48756\n28256\n20500\n16504\n6686\n9818\n19822\n\n\nJaipur (M Corp.) (Part) WARD NO.-0003\n39281\n21036\n18245\n4944\n2661\n2283\n4532\n2450\n2082\n610\n306\n304\n30044\n17269\n12775\n9237\n3767\n5470\n12169\n\n\nJaipur (M Corp.) (Part) WARD NO.-0004\n40485\n21147\n19338\n5086\n2771\n2315\n3640\n1921\n1719\n895\n443\n452\n30269\n16558\n13711\n10216\n4589\n5627\n13433\n\n\nJaipur (M Corp.) (Part) WARD NO.-0005\n40360\n21064\n19296\n4409\n2400\n2009\n3097\n1626\n1471\n425\n210\n215\n32848\n17623\n15225\n7512\n3441\n4071\n13216\n\n\n\n\n\n\n\nMnemonic: we used iloc to select rows/cols based on integer location and we use loc to select rows/cols based on name location.\nP.S. You can reset the data frame using df.reset_index(inplace=True)."
  },
  {
    "objectID": "lectures/2.2-Data.html#saving",
    "href": "lectures/2.2-Data.html#saving",
    "title": "Pandas",
    "section": "Saving",
    "text": "Saving\nPandas can write to a wide range of file types, including:\n\n\n\nCommand\nSaved As…\n\n\n\n\ndf.to_csv(&lt;path&gt;)\nCSV file. But note the options to change sep (default is ',') and to suppress index output (index=False).\n\n\ndf.to_excel(&lt;path&gt;)\nXLSX file. But note the options to specify a sheet_name, na_rep, and so on, as well as to suppress the index (index=False).\n\n\ndf.to_parquet(&lt;path&gt;)\nDirectly usable by many languages. Requires pyarrow to be installed to access the options.\n\n\ndf.to_latex(&lt;path&gt;))\nWrite a LaTeX-formatted table to a file. Display requires booktabs. Could do copy+paste with print(df.to_latex()).\n\n\ndf.to_markdown(&lt;path&gt;)\nWrite a Markdown-formatted table to a file. Requires tabulate. Could do copy+paste with print(df.to_markdown()).\n\n\n\nIn most cases compression is detected automatically (e.g. df.to_csv('file.csv.gz')) but you can also specify it (e.g. df.to_csv('file.csv.gz', compression='gzip')).1\nFor instance, a bit.ly link to a Gzipped file requires compression='gzip' because there’s nothing in the link itself to tell Pandas what to expect."
  },
  {
    "objectID": "lectures/2.2-Data.html#resources",
    "href": "lectures/2.2-Data.html#resources",
    "title": "Pandas",
    "section": "Resources",
    "text": "Resources\n\nData Cleaning with Numpy and Pandas\nPandas dtypes\nThe Index Explained\nUsing Pandas iloc\nA Clear Explanation of the Pandas Index\nUfuncs and Apply"
  },
  {
    "objectID": "lectures/2.2-Data.html#accessing-a-series",
    "href": "lectures/2.2-Data.html#accessing-a-series",
    "title": "Pandas",
    "section": "Accessing a Series",
    "text": "Accessing a Series\n\nprint(type(df['State']))            # type for column\nprint(type(df['State'].array))      # type for values\nprint(df['State'].array[:5])        # first five values\nprint(f\"1: {df['TOT_P'].mean()}\")   # summarise a series/column\nprint(f\"2: {df.TOT_P.mean():0.2f}\") # if no spaces in name\n\n&lt;class 'pandas.core.series.Series'&gt;\n&lt;class 'pandas.core.arrays.numpy_.NumpyExtensionArray'&gt;\n&lt;NumpyExtensionArray&gt;\n[8, 8, 8, 8, 8]\nLength: 5, dtype: int64\n1: 14301.45294806716\n2: 14301.45\n\n\n\nNotice that we’ve got two ways of accessing a pandas Series:\n\nThe dictionary-like way: df['Latitude']; this works for all columns, always.\nThe method-like way: df.Latitude; this works for ‘reading’ columns without spaces in their names."
  },
  {
    "objectID": "lectures/2.2-Data.html#nosing-around",
    "href": "lectures/2.2-Data.html#nosing-around",
    "title": "Pandas",
    "section": "Nosing Around",
    "text": "Nosing Around\ndf.head(3)                       # First 3 rows of df\ndf[['TOT_F','F_SC']].tail(3)     # Last 3 rows of selected columns\ndf.sample(frac=0.3)              # A random 30% sample\ndf.sample(3, random_state=42)    # A random sample of 3 with a seed\ndf.sample(3, random_state=42)    # Same sample!\n\nHead and tail are used on the Command Line. Random sampling with seeds is covered in my talk on Randomness. We’ve even got Lists of Lists, which is a really basic data structure!"
  },
  {
    "objectID": "lectures/2.2-Data.html#dealing-with-money",
    "href": "lectures/2.2-Data.html#dealing-with-money",
    "title": "Pandas",
    "section": "Dealing with Money",
    "text": "Dealing with Money\nYou may encounter currency treated as a string, instead of a number. Normally, this is because of the way that the data is formatted (e.g. ‘₹1.5 lakh’) To deal with pricing information treated as a string:\n# You would need a function to deal with lakh and crore\ndf['price'].str.replace('₹','').str.\\\n            replace(',','').astype(float)\nMany more examples accessible via Google!\n\nAnother thing you might notice here: adding .cat allows us to access category methods for the Series; adding .str allows us to access string methods for the Series."
  },
  {
    "objectID": "lectures/2.2-Data.html#indexes-contd-1",
    "href": "lectures/2.2-Data.html#indexes-contd-1",
    "title": "Pandas",
    "section": "Indexes (cont’d 1)",
    "text": "Indexes (cont’d 1)\nSo now we can pull the data for a single ward like this:\n\ndf.loc['Jaipur (M Corp.) (Part) WARD NO.-0002',:]\n\nState                 8\nDistrict            110\nSubdistt            546\nTown/Village     800522\nWard                  2\n                  ...  \nMARG_OT_0_3_M        64\nMARG_OT_0_3_F        59\nNON_WORK_P        45438\nNON_WORK_M        17962\nNON_WORK_F        27476\nName: Jaipur (M Corp.) (Part) WARD NO.-0002, Length: 93, dtype: object"
  },
  {
    "objectID": "lectures/2.2-Data.html#indexes-contd-2",
    "href": "lectures/2.2-Data.html#indexes-contd-2",
    "title": "Pandas",
    "section": "Indexes (cont’d 2)",
    "text": "Indexes (cont’d 2)\nAnd we can pull data for a range of wards like this:\n\ndf.loc['Jaipur (M Corp.) (Part) WARD NO.-0002':'Jaipur (M Corp.) (Part) WARD NO.-0004',\n  'TOT_P':'F_06']\n\n\n\n\n\n\n\n\nTOT_P\nTOT_M\nTOT_F\nP_06\nM_06\nF_06\n\n\nName\n\n\n\n\n\n\n\n\n\n\nJaipur (M Corp.) (Part) WARD NO.-0002\n65260\n34942\n30318\n8296\n4597\n3699\n\n\nJaipur (M Corp.) (Part) WARD NO.-0003\n39281\n21036\n18245\n4944\n2661\n2283\n\n\nJaipur (M Corp.) (Part) WARD NO.-0004\n40485\n21147\n19338\n5086\n2771\n2315\n\n\n\n\n\n\n\nMnemonic: we used iloc to select rows/cols based on integer location and we use loc to select rows/cols based on name location.\nP.S. You can reset the data frame using df.reset_index(inplace=True)."
  },
  {
    "objectID": "lectures/2.2-Data.html#result",
    "href": "lectures/2.2-Data.html#result",
    "title": "Pandas",
    "section": "Result!",
    "text": "Result!\n\nNow you can do all of your analysis quickly in Python but send your manager a well-formatted Excel spreadsheet and pretend it took you ages…"
  },
  {
    "objectID": "practicals/Think_Data.html",
    "href": "practicals/Think_Data.html",
    "title": "Think Data",
    "section": "",
    "text": "In this notebook we are going to look in more detail at how we can reduce, reuse, and recycle our code to make our lives easier and our code more efficient."
  },
  {
    "objectID": "practicals/Think_Data.html#reading-a-remote-file",
    "href": "practicals/Think_Data.html#reading-a-remote-file",
    "title": "Think Data",
    "section": "Reading a Remote File",
    "text": "Reading a Remote File\nSo, we are going to download a file from GitHub, but we aren’t going to to try to turn it into data or otherwise make ‘sense’ of it yet, we just want to read it. We are then going to build from this first step towards the rest of the steps!\nBecause we’re accessing data from a ‘URL’ we need to use the urlopen function from the urllib.request package. If you’re wondering how we know to use this function and package, you might google something like: read remote csv file python 3 which in turn might get you to a StackOverflow question and answer like this.\n\n\n\nfrom urllib.request import urlopen\nhelp(urlopen)\n\nHelp on function urlopen in module urllib.request:\n\nurlopen(url, data=None, timeout=&lt;object object at 0x100a748c0&gt;, *, cafile=None, capath=None, cadefault=False, context=None)\n    Open the URL url, which can be either a string or a Request object.\n\n    *data* must be an object specifying additional data to be sent to\n    the server, or None if no such data is needed.  See Request for\n    details.\n\n    urllib.request module uses HTTP/1.1 and includes a \"Connection:close\"\n    header in its HTTP requests.\n\n    The optional *timeout* parameter specifies a timeout in seconds for\n    blocking operations like the connection attempt (if not specified, the\n    global default timeout setting will be used). This only works for HTTP,\n    HTTPS and FTP connections.\n\n    If *context* is specified, it must be a ssl.SSLContext instance describing\n    the various SSL options. See HTTPSConnection for more details.\n\n    The optional *cafile* and *capath* parameters specify a set of trusted CA\n    certificates for HTTPS requests. cafile should point to a single file\n    containing a bundle of CA certificates, whereas capath should point to a\n    directory of hashed certificate files. More information can be found in\n    ssl.SSLContext.load_verify_locations().\n\n    The *cadefault* parameter is ignored.\n\n\n    This function always returns an object which can work as a\n    context manager and has the properties url, headers, and status.\n    See urllib.response.addinfourl for more detail on these properties.\n\n    For HTTP and HTTPS URLs, this function returns a http.client.HTTPResponse\n    object slightly modified. In addition to the three new methods above, the\n    msg attribute contains the same information as the reason attribute ---\n    the reason phrase returned by the server --- instead of the response\n    headers as it is specified in the documentation for HTTPResponse.\n\n    For FTP, file, and data URLs and requests explicitly handled by legacy\n    URLopener and FancyURLopener classes, this function returns a\n    urllib.response.addinfourl object.\n\n    Note that None may be returned if no handler handles the request (though\n    the default installed global OpenerDirector uses UnknownHandler to ensure\n    this never happens).\n\n    In addition, if proxy settings are detected (for example, when a *_proxy\n    environment variable like http_proxy is set), ProxyHandler is default\n    installed and makes sure the requests are handled through the proxy.\n\n\n\n\nAs you can see, there is lot of information here about how things work. A lot of it won’t make much sense at the moment. That’s ok. Some of this doesn’t make much sense to me, but that’s because this is the full documentation from Python so it’s trying to cover all the bases. You don’t need to read every line of this, what you are looking is information about things like the ‘signature’ (what parameters the function accepts) and its output. Of course, you can also just Google it!\n\n\n\n\n\n\nTip\n\n\n\nRemember that you can use dir(...) and help(...) to investigate what a package offers. You can also get help in Jupyter by typing ? before the function that you want to call.\n\n\n\n\n\n\n\n\nURLError?\n\n\n\n\n\nIf you are working behind a firewall (esp. if working on this practical in, say, China) then there is a chance you will get a URLError (&lt;urlopen error [Errno 110044] getaddrinfo failed&gt;). This is a ‘proxy error’ and in this case you may need to configure your environment as follows:\nimport os\nos.environ['HTTP_PROXY'] = 'http://127.0.0.1:10809'\nos.environ['HTTPS_PROXY'] = 'http://127.0.0.1:10809'\n\n\n\nBefore you start working on the code, why not open the data file directly in your browser? It’s pretty small, and it will give you a sense of what is going on.\nfrom urllib.request import URLError\nfrom urllib.request import urlopen\n\n# Given the info you were given above, what do you \n# think the value of 'url' should be? What\n# type of variable is it? int or string? \nurl = 'https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv'\n\n# Read the URL stream into variable called 'response'\n# using the function that we imported above\ntry:\n    response = urlopen(url)\nexcept URLError as e:\n    print(\"Unable to connect to URL!\")\n    print(e)\n\n# You might want to explore what `__class__` and `__name__`\n# are doing, but basically the give us a way of finding out what\n# is 'behind' more complex variables\n\n# Now read from the stream, decoding so that we get actual text\nraw = response.read()\n\nprint(f\"'raw' variable is of type: '{raw.__class__.__name__}'.\")\nprint(f\"Raw content is:\\n{raw[:75]}...\\n\")\n\ndata = raw.decode('utf-8')\n\nprint(f\"'data' variable is of type: '{data.__class__.__name__}'.\")\nprint(f\"Decoded content is:\\n{data[:75]}...\")\n\n\n\n\n\n\nNote\n\n\n\nNotice that the raw data has the format b'...' with all of the data seemingly on one line, while the decoded version in data is ‘correctly’ structured with lines! The ‘raw’ data is in bytecode format which is not, strictly, a string. It only becomes a string when we ‘decode it’ to utf-8 (which is the ‘encoding’ of text that supports most human languages). While the computer doesn’t particularly care, we do!\n\n\nRemember that you can treat strings as lists, so when we print below we cut off the output using the list[:&lt;Some Number&gt;] syntax.\n\nprint(f\"There are {len(data)} characters in the data variable.\")\nprint(f\"The first 125 characters are: '{data[:125]}'\") # Notice that '\\n' count here!\n\nThere are 352 characters in the data variable.\nThe first 125 characters are: 'City,Population,Latitude,Longitude\nPerth,45770,56.39583,-3.43333\nArmagh,14777,54.3499,-6.6546\nDundee,147268,56.462,-2.9707'\n\n\nSo this is definitely text, but it doesn’t (yet) look entirely like the data we see because it’s still just one long string, and not data which has individual records on each line. To split the text into individual lines, we can use the handily named .splitlines() method (more on methods below):\n\nQuestionAnswer\n\n\nrows = ??.splitlines()\nprint(f\"'rows' variable is of type: {rows.__class__.__name__}'.\")\n\n\nrows = data.splitlines()\nprint(f\"'rows' variable is of type: {rows.__class__.__name__}'.\")\n'rows' variable is of type: list'.\n\n\n\nNote now, how the data variable has type list. So to view the data as we see them in the original online file, we can now use a for loop to print out each element of the list (each element being a row of the original online file):\n\nQuestionAnswer\n\n\nprint(f\"There are {??} rows of data.\")\nprint(\"\\n\".join(??[0:2])) # New syntax alert! notice we can *join* list elements\n\n\nprint(f\"There are {len(rows)} rows of data.\")\nprint(\"\\n\".join(rows[0:2])) # New syntax alert! notice we can *join* list elements\nThere are 11 rows of data.\nCity,Population,Latitude,Longitude\nPerth,45770,56.39583,-3.43333\n\n\n\nThat’s a little hard to read, though something has clearly changed. Let’s try printing the last row:\n\nprint(rows[-1])\n\nBangor,18808,53.228,-4.128\n\n\nCongratulations! You’ve now read a text file sitting on a server in, I think, Canada and Python didn’t care. You’ve also converted a plain-text file to a row-formatted list."
  },
  {
    "objectID": "practicals/Think_Data.html#text-into-data",
    "href": "practicals/Think_Data.html#text-into-data",
    "title": "Think Data",
    "section": "Text into Data",
    "text": "Text into Data\nWe now need to work on turning the response into useful data. We got partway there by splitting on line-breaks (splitlines()), but now we need to get columns for each line. You’ll notice that we are dealing with a CSV (Comma-Separated Value) file and that the format looks quite simple… So, in theory, to turn this into data we ‘just’ need to split each row into separate fields using the commas.\nThere’s a handy function associated with strings called split:\n\nprint('abcdefgh'.split('d'))\n\n['abc', 'efgh']\n\n\nYou can also investigate further how the split function works using:\n\nhelp('abcdefgh'.split)\n\nHelp on built-in function split:\n\nsplit(sep=None, maxsplit=-1) method of builtins.str instance\n    Return a list of the substrings in the string, using sep as the separator string.\n\n      sep\n        The separator used to split the string.\n\n        When set to None (the default value), will split on any whitespace\n        character (including \\n \\r \\t \\f and spaces) and will discard\n        empty strings from the result.\n      maxsplit\n        Maximum number of splits.\n        -1 (the default value) means no limit.\n\n    Splitting starts at the front of the string and works to the end.\n\n    Note, str.split() is mainly useful for data that has been intentionally\n    delimited.  With natural text that includes punctuation, consider using\n    the regular expression module.\n\n\n\nSo this seems like a good solution to turn our text into data:\n\ntest = rows[-1].split(',')\nprint(test)\nprint(f\"The population of {test[0]} is {int(test[1]):,}\")\n\n['Bangor', '18808', '53.228', '-4.128']\nThe population of Bangor is 18,808\n\n\nI’d say that we’re now getting quite close to something that looks like ‘real data’: I know how to convert a raw response from a web server into a string, to split that string into rows, and can even access individual elements from a row!"
  },
  {
    "objectID": "practicals/Think_Data.html#the-advantages-of-a-package",
    "href": "practicals/Think_Data.html#the-advantages-of-a-package",
    "title": "Think Data",
    "section": "The Advantages of a Package",
    "text": "The Advantages of a Package\n\n\n\n\n\n\nCaution\n\n\n\nThere are two problems to the data.splitlines() and row.split(',') approach! One of them is visible (though not obvious) in the examples above, the other is not.\n\n\n\nRemember that 10 and '10' are not the same thing. To comma-format the population of Sheffield you’ll see that I had to do int(...) in order to turn '685368' into a number. So our approach so far doesn’t know anything about the type of data we’re working with.\nWe are also implicitly assuming that commas can only appear at field boundaries (i.e. that they can only appear to separate one column of data from the next). In other words, just using split(',') doesn’t work if any of the fields can themselves contain a comma!\nThere’s actually a third potential issue, but it’s so rare that we would need to take a completely different approach to deal with it: we are also assuming that newlines (\\n) can only appear at record boundaries (i.e. that the can only appear to separate one row of data from the next). In those cases, using splitlines() also doesn’t work, but this situation is (thankfully) very rare indeed.\n\nThis is where using code that someone else who is much more interested (and knowledgeable) has written and contributed is helpful: we don’t need to think through how to deal with this sort of thing ourselves, we can just find a library that does what we need and make use of its functionality. I’ve given you the skeleton of the answer below, but you’ll need to do a little Googling to find out how to \"read csv python\".\nNote: For now just focus on problem #2.\nfrom urllib.request import urlopen\nimport csv\n\nurl = 'https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv'\nresponse = urlopen(url)\nraw = response.read()\n\n# Now take the raw data, decode it, and then\n# pass it over to the CSV reader function\ncsvfile  = csv.reader(raw.decode('utf-8').splitlines()) \n\nurlData = [] # Somewhere to store the data\nfor row in csvfile:              \n    urlData.append( row )\n\nprint(\"urlData has \" + str(len(urlData)) + \" rows and \" + str(len(urlData[0])) + \" columns.\")\nprint(urlData[-1]) # Check it worked!\nIf it worked, then you should have this output:\n\n\nurlData has 11 rows and 4 columns.\n['Bangor', '18808', '53.228', '-4.128']\n\n\nTo you that might look a lot worse that the data that you originally had, but to a computer that list-of-lists is something it can work with; check it out:\n\nfor u in urlData[1:6]: # For each row in the first 5 items in list\n    print(f\"The city of '{u[0]}' has a population of {int(u[1]):,}\") # Print out the name and pop\n\nThe city of 'Perth' has a population of 45,770\nThe city of 'Armagh' has a population of 14,777\nThe city of 'Dundee' has a population of 147,268\nThe city of 'Colchester' has a population of 194,706\nThe city of 'Salisbury' has a population of 40,302\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhy did I use urlData[1:] instead of urlData?\nIf you print urlData[0] you’ll see that this is the ‘header’ row that tells us what each column contains! So if we try to convert the column name to an integer (int(u[1])) we will get an error!\nThe advantage of using the csv library over plain old string.split is that the csv library knows how to deal with fields that contain commas (e.g. \"Cardfiff, Caerdydd\" or \"An Amazing 4 Bedroom Home, Central London, Sleeps 12\") and so is much more flexible and consistent that our naive split approach. The vast majority of common tasks (reading certain types of files, getting remote files, etc.) have libraries that do exactly what you want without you needing to write much code yourself to take advantage of it. You should always have a look around online to see if a library exists before thinking that you need to write everything/anything from scratch. The tricky part is knowing what words to use for your search and how to read the answers that you find…\n\n\nLet’s try this with a ‘bigger’ data set… In an ideal world, the ‘power’ of code is that once we’ve solved the problem once, we’ve solved it more generally as well. So let’s try with the ‘scaled-up’ data set and see waht happens!\nfrom urllib.request import urlopen\nimport csv\n\nurl = \"https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities.csv\"\nresponse = urlopen(url)\nraw = response.read()\n\ncsvfile = csv.reader(raw.decode('utf-8').splitlines())\n\nurlData = [] # Somewhere to store the data\n\nfor row in csvfile:              \n    urlData.append( row )\n\nprint(f\"urlData has {len(urlData)} rows and {len(urlData[0])} columns.\")\n\nfor u in urlData[70:]:  # For each row in the list\n    print(f\"The city of '{u[0]}' has a population of {u[1]}\") # Print out the name and pop\n\n\n\n\n\n\nWhat mistake have I made here?\n\n\n\n\n\nI have assumed that, just because the files have similar names, they must also have similar layouts!\n\nprint(f\"The URL's data labels are: {', '.join(urlData[0])}\")\n\nThe URL's data labels are: City, Population, Latitude, Longitude"
  },
  {
    "objectID": "practicals/Think_Data.html#insight",
    "href": "practicals/Think_Data.html#insight",
    "title": "Think Data",
    "section": "Insight!",
    "text": "Insight!\nSo, although the code was basically the same for both of these files (good), we would need to change quite a bit in order to print out the same information from different versions of the same data. So our code is rather brittle.\nOne of the issues is that our instincts about how to manage data doesn’t align with how the computer can most efficiently manage it. We make the mistake of thinking that the computer needs to do things that same way that we do when reading text and so assume that we need to:\n\nRepresent the rows as a list.\nRepresent the columns as a list for each row.\n\nThis thinking suggests that the ‘right’ data structure would clearly be a list-of-lists (LoLs!), but if you understand what happened here then the next section will make a lot more sense!"
  },
  {
    "objectID": "practicals/Think_Data.html#understanding-whats-an-appropriate-data-structure",
    "href": "practicals/Think_Data.html#understanding-whats-an-appropriate-data-structure",
    "title": "Think Data",
    "section": "Understanding What’s an ‘Appropriate’ Data Structure",
    "text": "Understanding What’s an ‘Appropriate’ Data Structure\nIf you stop to think about it, then our list-of-lists approach to the data isn’t very easy to navigate. Notice that if the position or name of a column changes then we need to change our program every time we re-run it! It’s not very easy to read either since we don’t really know what u[5] is supposed to be. That way lies all kinds of potential errors!\nAlso consider that, in order to calculate out even a simple aggregate such as the sum of a field for all rows we need to step through a lot of irrelevant data as well: we have to write a for loop and then step through each row with an ‘accumulator’ (somewhere to store the total). That’s slow.\nThat doesn’t make much sense since this should all be easier and faster in Python than in Excel, but right now it’s harder, and quite possibly slower as well! So how does the experienced programmer get around this? ‘Simple’ (i.e. neither simple, nor obvious, until you know the answer): she realises that the data is organised the wrong way! We humans tend to think in rows of data: this apartment has the following attributes (price, location, etc.), or that city has the following attributes (population, location). We read across the row because that’s the easiest way for us to think about it. But, in short, a list-of-lists does not seem to be the right way to store this data!\nCrucially, a computer doesn’t have to work that way. For a computer, it’s as easy to read down a column as it is to read across a row. In fact, it’s easier, because each column has the same type of data: one column contains names (strings), another column contains prices (integers), and other columns contain other types of data (floats, etc.). Better still, the order of the columns often doesn’t matter as long as we know what the columns are called: it’s easier to ask for the ‘description column’ than it is to ask for the 6th column since, for all we know, the description column might be in a different place for different files but they are all (relatively) likely to use the ‘description’ label for the column itself.\n\nA Dictionary of Lists to the Rescue\nSo, if we don’t care about column order, only row order, then a dictionary of lists would be a nice way to handle things. And why should we care about column order? With our CSV files above we already saw what a pain it was to fix things when the layout of the columns changed from one data set to the next. If, instead, we can just reference the ‘description’ column then it doesn’t matter where that column actually is. Why is that?\nWell, here are the first four rows of data from a list-of-lists for city sizes:\nmyData = [\n  ['id', 'Name', 'Rank', 'Longitude', 'Latitude', 'Population'], \n  ['1', 'Greater London', '1', '-18162.92767', '6711153.709', '9787426'], \n  ['2', 'Greater Manchester', '2', '-251761.802', '7073067.458', '2553379'], \n  ['3', 'West Midlands', '3', '-210635.2396', '6878950.083', '2440986']\n]\nNow, here’s how it would look as a dictionary of lists organised by column, and not by row:\n\nmyData = {\n    'id'         : [0, 1, 2, 3, 4, 5],\n    'Name'       : ['London', 'Manchester', 'Birmingham','Edinburgh','Inverness','Lerwick'],\n    'Rank'       : [1, 2, 3, 4, 5, 6],\n    'Longitude'  : [-0.128, -2.245, -1.903, -3.189, -4.223, -1.145],\n    'Latitude'   : [51.507, 53.479, 52.480, 55.953, 57.478, 60.155],\n    'Population' : [9787426, 2705000, 1141816, 901455, 70000, 6958],\n}\n\nprint(myData['Name'])\nprint(myData['Population'])\n\n['London', 'Manchester', 'Birmingham', 'Edinburgh', 'Inverness', 'Lerwick']\n[9787426, 2705000, 1141816, 901455, 70000, 6958]\n\n\nWhat does this do better? Well, for starters, we know that everything in the ‘Name’ column will be a string, and that everything in the ‘Longitude’ column is a float, while the ‘Population’ column contains integers. So that’s made life easier already, but the real benefit is coming up…\n\n\nBehold the Power of the DoL\nNow let’s look at what you can do with this… but first we need to import one more package that you’re going to see a lot over the rest of term: numpy (Numerical Python), which is used so much that most people simply refer to it as np. This is a huge package in terms of features, but right now we’re interested only in the basic arithmatic functions: mean, max, and min.\n\n\n\n\n\n\nWe’ll step through most of these in detail below.\n\n\n\n\n\n\n\n\n\nFind the latitude of Manchester:\n\ncity = \"Manchester\"\nlat = myData['Latitude'][ myData['Name'].index(city) ]\nprint(f\"{city}'s latitude is {lat}\")\n\nManchester's latitude is 53.479\n\n\nPrint the location of Lerwick:\n\nQuestionAnswer\n\n\ncity = \"Lerwick\"\nprint(f\"The town of {city} can be found at \" + \n      f\"{abs(myData[??][ ?? ])}ºW, {myData['Latitude'][ ?? ]}ºN\")\n\n\ncity = \"Lerwick\"\nprint(f\"The town of {city} can be found at \" + \n      f\"{abs(myData['Longitude'][myData['Name'].index(city)])}ºW, {myData['Latitude'][myData['Name'].index(city)]}ºN\")\nThe town of Lerwick can be found at 1.145ºW, 60.155ºN\n\n\n\nFind the easternmost city:\n\nQuestionAnswer\n\n\ncity = myData['Name'][ myData[??].index( max(myData[??]) ) ]\nprint(f\"The easternmost city is: {city}\")\n\n\ncity = myData['Name'][ myData['Longitude'].index( max(myData['Longitude']) ) ]\nprint(f\"The easternmost city is: {city}\")\nThe easternmost city is: London\n\n\n\nFind the mean population of the cities using a handy package called numpy:\n\nQuestionAnswer\n\n\nimport numpy as np\nmean = np.??(myData['Population'])\nprint(f\"The mean population is: {mean}\")\n\n\nimport numpy as np\nmean = np.mean(myData['Population'])\nprint(f\"The mean population is: {mean}\")\nThe mean population is: 2435442.5\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nStop! Look closely at what is going on. There’s a lot of content to process in the code above, so do not rush blindly on if this is confusing. Try pulling it apart into pieces and then reassemble it. Start with the bits that you understand and then add complexity.\n\n\nWe’ll go through each one in turn, but they nearly all work in the same way and the really key thing is that you’ll notice that we no longer have any loops (which are slow) just index or np.&lt;function&gt; (which is very fast).\n\n\nThe Population of Manchester\nThe code can look pretty daunting, so let’s break it down into two parts. What would you get if you ran just this code?\n\nmyData['Population'][1]\n\n2705000\n\n\nRemember that this is a dictionary-of-lists (DoL). So, Python first looks for a key named Population in the myData dictionary. It finds out that the value associated with this key is a list and in this example, it just pulls out the second value (index 1). Does that part make sense?\nNow, to the second part:\n\nmyData['Name'].index('Manchester')\n\n1\n\n\nHere we look in the dictionary for the key Name and find that that’s also a list. All we’re doing here is asking Python to find the index of ‘Manchester’ for us in that list. And myData['Name'].index('Manchester') gives us back a 1, so instead of just writing myData['Population'][1] we can replace the 1 with myData['Name'].index('Manchester')! Crucially, notice the complete absence of a for loop?\nDoes that make sense? If it does then you should be having a kind of an 🤯 moment because what we’ve done by taking a column view, rather than a row view, is to make Python’s index() command do the work for us. Instead of having to look through each row for a field that matches ‘Name’ and then check to see if it’s ‘Manchester’, we’ve pointed Python at the right column immediately and asked it to find the match (which it can do very quickly). Once we have a match then we also have the row number to go and do the lookup in the ‘Population’ column because the index is the row number!\n\n\nThe Easternmost City\nWhere this approach really comes into its own is on problems that involve maths. To figure out the easternmost city in this list we need to find the maximum Longitude and then use that value to look up the city name. So let’s do the same process of pulling this apart into two steps. Let start with the easier bit:\n\nmyData['Name'][0]\n\n'London'\n\n\nThat would give us the name of a city, but we don’t just want the first city in the list, we want the one with the maximum longitude. To achieve that we need to somehow replace the 0 with the index of the maximum longitude. Let’s break this down further:\n\nWe first need to find the maximum longitude.\nWe then need to find the index of that maximum longitude.\n\nSo Step 1 would be:\n\nmax_lon = max(myData['Longitude'])\n\nBecause the max(...) helps us to find the maximum longitude in the Longitude list. Now that we have that we can proceed to Step 2:\n\nmyData['Longitude'].index(max_lon)\n\n0\n\n\nSo now we ask Python to find the position of max_lon in the list. But rather than doing this in two steps we can combine into one if we write it down to make it easier to read:\n\nmyData['Longitude'].index(\n    max(myData['Longitude'])\n)\n\n0\n\n\nThere’s the same .index which tells us that Python is going to look for something in the list associated with the Longitude key. All we’ve done is change what’s inside that index function to max(myData['Longitude']). This is telling Python to find the maximum value in the myData['Longitude'] list. So to explain this in three steps, what we’re doing is:\n\nFinding the maximum value in the Longitude column (we know there must be one, but we don’t know what it is!),\nFinding the index (position) of that maximum value in the Longitude column (now that we know what the value is!),\nUsing that index to read a value out of the Name column.\n\nI am a geek, but that’s pretty cool, right? In one line of code we managed to quickly find out where the data we needed was even though it involved three discrete steps. Think about how much work you’d have to do if you were still thinking in rows, not columns!\n\n\nThe Location of Lerwick\nLerwick is a small town in the Shetlands, way up to the North of mainland U.K. and somewhere I’ve wanted to go ever since I got back from Orkney–but then I spent my honeymoon in the far North of Iceland, so perhaps I just don’t like being around lots of people… 🙃\nAnyway, this one might be a tiny bit easier conceptually than the other problems, except that I’ve deliberately used a slightly different way of showing the output that might be confusing:\nPrint the location of Lerwick:\n\ncity = \"Lerwick\"\nprint(f\"The town of {city} can be found at \" + \n      f\"{abs(myData['Longitude'][myData['Name'].index(city)])}ºW, {myData['Latitude'][myData['Name'].index(city)]}ºN\")\n\nThe town of Lerwick can be found at 1.145ºW, 60.155ºN\n\n\nThe first thing to do is to pull apart the print statement: you can see that this is actually just two ‘f-strings’ joined by a +–having that at the end of the line tells Python that it should carry on to the next line. That’s a handy way to make your code a little easier to read. If you’re creating a list and it’s getting a little long, then you can also continue a line using a , as well!\n\n1. The first f-string\nThe first string will help you to make sense of the second: f-strings allow you to ‘interpolate’ a variable into a string directly rather than having to have lots of str(x) + \" some text \" + str(y). You can write f\"{x} some text {y}\" and Python will automatically convert the variables x and y to strings and replace {x} with the value of x and {y} with the value of y.\nSo here f\"The town of {city} can be found at \" becomes f\"The town of Lerwick can be found at \" because {city} is replaced by the value of the variable city. This makes for code that is easier for humans to read and so I’d consider that a good thing.\n\n\n2. The second f-string\nThis one is hard because there’s just a lot of code there. But, again, if we start with what we recognise that it gets just a little bit more manageable… Also, it stands to reason that the only difference between the two outputs is that one asks for the ‘Longitude’ and the other for the ‘Latitude’. So if you can make sense of one you have automatically made sense of the other and don’t need to work it all out.\nLet’s start with a part that you might recognise:\n\nmyData['Name'].index(city)\n\n5\n\n\nYou’ve got this. This is just asking Python to work out the index of Lerwick (because city = 'Lerwick'). So it’s a number. 5 in this case. And we can then think, ’OK so what does this return:\n\nmyData['Longitude'][5]\n\n-1.145\n\n\nAnd the answer is -1.145. That’s the Longitude of Lerwick! There’s just one last thing: notice that we’re talking about degrees West here. So the answer isn’t a negative (because negative West degrees would be East!), it’s the absolute value. And that is the final piece of the puzzle: abs(...) gives us the absolute value of a number!\n\nhelp(abs)\n\nHelp on built-in function abs in module builtins:\n\nabs(x, /)\n    Return the absolute value of the argument.\n\n\n\n\n\n\nThe Average City Size\nHere we’re going to ‘cheat’ a little bit: rather than writing our own function, we’re going to import a package and use someone else’s function. The numpy package contains a lot of useful functions that we can call on (if you don’t believe me, add “dir(np)” on a new line after the import statement), and one of them calculates the average of a list or array of data.\n\nprint(f\"The mean population is {np.mean(myData['Population'])}\")\n\nThe mean population is 2435442.5\n\n\nThis is where our new approach really comes into its own: because all of the population data is in one place (a.k.a. a series or column), we can just throw the whole list into the np.mean function rather than having to use all of those convoluted loops and counters. Simples, right?\nNo, not simple at all, but we’ve come up with a way to make it simple.\n\n\nRecap!\nSo the really clever bit in all of this isn’t switching from a list-of-lists to a dictionary-of-lists, it’s recognising that the dictionary-of-lists is a better way to work with the data that we’re trying to analyse and that that there are useful functions that we can exploit to do the heavy lifting for us. Simply by changing the way that we stored the data in a ‘data structure’ (i.e. complex arrangement of lists, dictionaries, and variables) we were able to do away with lots of for loops and counters and conditions, and reduce many difficult operations to something that could be done on one line!"
  },
  {
    "objectID": "practicals/Think_Data.html#brain-teaser",
    "href": "practicals/Think_Data.html#brain-teaser",
    "title": "Think Data",
    "section": "Brain Teaser",
    "text": "Brain Teaser\n\n\n\n\n\n\nDifficulty: 🤯.\n\n\n\n\n\n\n\n\n\nWhy not have a stab at writing the code to print out the 4th most populous city? This can still be done on one line, though you might want to start by breaking the problem down:\n\nHow do I find the 4th largest value in a list?\nHow do I find the index of the 4th largest value in a list?\nHow do I use that to look up the name associated with that index?\n\nYou’ve already done #2 and #3 above so you’ve solved that problem. If you can solve #1 then the rest should fall into place.\n\n\n\n\n\n\nTip\n\n\n\nYou don’t want to use &lt;list&gt;.sort() because that will sort your data in place and break the link between the indexes across the ‘columns’; you want to research the function sorted(&lt;list&gt;) where &lt;list&gt; is the variable that holds your data and sorted(...) just returns whatever you pass it in a sorted order without changing the original list. You’ll see why this matters if you get the answer… otherwise, wait a few days for the answers to post.\n\n\n\nQuestionAnswer\n\n\n# Print out the name of the 4th most populous city-region\ncity = ??\n\nprint(\"The fourth most populous city is: \" + str(city))\n\n\n# Print out the name of the 4th most populous city-region\ncity = myData['Name'][\n    myData['Population'].index(sorted(myData['Population'], reverse=True)[3])\n]\n\nprint(\"The fourth most populous city is: \" + str(city))\nThe fourth most populous city is: Edinburgh\n\n\n\nThe answer is Edinburgh."
  },
  {
    "objectID": "practicals/Think_Data.html#bringing-it-all-together",
    "href": "practicals/Think_Data.html#bringing-it-all-together",
    "title": "Think Data",
    "section": "Bringing it all together…",
    "text": "Bringing it all together…\nConceptually, this is one of the hardest practicals in the entire term because it joins up so many of the seemingly simple ideas that you covered in Code Camp into a very complex ‘stew’ – all our basic ingredients (lists, dictionaries, etc.) have simmered for a bit, been stirred up together, and become something entirely new and more complex.\nSo if this practical doesn’t make sense to you on the first runthrough, I’d suggest going back through the second half of the practical again in a couple of days’ time – that will give your brain a little time to wrap itself around the basics before you throw the hard stuff at it again. Don’t panic if it doesn’t all make sense on the second runthrough either – this is like a language, you need to practice! With luck, the second time you went through this code a little bit more made sense. If you need to do it a third time you’ll find that even more makes sense… and so on.\nThis is a very challenging notebook because it takes you both through the process of building a function incrementally and through a ‘simple’ example of how Python classes actually work. You will need to understand these two very different elements in order to make the most of the remaining 6 weeks of term, because we both improve our code incrementally and make use of objects and their inheritances extensively. You also get an extra chance to revisit the differences between LoLs and DoLs because you will undoubtedly encounter and make use of these data structures even after you become a skillfull Python programmer.\n\n\n\n\n\n\nWarning\n\n\n\nThis is a very challenging practical and you should do your best to ensure that you actually understand what you have done and why.\n\n\n\n\n\n\n\n\nGroup Sign-Up\n\n\n\nYou should now make it a priority Sign Up!"
  },
  {
    "objectID": "practicals/Think_Data.html#the-way-that-doesnt-work",
    "href": "practicals/Think_Data.html#the-way-that-doesnt-work",
    "title": "Think Data",
    "section": "The Way That Doesn’t Work",
    "text": "The Way That Doesn’t Work\nRecall that this is how four rows of ‘data’ for city sizes organised by row as a list-of-lists look:\n\nmyData = [\n    ['id', 'Name', 'Rank', 'Longitude', 'Latitude', 'Population'], \n    ['1', 'Greater London', '1', '-18162.92767', '6711153.709', '9787426'], \n    ['2', 'Greater Manchester', '2', '-251761.802', '7073067.458', '2553379'], \n    ['3', 'West Midlands', '3', '-210635.2396', '6878950.083', '2440986']\n]\n\nTo print out a list of every city in the data set when we don’t know where the Name column is in the file we have to jump through some hoops:\n\ncities = []\n\ncol    = myData[0].index('Name')\nfor i in range(1, len(myData)):\n    cities.append(myData[i][col])\n\nprint(\"The cities in the data set are: \" + \", \".join(cities))\n\nThe cities in the data set are: Greater London, Greater Manchester, West Midlands\n\n\nAnd it’s the same kind of faff if we want to find out if Edinburgh is included in the data set:\n\ncol   = myData[0].index('Name')\nfound = False\nfor i in range(1, len(myData)):\n    if myData[i][col] == 'Edinburgh':\n        print(\"Found Edinburgh in the data set!\")\n        found = True\n        break\n\nif found == False:\n    print(\"Didn't find Edinburgh in the data set.\")\n\nDidn't find Edinburgh in the data set."
  },
  {
    "objectID": "practicals/Think_Data.html#the-way-that-does-work",
    "href": "practicals/Think_Data.html#the-way-that-does-work",
    "title": "Think Data",
    "section": "The Way That Does Work",
    "text": "The Way That Does Work\nCompare that code to how it works for a dictionary-of-lists organised by column. Now try printing out the cities in the data:\n\nmyData = {\n    'id'         : [0, 1, 2, 3, 4, 5],\n    'Name'       : ['Greater London', 'Greater Manchester', 'Birmingham','Edinburgh','Inverness','Lerwick'],\n    'Rank'       : [1, 2, 3, 4, 5, 6],\n    'Longitude'  : [-0.128, -2.245, -1.903, -3.189, -4.223, -1.145],\n    'Latitude'   : [51.507, 53.479, 52.480, 55.953, 57.478, 60.155],\n    'Population' : [9787426, 2705000, 1141816, 901455, 70000, 6958],\n}\n\nTo print out a list of every city in the data set:\n\nprint(\", \".join(myData['Name']))\n\nGreater London, Greater Manchester, Birmingham, Edinburgh, Inverness, Lerwick\n\n\nTo find out if Edinburgh is included in the list of data:\n\nif 'Edinburgh' in myData['Name']:\n    print(\"Found Edinburgh in the data set!\")\nelse:\n    print(\"Didn't find Edinburgh in the data set.\")\n\nFound Edinburgh in the data set!\n\n\nSee how even basic questions like “Is Edinburgh in our list of data?” are suddenly easy to answer? We no longer need to loop over the entire data set in order to find one data point. In addition, we know that everything in the ‘Name’ column will be a string, and that everything in the ‘Longitude’ column is a float, while the ‘Population’ column contains integers. So that’s made life easier already. But let’s test this out and see how it works."
  },
  {
    "objectID": "practicals/Think_Data.html#calculate-mean",
    "href": "practicals/Think_Data.html#calculate-mean",
    "title": "Think Data",
    "section": "Calculate Mean",
    "text": "Calculate Mean\nLet’s start by calculating the sample mean (use Google: Python numpy mean...):\n\nimport numpy as np\n# Use numpy functions to calculate mean and standard deviation\nmean = np.mean(myData['Population'])\nprint(f\"City distribution has a mean of {mean:,.0f}.\")\n\nCity distribution has a mean of 2,435,442."
  },
  {
    "objectID": "practicals/Think_Data.html#calculate-standard-deviation",
    "href": "practicals/Think_Data.html#calculate-standard-deviation",
    "title": "Think Data",
    "section": "Calculate Standard Deviation",
    "text": "Calculate Standard Deviation\n\n\n\n\n\n\nDifficulty level: Low-ish.\n\n\n\n\n\n\n\n\n\nNow let’s do the standard deviation:\n\nQuestionAnswer\n\n\nimport numpy as np\n# Use numpy functions to calculate mean and standard deviation\nstd  = np.??(??)\nprint(f\"City distribution has a standard deviation of {std:,.2f}.\")\n\n\nimport numpy as np\n# Use numpy functions to calculate mean and standard deviation\nstd  = np.std(myData['Population'])\nprint(f\"City distribution has a standard deviation of {std:,.2f}.\")\nCity distribution has a standard deviation of 3,406,947.93.\n\n\n\nSo the numpy package gives us a way to calculate the mean and standard deviation quickly and without having to reinvent the wheel. The other potentially new thing here is {std:,.2f}. This is about string formatting and the main thing to recognise is that this means ‘format this float with commas separating the thousands/millions and 2 digits to the right’. The link I’ve provided uses the slightly older approach of &lt;str&gt;.format() but the formatting approach is the same."
  },
  {
    "objectID": "practicals/Think_Data.html#for-loops-without-for-loops",
    "href": "practicals/Think_Data.html#for-loops-without-for-loops",
    "title": "Think Data",
    "section": "For Loops Without For Loops",
    "text": "For Loops Without For Loops\n\n\n\n\n\n\nDifficulty level: Medium.\n\n\n\n\n\n\n\n\n\nNow we’re going to see something called a List Comprehension.\nIn Python you will see code like this a lot: [x for x in list]. This syntax is known as a ‘list comprehension’ and is basically a for loop on one line with the output being assigned to a list. So we can apply an operation (converting to a string, subtracting a value, etc.) to every item in a list without writing out a full for loop.\nHere’s a quick example just to show you what’s going on:\n\ndemo = range(0,10) # &lt;- a *range* of numbers between 0 and 9 (stop at 10)\nprint([x**2 for x in demo]) # square every element of demo\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nNow let’s apply this to our problem. We calculated the the mean and standard deviation above, so now we want to apply the z-score formula to every element of the Population list… Remember that the format for the z-score (when dealing with a sample) is:\n\\[\nz = \\frac{x - \\bar{x}}{s}\n\\]\nAnd the population standard deviation (by which I mean, if you are dealing with all the data, and not a subsample as we are here) is:\n\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\n\nQuestionAnswer\n\n\nrs = [(x - ??)/?? for x in myData['Population']] # rs == result set\nprint([f\"{x:.3f}\" for x in rs])\n\n\nrs = [(x - mean)/std for x in myData['Population']] # rs == result set\nprint([f\"{x:.3f}\" for x in rs])\n['2.158', '0.079', '-0.380', '-0.450', '-0.694', '-0.713']"
  },
  {
    "objectID": "practicals/Think_Data.html#appending",
    "href": "practicals/Think_Data.html#appending",
    "title": "Think Data",
    "section": "Appending",
    "text": "Appending\n\n\n\n\n\n\nDifficulty level: trivial\n\n\n\n\n\n\n\n\n\nAnd now let’s add it to the data set:\n\nmyData['Std. Population'] = rs\nprint(myData['Std. Population'])\n\n[2.1579383252868527, 0.0791199354729932, -0.3797024575689938, -0.45025269939207097, -0.6942995760276591, -0.7128035277711219]\n\n\nAnd just to show how everything is in a single data structure:\n\nfor c in myData['Name']:\n    idx = myData['Name'].index(c)\n    print(f\"{c} has a population of {myData['Population'][idx]:,} and standardised score of {myData['Std. Population'][idx]:.3f}\")\n\nGreater London has a population of 9,787,426 and standardised score of 2.158\nGreater Manchester has a population of 2,705,000 and standardised score of 0.079\nBirmingham has a population of 1,141,816 and standardised score of -0.380\nEdinburgh has a population of 901,455 and standardised score of -0.450\nInverness has a population of 70,000 and standardised score of -0.694\nLerwick has a population of 6,958 and standardised score of -0.713"
  },
  {
    "objectID": "practicals/Think_Data.html#downloading-from-a-url",
    "href": "practicals/Think_Data.html#downloading-from-a-url",
    "title": "Think Data",
    "section": "Downloading from a URL",
    "text": "Downloading from a URL\nLet’s focus on the first part first because that’s the precondition for everything else. If we can get the ‘download a file from a URL’ working then the rest will gradually fall into place through iterative improvments!\n\nFinding an Existing Answer\n\n\n\n\n\n\nDifficulty level: Low\n\n\n\n\n\n\n\n\n\nFirst, let’s be sensibly lazy–we’ve already written code to read a file from the Internet and turn it into a list of lists. So I’ve copy+pasted that into the code block below since we’re going to start from this point; however, just to help you check your own understanding, I’ve removed a few bits and replaced them with ??. Sorry, it’s good practice. 😈\n\nQuestionAnswer\n\n\nfrom urllib.request import urlopen\nimport csv\n\nurl = \"https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv\"\n\nurlData = [] # Somewhere to store the data\n\nresponse = urlopen(url) # Get the data using the urlopen function\ncsvfile  = csv.reader(response.read().decode('utf-8').splitlines()) # Pass it over to the reader\n\nfor row in csvfile:\n    urlData.append(??)\n\nprint(\"urlData has \" + str(len(urlData)) + \" rows and \" + str(len(urlData[0])) + \" columns.\")\nprint(urlData[-1]) # Check it worked!\nYou should get:\nurlData has 11 rows and 4 columns.  [‘Bangor’, ‘18808’, ‘53.228’, ‘-4.128’]\n\n\nfrom urllib.request import urlopen\nimport csv\n\nurl = \"https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv\"\n\nurlData = [] # Somewhere to store the data\n\nresponse = urlopen(url) # Get the data using the urlopen function\ncsvfile  = csv.reader(response.read().decode('utf-8').splitlines()) # Pass it over to the reader\n\nfor row in csvfile:\n    urlData.append(row)\n\nprint(\"urlData has \" + str(len(urlData)) + \" rows and \" + str(len(urlData[0])) + \" columns.\")\nprint(urlData[-1]) # Check it worked!\n\n\n\n\n\nGetting Organised\n\n\n\n\n\n\nDifficulty level: Low\n\n\n\n\n\n\n\n\n\nLet’s take the code above and modify it so that it is:\n\nA function that takes two arguments: a URL; and a destination filename.\nImplemented as a function that checks if a file exists already before downloading it again.\n\nYou will find that the os module helps here because of the path function. And you will need to Google how to test if a file exists. I would normally select a StackOverflow link in the results list over anything else because there will normally be an explanation included of why a particular answer is a ‘good one’. I also look at which answers got the most votes (not always the same as the one that was the ‘accepted answer’). In this particular case, I also found this answer useful.\nI would start by setting my inputs:\n\nimport os\nurl = \"https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv\"\nout = os.path.join('data','Wikipedia-Cities.csv') # Print `out` if you aren't sure what this has done!\n\n\n\nSketching the Function\n\n\n\n\n\n\nDifficulty level: Low, if you’ve watched the videos…\n\n\n\n\n\n\n\n\n\nThen I would sketch out how my function will work using comments. And the simplest thing to start with is checking whether the file has already been downloaded:\n\nQuestionAnswer\n\n\nfrom urllib.request import urlopen\n\ndef get_url(src, dest):\n    \n    # Check if dest exists -- if it does\n    # then we can skip downloading the file,\n    # otherwise we have to download it!\n    if os.path.isfile(??):\n        print(f\"{dest} found!\")\n    else:\n        print(f\"{dest} *not* found!\")\n        \nget_url(url, out)\n\n\nfrom urllib.request import urlopen\n\ndef get_url(src, dest):\n\n    # Check if dest exists -- if it does\n    # then we can skip downloading the file,\n    # otherwise we have to download it!\n    if os.path.isfile(dest):\n        print(f\"{dest} found!\")\n    else:\n        print(f\"{dest} *not* found!\")\n\nget_url(url, out)\ndata/Wikipedia-Cities.csv found!\n\n\n\n\n\nFleshing Out the Function\n\n\n\n\n\n\nDifficulty level: Medium\n\n\n\n\n\nIf you really explore what’s going on in the function rather than just running it and moving on.\n\n\n\nI would then flesh out the code so that it downloads the file if it isn’t found and then, either way, returns the local file path for our CSV reader to extract:\n\ndef get_url(src, dest):\n    \n    # Check if dest does *not* exist -- that\n    # would mean we had to download it!\n    if os.path.isfile(dest):\n        print(f\"{dest} found locally!\")\n    else:\n        print(f\"{dest} not found, downloading!\")\n        \n        # Get the data using the urlopen function\n        response = urlopen(src) \n        filedata = response.read().decode('utf-8')\n        \n        # Extract the part of the dest(ination) that is *not*\n        # the actual filename--have a look at how \n        # os.path.split works using `help(os.path.split)`\n        path = list(os.path.split(dest)[:-1])\n        \n        # Create any missing directories in dest(ination) path\n        # -- os.path.join is the reverse of split (as you saw above)\n        # but it doesn't work with lists... so I had to google how \n        # to use the 'splat' operator! os.makedirs creates missing \n        # directories in a path automatically.\n        if len(path) &gt;= 1 and path[0] != '':\n            os.makedirs(os.path.join(*path), exist_ok=True)\n        \n        with open(dest, 'w') as f:\n            f.write(filedata)\n            \n        print(f\"Data written to {dest}!\")\n    \n    return dest\n        \n# Using the `return contents` line we make it easy to \n# see what our function is up to.\nsrc = get_url(url, out)\n\ndata/Wikipedia-Cities.csv found locally!"
  },
  {
    "objectID": "practicals/Think_Data.html#decorating",
    "href": "practicals/Think_Data.html#decorating",
    "title": "Think Data",
    "section": "Decorating!",
    "text": "Decorating!\nLet’s now look into simplifying this code using a dectorator! Our function has become a bit unwieldy and we want to look at how we can simplify that.\nThe ‘obvious’ (i.e. not obvious) way to do this is to implement the check for a local copy as a decorator on the downloading function. So we have a function that downloads, and a decorator function that checks if the download should even be triggered.\n\nfrom functools import wraps\ndef check_cache(f):\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        src  = args[0]\n        dest = args[1]\n        if os.path.isfile(dest):\n            print(f\"{dest} found locally!\")\n            return(dest)\n        else:\n            print(f\"{dest} not found, downloading!\")\n            return(f(src, dest))\n    return wrapper\n\n@check_cache\ndef get_url(src, dest):    \n    # Get the data using the urlopen function\n    response = urlopen(src) \n    filedata = response.read().decode('utf-8')\n     \n    # Extract the part of the dest(ination) that is *not*\n    # the actual filename--have a look at how \n    # os.path.split works using `help(os.path.split)`\n    path = list(os.path.split(dest)[:-1])\n     \n    # Create any missing directories in dest(ination) path\n    # -- os.path.join is the reverse of split (as you saw above)\n    # but it doesn't work with lists... so I had to google how \n    # to use the 'splat' operator! os.makedirs creates missing \n    # directories in a path automatically.\n    if len(path) &gt;= 1 and path[0] != '':\n        os.makedirs(os.path.join(*path), exist_ok=True)\n     \n    with open(dest, 'w') as f:\n        f.write(filedata)\n         \n    print(f\"Data written to {dest}!\")\n    \n    return dest\n        \n# Using the `return contents` line we make it easy to \n# see what our function is up to.\nsrc = get_url(url, out)\n\ndata/Wikipedia-Cities.csv found locally!\n\n\nI’m not going to pretend that’s the best use of a decorator, but it does neatly separate the downloading function from the caching function. In fact, there is already a cache decorator and some of these have unlimited capacity; however, they are intended to run in a ‘live’ context, so you’d still need to download the file again any time you start a new notebook or restart Docker. This caching function saves the actual data locally to dest.\n\n\n\n\n\n\nStop!\n\n\n\nIt really would be a good idea to put in the effort to make sense of how this function works. There is a lot going on here and understanding how this function works will help you to understand how to code. You should notice that we don’t try to check if the data file contains any useful data! So if you download or create an empty file while testing, you won’t necessarily get an error until you try to turn it into data afterwards!"
  },
  {
    "objectID": "practicals/Think_Data.html#parse-the-csv-file",
    "href": "practicals/Think_Data.html#parse-the-csv-file",
    "title": "Think Data",
    "section": "Parse the CSV File",
    "text": "Parse the CSV File\n\n\n\n\n\n\nDifficulty: Low\n\n\n\n\n\n\n\n\n\nNow we turn to the next task: parsing the file if it’s a CSV. This implies that it might not be so that’s something we should also consider!\n\nQuestionAnswer\n\n\nimport csv\n\ndef read_csv(src):\n    \n    csvdata = []\n    with open(src, 'r') as f:\n        csvr = csv.??(f)\n        \n        for r in csvr:\n            csvdata.append(??)\n    \n    # Return list of lists\n    return ??\n\nread_csv(src)\n#read_csv('foo.bar') # &lt;- Notice what happens if you try to run this code\n#read_csv('Practical-04-Objects-Answers.ipynb') # Or this code!\n\n\nimport csv\n\ndef read_csv(src):\n\n    csvdata = []\n    with open(src, 'r') as f:\n        csvr = csv.reader(f)\n\n        for r in csvr:\n            csvdata.append(r)\n\n    # Return list of lists\n    return csvdata\n\nread_csv(src)\n#read_csv('foo.bar') # &lt;- Notice what happens if you try to run this code\n#read_csv('Practical-04-Objects-Answers.ipynb') # Or this code!\n[['City', 'Population', 'Latitude', 'Longitude'],\n ['Perth', '45770', '56.39583', '-3.43333'],\n ['Armagh', '14777', '54.3499', '-6.6546'],\n ['Dundee', '147268', '56.462', '-2.9707'],\n ['Colchester', '194706', '51.88861', '0.90361'],\n ['Salisbury', '40302', '51.07', '-1.79'],\n ['Portsmouth', '205056', '50.80583', '-1.08722'],\n ['Wakefield', '325837', '53.683', '-1.499'],\n ['Bradford', '522452', '53.792', '-1.754'],\n ['Lancaster', '138375', '54.047', '-2.801'],\n ['Bangor', '18808', '53.228', '-4.128']]\n\n\n\nYou should get:\n [[‘City’, ‘Population’, ‘Latitude’, ‘Longitude’], [‘Perth’, ‘45770’, ‘56.39583’, ‘-3.43333’], [‘Armagh’, ‘14777’, ‘54.3499’, ‘-6.6546’], [‘Dundee’, ‘147268’, ‘56.462’, ‘-2.9707’], [‘Colchester’, ‘194706’, ‘51.88861’, ‘0.90361’], [‘Salisbury’, ‘40302’, ‘51.07’, ‘-1.79’], [‘Portsmouth’, ‘205056’, ‘50.80583’, ‘-1.08722’], [‘Wakefield’, ‘325837’, ‘53.683’, ‘-1.499’], [‘Bradford’, ‘522452’, ‘53.792’, ‘-1.754’], [‘Lancaster’, ‘138375’, ‘54.047’, ‘-2.801’], [‘Bangor’, ‘18808’, ‘53.228’, ‘-4.128’]]"
  },
  {
    "objectID": "practicals/Think_Data.html#convert-the-csv-into-a-dol",
    "href": "practicals/Think_Data.html#convert-the-csv-into-a-dol",
    "title": "Think Data",
    "section": "Convert the CSV into a DoL",
    "text": "Convert the CSV into a DoL\n\n\n\n\n\n\nDifficulty: Medium.\n\n\n\n\n\n\n\n\n\nNow we can focus on converting the CSV data to a dictionary-of-lists! We’re going to start with the same function name but expand what the function does. This kind of iteration is common in software development.\n\nQuestionAnswer\n\n\ndef read_csv(src):\n    \n    csvdata = {} # An empty dictionary-of-lists\n    \n    with open(??, 'r') as f:\n        csvr = csv.reader(f)\n        \n        # Read in our column names and\n        # initialise the dictionary-of-lists\n        csvcols = next(csvr) \n        for c in csvcols:\n            csvdata[c] = []\n        \n        # Notice this code is still the same, \n        # we just used next(csvr) to get the \n        # header row first!\n        for r in ??: \n            # Although you can often assume that the order \n            # of the keys is the same, Python doesn't \n            # guarantee it; this way we will always make\n            # the correct assignment.\n            for idx, c in enumerate(csvcols):\n                csvdata[??].append(r[idx])\n    \n    # Return dictionary of lists\n    return csvdata\n\nread_csv(src)\n\n\nimport csv\n\ndef read_csv(src):\n\n    csvdata = {} # An empty dictionary-of-lists\n\n    with open(src, 'r') as f:\n        csvr = csv.reader(f)\n\n        # Read in our column names and\n        # initialise the dictionary-of-lists\n        csvcols = next(csvr)\n        for c in csvcols:\n            csvdata[c] = []\n\n        # Notice this code is still the same,\n        # we just used next(csvr) to get the\n        # header row first!\n        for r in csvr:\n            # Although you can often assume that the order\n            # of the keys is the same, Python doesn't\n            # guarantee it; this way we will always make\n            # the correct assignment.\n            for idx, c in enumerate(csvcols):\n                csvdata[c].append(r[idx])\n\n    # Return dictionary of lists\n    return csvdata\n\nread_csv(src)\n{'City': ['Perth',\n  'Armagh',\n  'Dundee',\n  'Colchester',\n  'Salisbury',\n  'Portsmouth',\n  'Wakefield',\n  'Bradford',\n  'Lancaster',\n  'Bangor'],\n 'Population': ['45770',\n  '14777',\n  '147268',\n  '194706',\n  '40302',\n  '205056',\n  '325837',\n  '522452',\n  '138375',\n  '18808'],\n 'Latitude': ['56.39583',\n  '54.3499',\n  '56.462',\n  '51.88861',\n  '51.07',\n  '50.80583',\n  '53.683',\n  '53.792',\n  '54.047',\n  '53.228'],\n 'Longitude': ['-3.43333',\n  '-6.6546',\n  '-2.9707',\n  '0.90361',\n  '-1.79',\n  '-1.08722',\n  '-1.499',\n  '-1.754',\n  '-2.801',\n  '-4.128']}\n\n\n\nYou should get something that starts:\n\n\n{'City': ['Perth', 'Armagh', 'Dundee', 'Colchester', 'Salisbury', 'Portsmou..."
  },
  {
    "objectID": "practicals/Think_Data.html#adding-docstring",
    "href": "practicals/Think_Data.html#adding-docstring",
    "title": "Think Data",
    "section": "Adding Docstring",
    "text": "Adding Docstring\n\n\n\n\n\n\nDifficulty: Low\n\n\n\n\n\n\n\n\n\nWe’ve assumed that the first row of our data set is always a header (i.e. list of column names). If it’s not then this code is going to have problems. A robust function would allow us to specify column names, skip rows, etc. when we create the data structure, but let’s not get caught up in that level of detail. Notice that I’ve also, for the first time:\n\nUsed the docstring support offered by Python. You’ll be able to use help(...) and get back the docstring help!\nProvided hints to Python about the expected input and output data types. This can help to ensure consistency and is also critical in testing / continuous integration when working with others on a codebase.\n\n\ndef read_csv(src:str) -&gt; dict:\n    \"\"\"\n    Converts a CSV file to a dictionary-of-lists (dol),\n    using the first row to create column names.\n    \n    param src: a local CSV file\n    returns: a dictionary-of-lists\n    \"\"\"\n    csvdata = {} # An empty dictionary-of-lists\n    \n    with open(src, 'r') as f:\n        csvr = csv.reader(f)\n        \n        # Read in our column names and\n        # initialise the dictionary-of-lists\n        csvcols = next(csvr) \n        for c in csvcols:\n            csvdata[c] = []\n        \n        # Notice this code is still the same, \n        # we just used next(csvr) to get the \n        # header row first!\n        for r in csvr: \n            # Although you can often assume that the order \n            # of the keys is the same, Python doesn't \n            # guarantee it; this way we will always make\n            # the correct assignment.\n            for idx, c in enumerate(csvcols):\n                csvdata[c].append(r[idx])\n    \n    # Return dictionary of lists\n    return csvdata\n\nds = read_csv(src)\n\n\nhelp(read_csv)\n\nHelp on function read_csv in module __main__:\n\nread_csv(src: str) -&gt; dict\n    Converts a CSV file to a dictionary-of-lists (dol),\n    using the first row to create column names.\n\n    param src: a local CSV file\n    returns: a dictionary-of-lists\n\n\n\n\nprint(\"Columns are: \" + \", \".join(ds.keys()))\nprint(f\"First two cities are: {ds['City'][:2]}\")\nprint(f\"First two populations are: {ds['Population'][:2]}\")\nprint(f\"First two latitudes are: {ds['Latitude'][:2]}\")\n\nColumns are: City, Population, Latitude, Longitude\nFirst two cities are: ['Perth', 'Armagh']\nFirst two populations are: ['45770', '14777']\nFirst two latitudes are: ['56.39583', '54.3499']"
  },
  {
    "objectID": "practicals/Think_Data.html#creating-a-package",
    "href": "practicals/Think_Data.html#creating-a-package",
    "title": "Think Data",
    "section": "Creating a Package",
    "text": "Creating a Package\nWe’re not going to tackle this now, but it’s important that you understand how what we’ve done connects to what we’re about to do, and the concept of a package is the bridge. We’ve already covered this in the pre-recorded lectures, but if you want to actually try to create your own package, the simplest way to do this is to:\n\nCopy the read_csv into a new file called, for instance, utils.py.\nMake sure you delete this function from the current ‘namespace’ (del(read_csv)) by which I mean that the read_csv function no longer exists (running help(read_csv) should give you an error!).\nTry importing the function from the file: from utils import read_csv and run the help(read_csv) code again.\n\nAssuming that you’ve done everything correctly, we’ve now brought in code from another file without having to write it into our main Python script file. In Python, many of the most complex libraries are spread across the equivalent of many utils.py files, but on top of that when we import and run them they are also creating objects from classes defined in those files.\nWhat we now want to do is use a fairly simple example using different ‘shapes’ (pyramids, cubes, etc.) that allow us to explore how classes work through inheritance from parents and can extend of overwrite the functionality provided by the parent class. We’ll need this understanding in order to grasp how Pandas and GeoPandas work specifically, but also how Python works more generally."
  },
  {
    "objectID": "practicals/Think_Data.html#abstract-base-class",
    "href": "practicals/Think_Data.html#abstract-base-class",
    "title": "Think Data",
    "section": "Abstract Base Class",
    "text": "Abstract Base Class\nThis class appears to do very little, but there are two things to notice:\n\nIt provides a constructor (__init__) that sets the shape_type to the name of the class automatically (so a square object has shape_type='Square') and it stores the critical dimension of the shape in self.dim.\nIt provides methods (which only raise exceptions) that will allow one shape to be used in the place of any other shape that inherits from shape.\n\n\n# Base class shape\nclass shape(object): # Inherit from base class \n    def __init__(self, dimension:float=None):\n        self.shape_type = self.__class__.__name__.capitalize()\n        self.dim = dimension\n        return\n    \n    def diameter(self):\n        raise Exception(\"Unimplmented method error.\")\n    \n    def volume(self):\n        raise Exception(\"Unimplmented method error.\")\n    \n    def surface(self):\n        raise Exception(\"Unimplmented method error.\")\n        \n    def type(self):\n        return(self.shape_type)\n\nWe can now create a new shape object (an instance of the shape class) but we can’t do much that is useful with it:\n\ns = shape(15)\n\ntry: \n    print(f\"I am a {s.type()}\")\n    print(f\"My volume is {s.volume()}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\nI am a Shape\nError: Unimplmented method error."
  },
  {
    "objectID": "practicals/Think_Data.html#cube",
    "href": "practicals/Think_Data.html#cube",
    "title": "Think Data",
    "section": "Cube",
    "text": "Cube\nImplements a cube:\n\nThe diameter of the cube is given by the Pythagorean formula for the length of the hypotenuse in 3D between opposing corners: \\(\\sqrt{d^{2} + d^{2} + d^{2}}\\) which we can reduce to \\(\\sqrt{3 d^{2}}\\).\nA cube’s volume is given by \\(d^{3}\\).\nA cube’s surface area will be the sum of its six faces: \\(6d^{2}\\).\n\n\nQuestionAnswer\n\n\nCan you work out the missing elements that will allow you to create a cube class?\n# Cube class\nclass cube(shape): # Inherit from shape \n    def __init__(self, dim:float):\n        super().__init__(dim)\n        return\n    \n    def diameter(self):\n        return (3 * self.??**2)**(1/2)\n    \n    def volume(self):\n        return self.dim**3\n    \n    def surface(self):\n        return ??*(self.dim**2)\n\n# If you've done everything correctly then\n# you will no longer get an error here...\ns = cube(15)\n\ntry: \n    print(f\"I am a {s.type()}\")\n    print(f\"My volume is {s.volume()}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n\n# Cube class\nclass cube(shape): # Inherit from shape\n    def __init__(self, dim:float):\n        super().__init__(dim)\n        return\n\n    def diameter(self):\n        return (3 * self.dim**2)**(1/2)\n\n    def volume(self):\n        return self.dim**3\n\n    def surface(self):\n        return 6*(self.dim**2)\n\n# If you've done everything correctly then\n# you will no longer get an error here...\ns = cube(15)\n\ntry: \n    print(f\"I am a {s.type()}\")\n    print(f\"My volume is {s.volume()}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\nI am a Cube\nMy volume is 3375"
  },
  {
    "objectID": "practicals/Think_Data.html#sphere",
    "href": "practicals/Think_Data.html#sphere",
    "title": "Think Data",
    "section": "Sphere",
    "text": "Sphere\nImplements a sphere:\n\nThe diameter is twice the critical dimension (radius): \\(2r\\).\nThe volume is \\(\\frac{4}{3} \\pi r^{3}\\).\nThe surface area will be \\(4 \\pi r^{2}\\).\n\nIf we were writing something more general, we’d probably have spheres as a special case of an ellipsoid!\n\nQuestionAnswer\n\n\nCan you work out the missing elements that will allow you to create a cube class?\n# Sphere class\nfrom math import pi\nclass sphere(shape): # Inherit from shape\n    def __init__(self, dim:float):\n        # Something...\n\n    def diameter(self):\n        # Something...\n\n    def volume(self):\n        # Something\n\n    def surface(self):\n        # Something\n\n# If you've done everything correctly then\n# you will no longer get an error here...\ns = sphere(15)\n\ntry: \n    print(f\"I am a {s.type()}\")\n    print(f\"My volume is {s.volume()}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n\nfrom math import pi\n# Sphere class\nclass sphere(shape): # Inherit from shape\n    def __init__(self, dim:float):\n        super().__init__(dim)\n        return\n\n    def diameter(self):\n        return self.dim*2\n\n    def volume(self):\n        return (4/3) * pi * self.dim**3\n\n    def surface(self):\n        return 4 * pi * (self.dim**2)\n\n# If you've done everything correctly then\n# you will no longer get an error here...\ns = sphere(15)\n\ntry: \n    print(f\"I am a {s.type()}\")\n    print(f\"My volume is {s.volume()}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\nI am a Sphere\nMy volume is 14137.166941154068"
  },
  {
    "objectID": "practicals/Think_Data.html#regular-pyramid",
    "href": "practicals/Think_Data.html#regular-pyramid",
    "title": "Think Data",
    "section": "Regular Pyramid",
    "text": "Regular Pyramid\nWe’re taking this to be a regular pyramid where all sides are equal:\n\nThe diameter is a line drawn across the base between opposing corners of the base so it’s just \\(\\sqrt{d^{2} + d^{2}}\\).\nThe volume is given by \\(V = b * h / 3\\) (where \\(b\\) is the area of the base, which in this case becomes \\(d^{2} * h/3\\)).\nThe surface area will be the base + 4 equilateral triangles: \\(d^{2} + 4 (d^{2}\\sqrt{3}/4)\\) which we can reduce to \\(d^{2} + d^{2}\\sqrt{3}\\)\n\nBut this requires a height method that is specific to pyramids:\n\nThe height is taken from the centre of the pyramid (which will be half the length of the hypotenuse for two edges): \\(l = \\sqrt{d{^2} + d^{2}}\\) and the long side (\\(d\\) again) which gives us \\(\\sqrt{l/2 + d^{2}}\\).\n\n\n\n\n\n\n\nClass Variables\n\n\n\nNote that this has a class variable called has_mummies since Egyptian regular pyramids are plagued by them! This class variable is set automatically for all instances of the pyramid class. Changing this variable can have weird effects so they’re not often changed.\n\n\n\n# Pyramid class\nclass pyramid(shape): # Inherit from shape\n\n    has_mummies = True # This is for *all* regular pyramids\n\n    def __init__(self, dim:float):\n        super().__init__(dim)\n        self.shape_type = 'Regular Pyramid'\n        return\n\n    def diameter(self):\n        return (self.dim**2 + self.dim**2)**(1/2)\n\n    def height(self):\n        return (self.diameter()/2 + self.dim**2)**(1/2)\n\n    def volume(self):\n        return self.dim**2 * self.height() / 3\n\n    def surface(self):\n        return self.dim**2 + self.dim**2 * 3**(1/2)"
  },
  {
    "objectID": "practicals/Think_Data.html#triangular-pyramid",
    "href": "practicals/Think_Data.html#triangular-pyramid",
    "title": "Think Data",
    "section": "Triangular Pyramid",
    "text": "Triangular Pyramid\nWe have chosen for triangular pyramid to inherit from regular pyramid. However, this is kind of a judgement call since there’s very little shared between the two types of pyramid and it’s arguable whether this one is actually simpler and should therefore be the parent class…\nJust to note, as well, that since all sides are equal this is an equilateral triangular pyramid. Anyway, the calculations are:\n\nThe diameter (longest line through the shape) will just be the edge: \\(d\\).\nThe volume \\(V = b * h / 3\\) where \\(b\\) is the area of an equilateral triangle.\nThe surface area will be \\(4b\\) where \\(b\\) is the area of an equilateral triangle.\n\nSo we now need two new formulas:\n\nThe height of the pyramid using (Pythagoras again): \\(h = \\sqrt{6}d/3\\).\nThe area of an equilateral triangle: \\(\\frac{\\sqrt{3}}{4} d^{2}\\)\n\nTriangular pyramids do not have a problem with mummies.\nWhy don’t you add some documentation to this class and the regular pyramid class so that we know how to use them correctly?\n\n# Triangular Pyramid class\nclass t_pyramid(pyramid): # Inherit from regular pyramid\n\n    has_mummies = False # This is for all triangular pyramids\n\n    def __init__(self, dim:float):\n        super().__init__(dim)\n        self.shape_type = 'Triangular Pyramid'\n        return\n\n    def diameter(self):\n        return self.dim\n\n    def height(self):\n        # h = sqrt(6)/3 * d\n        return 6**(1/2)/3 * self.dim\n\n    def base(self):\n        return 3**(1/2)/4 * self.dim**2\n\n    def volume(self):\n        return (1/3) * self.base() * self.height()\n\n    def surface(self):\n        return 4 * self.base()"
  },
  {
    "objectID": "practicals/Think_Data.html#testing-your-classes",
    "href": "practicals/Think_Data.html#testing-your-classes",
    "title": "Think Data",
    "section": "Testing Your Classes",
    "text": "Testing Your Classes\nIf you’ve implemented everything correctly then the following code should run.\n# How would you test these changes?\ns = sphere(10)\nprint(s.type())\nprint(f\"\\tVolume is: {s.volume():5.2f}\")\nprint(f\"\\tDiameter is: {s.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {s.surface():5.2f}\")\nprint(\"\")\n\nc = cube(10)\nprint(c.type())\nprint(f\"\\tVolume is: {c.volume():5.2f}\")\nprint(f\"\\tDiameter is: {c.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {c.surface():5.2f}\")\nprint(\"\")\n\np = pyramid(10)\nprint(p.type())\nprint(f\"\\tVolume is: {p.volume():5.2f}\")\nprint(f\"\\tDiameter is: {p.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {p.surface():5.2f}\")\nprint(f\"\\tHeight is: {p.height():5.2f}\")\nif p.has_mummies is True:\n    print(\"\\tMummies? Aaaaaaaaargh!\")\nelse:\n    print(\"\\tPhew, no mummies!\")\nprint(\"\")\n\np2 = t_pyramid(10)\nprint(p2.type())\nprint(f\"\\tVolume is: {p2.volume():5.2f}\")\nprint(f\"\\tDiameter is: {p2.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {p2.surface():5.2f}\")\nprint(f\"\\tHeight is: {p2.height():5.2f}\")\nif p2.has_mummies is True:\n    print(\"\\tMummies? Aaaaaaaaargh!\")\nelse:\n    print(\"\\tPhew, no mummies!\")\nprint(\"\")\n\n# Useful demonstration of how to find out if a method or attribute is\n# associated with a particular object\nif hasattr(p2,'base_area'):\n    print(f\"Shape of type '{p2.type()}' has attribute or method 'base_area'\")\nelse:\n    print(f\"Shape of type '{p2.type()}' does *not* have attribute or method 'base_area'\")\nprint(\"\")\nI get the following output:\n\n\nSphere\n    Volume is: 4188.79\n    Diameter is: 20.00\n    Surface Area is: 1256.64\n\nCube\n    Volume is: 1000.00\n    Diameter is: 17.32\n    Surface Area is: 600.00\n\nRegular Pyramid\n    Volume is: 344.92\n    Diameter is: 14.14\n    Surface Area is: 273.21\n    Height is: 10.35\n    Mummies? Aaaaaaaaargh!\n\nTriangular Pyramid\n    Volume is: 117.85\n    Diameter is: 10.00\n    Surface Area is: 173.21\n    Height is:  8.16\n    Phew, no mummies!\n\nShape of type 'Triangular Pyramid' does *not* have attribute or method 'base_area'"
  },
  {
    "objectID": "practicals/Think_Data.html#packaging-it-up",
    "href": "practicals/Think_Data.html#packaging-it-up",
    "title": "Think Data",
    "section": "Packaging It Up",
    "text": "Packaging It Up\nWait, you’re still working on this practical and haven’t thrown up your hands in disgust yet? OK, in that case you can have one more thing to do: turn all the shapes into a package that can be loaded via an import statement.\n\nCell Magic\nThis code allows Jupyter to reload external libraries if they are edited after you import them. When you are working on your own packages this is rather useful since you tend to make a lot of mistakes when packaging code up this way and it’s handy not to have to restart the entire notebook every time you fix a typo or change a function.\n%load_ext autoreload\n%autoreload 2\n\n\nImport Shapes\nMy suggestion is that you create a directory called shapes and copy all of the shape code (that’s the code for shape, cube, sphere, pyramid, tpyramid) into a file called __init__.py inside the shapes directory. You should then able to run the following:\nfor s in ['shape','sphere','cube','pyramid','t_pyramid']:\n    if s in locals():\n        del(s)\nfrom shapes import *\nWe need those first three lines of code to delete the existing classes from Python’s ‘memory’ so that we can be sure we’re importing the versions we saved to shapes/__init__.py.\n\n\nAdding Documentation\nIn an ideal world, this would also be the time to properly document your classes and methods. Here as some examples that you could add to the __init__.py package file.\nUnderneath the line class shape(object):, add:\n    \"\"\"Abstract base class for all ideal shape classes.\n\n    Keyword arguments:\n    dimension -- the principle dimension of the shape (default None)\n    \"\"\"\nUnderneath the line def type(self):, add:\n        \"\"\"\n        Returns the formatted name of the shape type. \n        \n        This is set automatically, but can be overwritten by setting the attribute shape_type.\n        \n        :returns: the name of the class, so shapes.cube is a `Cube` shape type\n        :rtype: str\n        \"\"\"\nThis would then allow you to run:\nfrom shapes import * # &lt;-- Change this if you didn't call your package `shapes`!\nhelp(shape)\nhelp(shape.type)"
  },
  {
    "objectID": "lectures/2.4-Mapping.html#two-cultures",
    "href": "lectures/2.4-Mapping.html#two-cultures",
    "title": "Think Maps",
    "section": "Two Cultures",
    "text": "Two Cultures\n\n\nPurely Computational\nBoth analysis and visualisation are accomplished via code:\n\nFully replicable (including random samples).\nFully documented (to extent commented by dev).\nFully portable (assuming no platform-specific code).\n\n\nMostly Computational\nOnly the analysis is accomplished via code, visualisation is via a GIS:\n\nWider variety of output formats (e.g. Atlases, 3D/web).\nBetter support for ‘finishing touches’ (e.g. scalebars, north arrows, rule-based labels, etc.).\nBetter-quality output for less effort (e.g. Model Builder + QGIS styles).\n\n\n\nWorth reflecting on pros and cons of these: when does one offer benefits over the other?"
  },
  {
    "objectID": "lectures/2.4-Mapping.html#the-challenge",
    "href": "lectures/2.4-Mapping.html#the-challenge",
    "title": "Think Maps",
    "section": "The Challenge",
    "text": "The Challenge\nThe hardest part of purely computational approaches is the need to anticipate how maps will look according to variations in:\n\nThe density and type of data\nThe context of the data\nThe different scales involved\nThe number of maps involved\nThe need to annotate and label elements\n\nUltimately, the complexity of the choices here may require the use of a scriptable GIS over ggplot or matplotlib.\n\nDon’t forget that both QGIS and Arc offer a ‘Model Builder’ that is basically ‘visual programming’."
  },
  {
    "objectID": "lectures/2.4-Mapping.html#constituency-cards",
    "href": "lectures/2.4-Mapping.html#constituency-cards",
    "title": "Think Maps",
    "section": "Constituency Cards",
    "text": "Constituency Cards\nConstituency cards are ‘scripted’ in QGIS:\n  \nSource: github.com/alasdairrae/wpc"
  },
  {
    "objectID": "lectures/2.4-Mapping.html#short-term-lets-in-scotland",
    "href": "lectures/2.4-Mapping.html#short-term-lets-in-scotland",
    "title": "Think Maps",
    "section": "Short-Term Lets in Scotland",
    "text": "Short-Term Lets in Scotland\nAnalysis of Airbnb and other short-term lets in Scotland feeding through into policy-making via Research into the impact of short-term lets on communities across Scotland"
  },
  {
    "objectID": "lectures/2.4-Mapping.html#every-building-in-america",
    "href": "lectures/2.4-Mapping.html#every-building-in-america",
    "title": "Think Maps",
    "section": "Every Building in America",
    "text": "Every Building in America\nBuilding footprints collected by Microsoft, but presentation by New York Times highlights society-nature interactions."
  },
  {
    "objectID": "lectures/2.4-Mapping.html#think-it-through",
    "href": "lectures/2.4-Mapping.html#think-it-through",
    "title": "Think Maps",
    "section": "Think it Through!",
    "text": "Think it Through!"
  },
  {
    "objectID": "lectures/2.4-Mapping.html#a-deceptively-simple-problem",
    "href": "lectures/2.4-Mapping.html#a-deceptively-simple-problem",
    "title": "Think Maps",
    "section": "A Deceptively Simple Problem",
    "text": "A Deceptively Simple Problem\n\nWe want to show data on a map in a way that is both accurate and informative.\n\nWhy might this not be possible?"
  },
  {
    "objectID": "lectures/2.4-Mapping.html#classification",
    "href": "lectures/2.4-Mapping.html#classification",
    "title": "Think Maps",
    "section": "Classification",
    "text": "Classification\nTrade-offs:\n\nThe greater the accuracy of a choropleth or other class-based map, the less it’s possible generalise from it.\nThere is no ‘right’ way to group data into an arbitrary number of discrete classes (a.k.a. to generalise).\n\nHumans can only take in so much data at once. Your choice of colour scheme, breaks, and classification can profoundly affect how people see the world."
  },
  {
    "objectID": "lectures/2.4-Mapping.html#six-views-of-employment",
    "href": "lectures/2.4-Mapping.html#six-views-of-employment",
    "title": "Think Maps",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment\n\n\n\n\n\nNo Class Breaks\n\n\n\n\n\n\n3 Quantiles"
  },
  {
    "objectID": "lectures/2.4-Mapping.html#six-views-of-employment-1",
    "href": "lectures/2.4-Mapping.html#six-views-of-employment-1",
    "title": "Think Maps",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment\n\n\n\n\n\n7 Quantiles\n\n\n\n\n\n\n7 Equal Interval"
  },
  {
    "objectID": "lectures/2.4-Mapping.html#six-views-of-employment-2",
    "href": "lectures/2.4-Mapping.html#six-views-of-employment-2",
    "title": "Think Maps",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment\n\n\n\n\n\n7 Pretty Breaks\n\n\n\n\n\n\n7 Natural Breaks"
  },
  {
    "objectID": "lectures/2.4-Mapping.html#consider",
    "href": "lectures/2.4-Mapping.html#consider",
    "title": "Think Maps",
    "section": "Consider",
    "text": "Consider\nWe want to:\n\nGroup features with similar values together.\nShow these in a way that doesn’t mislead the viewer.\n\nBut we have the following problems:\n\nToo many classes confuse the viewer.\nToo few classes hides structure/pattern."
  },
  {
    "objectID": "lectures/2.4-Mapping.html#choices-choices",
    "href": "lectures/2.4-Mapping.html#choices-choices",
    "title": "Think Maps",
    "section": "Choices, Choices",
    "text": "Choices, Choices\nAt the very least we have the following options:\n\nAssign classes manually.\nSplit range evenly (i.e. equal intervals).\nSplit data evenly (i.e. quantiles).\nSplit data according to distribution (i.e. SD).\nSplit data so that members of each group are more similar to each other than to members of another group (i.e. natural breaks/Jencks)."
  },
  {
    "objectID": "lectures/2.4-Mapping.html#look-at-the-data",
    "href": "lectures/2.4-Mapping.html#look-at-the-data",
    "title": "Think Maps",
    "section": "Look at the Data!",
    "text": "Look at the Data!\nDifferent colour and break schemes not only give us different views of the data, they give us different understandings of the data! Each scheme changes how the data looks and, consequently, how we perceive the distribution."
  },
  {
    "objectID": "lectures/2.4-Mapping.html#takeaway-maps-have-a-rhetoric",
    "href": "lectures/2.4-Mapping.html#takeaway-maps-have-a-rhetoric",
    "title": "Think Maps",
    "section": "Takeaway: Maps have a ‘Rhetoric’",
    "text": "Takeaway: Maps have a ‘Rhetoric’"
  },
  {
    "objectID": "lectures/2.4-Mapping.html#resources",
    "href": "lectures/2.4-Mapping.html#resources",
    "title": "Think Maps",
    "section": "Resources",
    "text": "Resources\n\nQGIS Styles to Share\nQGIS and 3D Visualisation\nModelling your data processing flow in QGIS\nQGIS Documentation\nWorking with Spatial Data in Python\nWeb Mapping Notes"
  },
  {
    "objectID": "lectures/2.4-Mapping.html#two-cultures-of-mapping",
    "href": "lectures/2.4-Mapping.html#two-cultures-of-mapping",
    "title": "Think Maps",
    "section": "Two Cultures of Mapping",
    "text": "Two Cultures of Mapping\n\n\nPurely Computational\nBoth analysis and visualisation are accomplished via code:\n\nFully replicable (including random samples).\nFully documented (to extent commented by dev).\nFully portable (assuming no platform-specific code).\n\n\nMostly Computational\nOnly the analysis is accomplished via code, visualisation is via a GIS:\n\nWider variety of output formats (e.g. Atlases, 3D/web).\nBetter support for ‘finishing touches’ (e.g. scalebars, north arrows, rule-based labels, etc.).\nBetter-quality output for less effort (e.g. Model Builder + QGIS styles).\n\n\n\nWorth reflecting on pros and cons of these: when does one offer benefits over the other?"
  },
  {
    "objectID": "lectures/2.3-Geodata.html#reading-writing",
    "href": "lectures/2.3-Geodata.html#reading-writing",
    "title": "Geopandas",
    "section": "Reading & Writing",
    "text": "Reading & Writing\nSupported file formats:\n\n\n\nType\nExtension\nNotes\n\n\n\n\nShape\n.shp (etc.)\nMaximum compatibility\n\n\nGeoPackage\n.gpkg\nGood default choice\n\n\nGeoJSON\n.geojson\nFor web mapping\n\n\nZip\n.zip\nFor use with Shapefiles\n\n\nWKT\n.txt\nPlain-text & SQL\n\n\nGeoParquet\n.geoparquet\nGood for large data sets & SQL\n\n\n\nAdditionally, it is possible to read only subsets of the data using row, column, geometry, and bbox filters."
  },
  {
    "objectID": "lectures/2.3-Geodata.html#reading-remote-files",
    "href": "lectures/2.3-Geodata.html#reading-remote-files",
    "title": "Geopandas",
    "section": "Reading (Remote Files)",
    "text": "Reading (Remote Files)\nAgain, depending on file size you may want to save these locally, but…\nimport geopandas as gpd\ngpkg_src = 'https://orca.casa.ucl.ac.uk/~jreades/jaipur/TM_WORLD_BORDERS-0.3.gpkg'\nworld = gpd.read_file(gpkg_src)\n# The ';' suppresses matplotlib output\nworld.plot(facecolor='white', edgecolor='darkblue');"
  },
  {
    "objectID": "lectures/2.3-Geodata.html#writing-local-files",
    "href": "lectures/2.3-Geodata.html#writing-local-files",
    "title": "Geopandas",
    "section": "Writing (Local Files)",
    "text": "Writing (Local Files)\nWrite any OGR-supported vector drivers.\nworld.to_file('world.gpkg', driver='GPKG')\nworld.to_file('world.shp', driver='ESRI Shapefile')\nworld.to_file('world.geojson', driver='GeoJSON')\n\nIf you forget to specify the driver it writes shapefiles by default. This is mainly an issue if you try to write a GeoPackage or GeoJSON file but then end up writing a shapefile to a directory called world.gpkg!"
  },
  {
    "objectID": "lectures/2.3-Geodata.html#data-structures",
    "href": "lectures/2.3-Geodata.html#data-structures",
    "title": "Geopandas",
    "section": "Data Structures",
    "text": "Data Structures\nGeoPandas does all this by adding just two new classes:\n\nGeoDataFrame\nGeoSeries\n\nIn principle, a GeoSeries can contain multiple geo-data types, but in practice you’ll want to be one of the following shapely classes:\n\nPoints / Multi-Points\nLines / Multi-Lines\nPolygons / Multi-Polygons"
  },
  {
    "objectID": "lectures/2.3-Geodata.html#consider",
    "href": "lectures/2.3-Geodata.html#consider",
    "title": "Geopandas",
    "section": "Consider",
    "text": "Consider\nRecall that we can ask if a particular object is an instance of any given class:\n\nprint(isinstance(world, str))\nprint(isinstance(world, pd.DataFrame))\nprint(isinstance(world, gpd.GeoDataFrame))\n\nFalse\nTrue\nTrue\n\n\n\nprint(isinstance(world.geometry, str))\nprint(isinstance(world.geometry, pd.Series))\nprint(isinstance(world.geometry, gpd.GeoSeries))\n\nFalse\nTrue\nTrue\n\n\n\nSo converting from Pandas to GeoPandas works well because GeoPandas knows all about Pandas.\nYou can use a GeoDataFrame anywhere you’d use a DataFrame with no loss of functionality! Same for a GeoSeries, though in this case a GeoSeries cannot perform the same statistical operations."
  },
  {
    "objectID": "lectures/2.3-Geodata.html#projections",
    "href": "lectures/2.3-Geodata.html#projections",
    "title": "Geopandas",
    "section": "Projections",
    "text": "Projections\nDepending on your data source, you may or may not have projection information attached to your GeoDataFrame:\n\nprint(world.crs)\n\nEPSG:4326\n\n\nBut:\n\nworld.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich"
  },
  {
    "objectID": "lectures/2.3-Geodata.html#finding-projections",
    "href": "lectures/2.3-Geodata.html#finding-projections",
    "title": "Geopandas",
    "section": "Finding Projections",
    "text": "Finding Projections\nYou’ll have already covered this in GIS, but you can find nearly any EPSG you might need at epsg.io. By far the most commonly-used here are:\n\nEPSG:4326 for the World Geodetic System 84 used in GPS.\nEPSG:27700 for OSGB 1936/British National Grid used in the UK.\nEPSG:32643 was what I was given for Jaipur geodata.\nEPSG:3857 was what I found for some boundary data for India.\n\nNote: recall that large territories (such as Canada, China and Russia) may well have multiple projections at the state of provincial level."
  },
  {
    "objectID": "lectures/2.3-Geodata.html#reprojection",
    "href": "lectures/2.3-Geodata.html#reprojection",
    "title": "Geopandas",
    "section": "Reprojection",
    "text": "Reprojection\nFor data sets without projection information (i.e. often anything loaded from a shapefile) you must gdf.set_crs(&lt;spec&gt;). For all others you should gdf.to_crs(&lt;spec&gt;).\n\nworld2 = world.to_crs('ESRI:54030')\nworld2.plot();"
  },
  {
    "objectID": "lectures/2.3-Geodata.html#the-spatial-index",
    "href": "lectures/2.3-Geodata.html#the-spatial-index",
    "title": "Geopandas",
    "section": "The Spatial Index",
    "text": "The Spatial Index\nWe can use GeoSeries’ spatial index directly to perform simple spatial queries:\n\nimport matplotlib.pyplot as plt\nwslice = world.cx[-50:50, -20:20] # cx = coordinate index\nax = wslice.plot()\nplt.axvline(-50, linestyle='--', color='red')\nplt.axvline(50, linestyle='--', color='red')\nplt.axhline(-20, linestyle='--', color='red')\nplt.axhline(20, linestyle='--', color='red');"
  },
  {
    "objectID": "lectures/2.3-Geodata.html#attributes",
    "href": "lectures/2.3-Geodata.html#attributes",
    "title": "Geopandas",
    "section": "Attributes",
    "text": "Attributes\nA GeoSeries has attributes like any other Series, but also includes some spatially-specifc ones:\n\narea — if a polygon\nbounds — for each feature\ntotal_bounds — for each GeoSeries\ngeom_type — if you don’t already know\nis_valid — if you need to test"
  },
  {
    "objectID": "lectures/2.3-Geodata.html#methods",
    "href": "lectures/2.3-Geodata.html#methods",
    "title": "Geopandas",
    "section": "Methods",
    "text": "Methods\nAdditional GeoSeries methods icnlude:\n\ndistance() — returns Series measuring distances to some other feature (called as: &lt;GeoSeries&gt;.distance(&lt;feature&gt;))\ncentroid — returns GeoSeries of strict centroids (called as: &lt;GeoSeries&gt;.centroid)\nrepresentative_point() — returns GeoSeries of points within features\nto_crs() and plot(), which you’ve already seen.\n\n\nNote that centroid is not called with parentheses. Technically it’s more like an attribute than a method."
  },
  {
    "objectID": "lectures/2.3-Geodata.html#relationship-tests",
    "href": "lectures/2.3-Geodata.html#relationship-tests",
    "title": "Geopandas",
    "section": "Relationship Tests",
    "text": "Relationship Tests\nSimple geographical tests:\n\ngeom_almost_equals() — tries to deal with rounding issues when comparing two features.\ncontains() — is shape contained within some other features.\nintersects() — does shape intersect some other features."
  },
  {
    "objectID": "lectures/2.3-Geodata.html#converting-non-spatial-data-1",
    "href": "lectures/2.3-Geodata.html#converting-non-spatial-data-1",
    "title": "Geopandas",
    "section": "Converting Non-Spatial Data 1",
    "text": "Converting Non-Spatial Data 1\nLat/Long and Northing/Easting benefit from a helper function gpd.points_from_xy():\nurl = 'https://bit.ly/3I0XDrq'\ndf  = pd.read_csv(url)\n\ngdf = gpd.GeoDataFrame(df, \n            geometry=gpd.points_from_xy(\n                        df['longitude'], \n                        df['latitude'], \n                        crs='epsg:4326'\n            )\n      )\ngdf.plot()\n\nYou can also use list comprehensions ([x for x in list]) and zip to combine two lists but then need to specify the CRS as a separate step!"
  },
  {
    "objectID": "lectures/2.3-Geodata.html#csv-to-points-in-3-lines",
    "href": "lectures/2.3-Geodata.html#csv-to-points-in-3-lines",
    "title": "Geopandas",
    "section": "CSV to Points in 3 Lines!",
    "text": "CSV to Points in 3 Lines!\n\n\nNotice that the default plot from a GeoDataFrame is… a map!"
  },
  {
    "objectID": "lectures/2.3-Geodata.html#converting-non-spatial-data-2",
    "href": "lectures/2.3-Geodata.html#converting-non-spatial-data-2",
    "title": "Geopandas",
    "section": "Converting Non-Spatial Data 2",
    "text": "Converting Non-Spatial Data 2\nOther feature types need to be in some kind of regular format such as Well-Known Text (WKT), GeoJSON, or something readable as a Shapely geometry.\nfrom shapely import wkt\n\n# Notice coordinate pairs and last point is same as first one\nbbox = 'POLYGON((5000000.0 2500000.0, 5000000.0 -2500000.0, -5000000.0 -2500000.0, -5000000.0 2500000.0, 5000000.0 2500000.0))'\n\n# Create GeoPandas from dict just like Pandas\nbgdf = gpd.GeoDataFrame({'id':[0], 'coordinates':bbox})\n\n# Turn it into a geometry\nbgdf['geometry'] = bgdf.coordinates.apply(wkt.loads)\nbgdf = bgdf.set_crs('ESRI:54030')\nbgdf.plot() # Not very interesting but...\n\nThese are more rarely used for our purposes but knowing that they exist is useful."
  },
  {
    "objectID": "lectures/2.3-Geodata.html#from-text-to-bounding-box",
    "href": "lectures/2.3-Geodata.html#from-text-to-bounding-box",
    "title": "Geopandas",
    "section": "From Text to Bounding Box",
    "text": "From Text to Bounding Box\nscale = int(float('1e7'))\nf,ax=plt.subplots(figsize=(8,4))\nworld2.plot(ax=ax)\nbgdf.plot(ax=ax, color='none', edgecolor='r')\nax.set_xlim([-0.75*scale, +0.75*scale])\nax.set_ylim([-3*scale/10, +3*scale/10])"
  },
  {
    "objectID": "lectures/2.3-Geodata.html#resources",
    "href": "lectures/2.3-Geodata.html#resources",
    "title": "Geopandas",
    "section": "Resources",
    "text": "Resources\n\nI Hate Coordinate Systems\nGeoPandas on ReadTheDocs\nDani’s GDS Course\nDani’s Web Mapping Course"
  },
  {
    "objectID": "lectures/2.3-Geodata.html#code-wont-stop-you-being-silly",
    "href": "lectures/2.3-Geodata.html#code-wont-stop-you-being-silly",
    "title": "Geopandas",
    "section": "Code Won’t Stop You Being Silly",
    "text": "Code Won’t Stop You Being Silly\n\nworld2 = world.to_crs('EPSG:27700')\nworld2.plot();\n\n\n\n\n\n\n\n\n\nUnlike a shapefile, you can have more than one geometry column, each with a different projection. However, only one will be plotted (the one named geometry or specified via set_geometry())."
  },
  {
    "objectID": "lectures/2.1-Mapping.html#two-cultures-of-mapping",
    "href": "lectures/2.1-Mapping.html#two-cultures-of-mapping",
    "title": "Think Maps",
    "section": "Two Cultures of Mapping",
    "text": "Two Cultures of Mapping\n\n\nPurely Computational\nBoth analysis and visualisation are accomplished via code:\n\nFully replicable (including random samples).\nFully documented (to extent commented by dev).\nFully portable (assuming no platform-specific code).\n\n\nMostly Computational\nOnly the analysis is accomplished via code, visualisation is via a GIS:\n\nWider variety of output formats (e.g. Atlases, 3D/web).\nBetter support for ‘finishing touches’ (e.g. scalebars, north arrows, rule-based labels, etc.).\nBetter-quality output for less effort (e.g. Model Builder + QGIS styles).\n\n\n\nWorth reflecting on pros and cons of these: when does one offer benefits over the other?"
  },
  {
    "objectID": "lectures/2.1-Mapping.html#the-challenge",
    "href": "lectures/2.1-Mapping.html#the-challenge",
    "title": "Think Maps",
    "section": "The Challenge",
    "text": "The Challenge\nThe hardest part of purely computational approaches is the need to anticipate how maps will look according to variations in:\n\nThe density and type of data\nThe context of the data\nThe different scales involved\nThe number of maps involved\nThe need to annotate and label elements\n\nUltimately, the complexity of the choices here may require the use of a scriptable GIS over ggplot or matplotlib.\n\nDon’t forget that both QGIS and Arc offer a ‘Model Builder’ that is basically ‘visual programming’."
  },
  {
    "objectID": "lectures/2.1-Mapping.html#constituency-cards",
    "href": "lectures/2.1-Mapping.html#constituency-cards",
    "title": "Think Maps",
    "section": "Constituency Cards",
    "text": "Constituency Cards\nConstituency cards are ‘scripted’ in QGIS:\n  \nSource: github.com/alasdairrae/wpc"
  },
  {
    "objectID": "lectures/2.1-Mapping.html#short-term-lets-in-scotland",
    "href": "lectures/2.1-Mapping.html#short-term-lets-in-scotland",
    "title": "Think Maps",
    "section": "Short-Term Lets in Scotland",
    "text": "Short-Term Lets in Scotland\nAnalysis of Airbnb and other short-term lets in Scotland feeding through into policy-making via Research into the impact of short-term lets on communities across Scotland"
  },
  {
    "objectID": "lectures/2.1-Mapping.html#every-building-in-america",
    "href": "lectures/2.1-Mapping.html#every-building-in-america",
    "title": "Think Maps",
    "section": "Every Building in America",
    "text": "Every Building in America\nBuilding footprints collected by Microsoft, but presentation by New York Times highlights society-nature interactions."
  },
  {
    "objectID": "lectures/2.1-Mapping.html#a-deceptively-simple-problem",
    "href": "lectures/2.1-Mapping.html#a-deceptively-simple-problem",
    "title": "Think Maps",
    "section": "A Deceptively Simple Problem",
    "text": "A Deceptively Simple Problem\n\nWe want to show data on a map in a way that is both accurate and informative.\n\nWhy might this not be possible?"
  },
  {
    "objectID": "lectures/2.1-Mapping.html#classification",
    "href": "lectures/2.1-Mapping.html#classification",
    "title": "Think Maps",
    "section": "Classification",
    "text": "Classification\nTrade-offs:\n\nThe greater the accuracy of a choropleth or other class-based map, the less it’s possible generalise from it.\nThere is no ‘right’ way to group data into an arbitrary number of discrete classes (a.k.a. to generalise).\n\nHumans can only take in so much data at once. Your choice of colour scheme, breaks, and classification can profoundly affect how people see the world."
  },
  {
    "objectID": "lectures/2.1-Mapping.html#six-views-of-employment",
    "href": "lectures/2.1-Mapping.html#six-views-of-employment",
    "title": "Think Maps",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment\n\n\n\n\n\nNo Class Breaks\n\n\n\n\n\n\n3 Quantiles"
  },
  {
    "objectID": "lectures/2.1-Mapping.html#six-views-of-employment-1",
    "href": "lectures/2.1-Mapping.html#six-views-of-employment-1",
    "title": "Think Maps",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment\n\n\n\n\n\n7 Quantiles\n\n\n\n\n\n\n7 Equal Interval"
  },
  {
    "objectID": "lectures/2.1-Mapping.html#six-views-of-employment-2",
    "href": "lectures/2.1-Mapping.html#six-views-of-employment-2",
    "title": "Think Maps",
    "section": "Six Views of Employment",
    "text": "Six Views of Employment\n\n\n\n\n\n7 Pretty Breaks\n\n\n\n\n\n\n7 Natural Breaks"
  },
  {
    "objectID": "lectures/2.1-Mapping.html#consider",
    "href": "lectures/2.1-Mapping.html#consider",
    "title": "Think Maps",
    "section": "Consider",
    "text": "Consider\nWe want to:\n\nGroup features with similar values together.\nShow these in a way that doesn’t mislead the viewer.\n\nBut we have the following problems:\n\nToo many classes confuse the viewer.\nToo few classes hides structure/pattern."
  },
  {
    "objectID": "lectures/2.1-Mapping.html#choices-choices",
    "href": "lectures/2.1-Mapping.html#choices-choices",
    "title": "Think Maps",
    "section": "Choices, Choices",
    "text": "Choices, Choices\nAt the very least we have the following options:\n\nAssign classes manually.\nSplit range evenly (i.e. equal intervals).\nSplit data evenly (i.e. quantiles).\nSplit data according to distribution (i.e. SD).\nSplit data so that members of each group are more similar to each other than to members of another group (i.e. natural breaks/Jencks)."
  },
  {
    "objectID": "lectures/2.1-Mapping.html#takeaway-maps-have-a-rhetoric",
    "href": "lectures/2.1-Mapping.html#takeaway-maps-have-a-rhetoric",
    "title": "Think Maps",
    "section": "Takeaway: Maps have a ‘Rhetoric’",
    "text": "Takeaway: Maps have a ‘Rhetoric’"
  },
  {
    "objectID": "lectures/2.1-Mapping.html#resources",
    "href": "lectures/2.1-Mapping.html#resources",
    "title": "Think Maps",
    "section": "Resources",
    "text": "Resources\n\nQGIS Styles to Share\nQGIS and 3D Visualisation\nModelling your data processing flow in QGIS\nQGIS Documentation\nWorking with Spatial Data in Python\nWeb Mapping Notes"
  },
  {
    "objectID": "lectures/2.2-Think_Data.html#which-of-these-is-data",
    "href": "lectures/2.2-Think_Data.html#which-of-these-is-data",
    "title": "Think Data",
    "section": "Which of These is Data?",
    "text": "Which of These is Data?\n\n\n\n\ncity,famous_for,sequence\nJaipur,Amber Fort,4\nDelhi,Taj Mahal,1\nAgra,Agra Fort,2\nRanthambore,Tigers!,3\n\n\n\n\n\n\n\n\n\n\n\n\ncity\nfamous_for\nsequence\n\n\n\n\n0\nJaipur\nAmber Fort\n4\n\n\n1\nDelhi\nTaj Mahal\n1\n\n\n2\nAgra\nAgra Fort\n2\n\n\n3\nRanthambore\nTigers!\n3"
  },
  {
    "objectID": "lectures/2.2-Think_Data.html#why-this-isnt-easy",
    "href": "lectures/2.2-Think_Data.html#why-this-isnt-easy",
    "title": "Think Data",
    "section": "Why This Isn’t Easy",
    "text": "Why This Isn’t Easy\n\n\nHere’s raw Excel data.\nWhat would we say the row and column names currently are?"
  },
  {
    "objectID": "lectures/2.2-Think_Data.html#with-labels",
    "href": "lectures/2.2-Think_Data.html#with-labels",
    "title": "Think Data",
    "section": "With Labels",
    "text": "With Labels\n\n\nMetadata is relevant to our understanding of the data and so is important, but it’s not relevant to treating the data as data so we need to be able to skip it.\nColumn names are going to be how we access a given attribute for each observation.\nRow names are not normally data themselves, but are basically labels or identifiers for observations. Another term for this would be the data index.\nIf we store row and column names/indices separately from the data then we don’t have to treat them as ‘special’ or factor them into, for example, the calculation of summary stats.\nAlso have to consider trade-offs around mapping the full column names on to something a little faster and easier to type!"
  },
  {
    "objectID": "lectures/2.2-Think_Data.html#how-about-in-india",
    "href": "lectures/2.2-Think_Data.html#how-about-in-india",
    "title": "Think Data",
    "section": "How About in India?",
    "text": "How About in India?"
  },
  {
    "objectID": "lectures/2.2-Think_Data.html#from-files-to-data",
    "href": "lectures/2.2-Think_Data.html#from-files-to-data",
    "title": "Think Data",
    "section": "From Files to Data",
    "text": "From Files to Data\nIn order to read a file you need to know a few things:\n\nWhat distinguishes one record from another?\nWhat distinguishes one field from another?\nWhat ensures that a field or record is valid?\nDoes the data have row or column names?\nIs there metadata?"
  },
  {
    "objectID": "lectures/2.2-Think_Data.html#structure-of-a-tabular-data-file",
    "href": "lectures/2.2-Think_Data.html#structure-of-a-tabular-data-file",
    "title": "Think Data",
    "section": "Structure of a Tabular Data File",
    "text": "Structure of a Tabular Data File\nRow and column names (indexes) make it a lot easier to find and refer to data but they are not data and don’t belong in the data set itself.\nOften, one record (a.k.a. observation) finishes and the next one starts with a ‘newline’ (\\n) or ‘carriage return’ (\\r) or both (\\r\\n) but it could be anything (e.g. EOR).\nOften, one field (a.k.a. attribute or value) finishes and the next one starts with a comma (,), but it could be anything (e.g. ; or | or EOF).\n\nHow would we choose a good field separator?\nPro tip: if we store column and row names separately from the data then we can access everything easily without having to factor in any ‘special’ values!\nNoice also the nd here. This is the escape sequence again that you also encountered when dealing with the Shell as well. Remember that \\ is necessary if you have a space in your file name or path."
  },
  {
    "objectID": "lectures/2.2-Think_Data.html#some-common-formats",
    "href": "lectures/2.2-Think_Data.html#some-common-formats",
    "title": "Think Data",
    "section": "Some Common Formats",
    "text": "Some Common Formats\n\n\n\n\n\n\n\n\n\nExtension\nField Separator\nRecord Separator\nPython Package\n\n\n\n\n.csv\n, but separator can appear in fields enclosed by \".\n\\n but could be \\r or \\r\\n.\ncsv\n\n\n.tsv or .tab\n\\t and unlikely to appear in fields.\n\\n but could be \\r or \\r\\n.\ncsv (!)\n\n\n.xls or .xlsx\nBinary, you need a library to read.\nBinary, you need a library to read.\nxlrd/xlsxwriter\n\n\n.sav or .sas\nBinary, you need a library to read.\nBinary, you need a library to read.\npyreadstat\n\n\n.json, .geojson\nComplex (,, [], {}), but plain text.\nComplex (,, [], {}), but plain text\njson, geojson\n\n\n.shp\nBinary, you need a library to read. Need at least 3 parts (shp,dbf,shx)!\nBinary, you need a library to read.\ngeopandas, fiona\n\n\n.feather\nBinary, you need a library to read.\nBinary, you need a library to read.\npyarrow, geofeather\n\n\n.parquet\nBinary, you need a library to read.\nBinary, you need a library to read.\npyarrow\n\n\n\n\nOne of the reasons we like CSV and TSV files is that they can be opened and interacted with using the Command Line (as well as Excel/Numbers/etc.) directly. As soon as you get into binary file formats you either need the original tool (and then export) or you need a tool that can read those formats. So the complexity level rises very quickly.\nOf course, sometimes you can gain (e.g. SPSS or SAS) in terms of obtaining information about variable types, levels, etc. but usually you use these when that’s all that’s available or when you want to write a file for others to use.\nThe two formats at the bottom of the table are there because they are useful: the feather format was designed for fast reads and for data interachange with R, while Parquet is a highly-compressed, column-oriented storage format for large data. So for modest-sized data sets (a few hundred MB), or situations where you are working across R and Python, then Feather cannot be beat. For ‘big data’ where you need access to parts of the data set and want to do lazy loading, then parquet is the winner."
  },
  {
    "objectID": "lectures/2.2-Think_Data.html#mapping-data-types",
    "href": "lectures/2.2-Think_Data.html#mapping-data-types",
    "title": "Think Data",
    "section": "‘Mapping’ Data Types",
    "text": "‘Mapping’ Data Types\nYou will often hear the term ‘mapping’ used in connection to data that is not spatial, what do they mean?\nHere’s a mapping:\n\n\n\n\n\n\n\nInput (e.g. Excel)\nOutput (e.g. Python)\n\n\n\n\nNULL, N/A, “”\nNone or np.nan\n\n\n0..n\nint\n\n\n0.00…n\nfloat\n\n\nTrue/False, Y/N, 1/0\nbool\n\n\nR, G, B (etc.)\nint or str (technically a set, but hard to use with data sets)\n\n\n‘Jon Reades’, ‘Huanfa Chen’, etc.\nstr\n\n\n‘3-FEB-2020’, ‘10/25/20’, etc.\ndatetime module (date, datetime or time)\n\n\n\n\nThese would be a mapping of variables between two formats. We talk of mapping any time we are taking inputs from one data set/format/data structure as a lookup for use with another data set/format/data structure.\nHave a think about how you can use an int to represent nominal data. There are two ways: one of which will be familiar to students who have taken a stats class (with regression) and one of which is more intuitive to ‘normal’ users…"
  },
  {
    "objectID": "lectures/2.2-Think_Data.html#testing-testing",
    "href": "lectures/2.2-Think_Data.html#testing-testing",
    "title": "Think Data",
    "section": "Testing, Testing",
    "text": "Testing, Testing\n\nYou should never assume that the data matches the spec."
  },
  {
    "objectID": "lectures/2.2-Think_Data.html#things-that-can-go-wrong",
    "href": "lectures/2.2-Think_Data.html#things-that-can-go-wrong",
    "title": "Think Data",
    "section": "Things That Can Go Wrong…",
    "text": "Things That Can Go Wrong…\nA selection of real issues I’ve seen in my life:\n\nTruncation: server ran out of diskspace or memory, or a file transfer was interrupted.\nTranslation: headers don’t line up with data.\nSwapping: column order differs from spec.\nIncompleteness: range of real values differs from spec.\nCorruption: field delimitters included in field values.\nErrors: data entry errors resulted in incorrect values or the spec is downright wrong.\nIrrelevance: fields that simply aren’t relevant to your analysis.\n\n\nThese will generally require you to engage with columns and rows (via sampling) on an individual level."
  },
  {
    "objectID": "lectures/2.2-Think_Data.html#geo-data-tables",
    "href": "lectures/2.2-Think_Data.html#geo-data-tables",
    "title": "Think Data",
    "section": "Geo-Data Tables",
    "text": "Geo-Data Tables\n\nimport geopandas as gpd\ngdf = gpd.read_parquet('../data/clean/Jaipur_Wards.geoparquet')\ngdf.head(3)\n\n\n\n\n\n\n\n\nId\nAREA\nWard_No\nPOP\nDENS_PPH\nNAME\ngeometry\nArea\nWard_Numbe\n\n\n\n\n0\n0\n781.0\n99.0\n12356\n16\nADARSH NAGAR\nPOLYGON ((588147.873 2977077.51, 588066.155 29...\nNaN\nNaN\n\n\n1\n0\n234.0\n76.0\n14120\n60\nADARSH NAGAR\nMULTIPOLYGON (((585516.609 2980880.352, 585516...\nNaN\nNaN\n\n\n2\n0\n18.0\n77.0\n13879\n771\nADARSH NAGAR\nPOLYGON ((583885.577 2978737.637, 583942.867 2...\nNaN\nNaN"
  },
  {
    "objectID": "lectures/2.2-Think_Data.html#resources",
    "href": "lectures/2.2-Think_Data.html#resources",
    "title": "Think Data",
    "section": "Resources",
    "text": "Resources\n\n\n\nUnderstanding Directories and Subdirectories\nReading and writing files\nWorking with OS path utilities\nFiles and file writing\nUsing file system shell methods\nOpening files\n\n\n\nText vs. binary mode\nText files\npetl\npandas 2.0 and the Arrow revolution (Part 1)\nWhat parquet files are my preferred API for bulk open data\nDuckDB Documentation"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#why-pandas",
    "href": "lectures/2.3-Numeric_Data.html#why-pandas",
    "title": "Pandas",
    "section": "Why Pandas?",
    "text": "Why Pandas?\nPandas is probably (together with scipy, numpy, and sklearn) the main reason that Python has become popular for data science. According to ‘Learn Data Sci’ it accounts for 1% of all Stack Overflow question views!\nYou will want to bookmark these:\n\npandas.pydata.org\nPandas Docs\npandas tutorial for beginners"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#pandas-terminology-data-frame",
    "href": "lectures/2.3-Numeric_Data.html#pandas-terminology-data-frame",
    "title": "Pandas",
    "section": "Pandas Terminology (Data Frame)",
    "text": "Pandas Terminology (Data Frame)"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#pandas-terminology-index",
    "href": "lectures/2.3-Numeric_Data.html#pandas-terminology-index",
    "title": "Pandas",
    "section": "Pandas Terminology (Index)",
    "text": "Pandas Terminology (Index)"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#pandas-terminology-series",
    "href": "lectures/2.3-Numeric_Data.html#pandas-terminology-series",
    "title": "Pandas",
    "section": "Pandas Terminology (Series)",
    "text": "Pandas Terminology (Series)"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#pandas-terminology-slice",
    "href": "lectures/2.3-Numeric_Data.html#pandas-terminology-slice",
    "title": "Pandas",
    "section": "Pandas Terminology (Slice)",
    "text": "Pandas Terminology (Slice)"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#using-pandas",
    "href": "lectures/2.3-Numeric_Data.html#using-pandas",
    "title": "Pandas",
    "section": "Using Pandas",
    "text": "Using Pandas\nHere’s code to read a (remote) CSV file:\nimport pandas as pd         # import package\nurl='https://orca.casa.ucl.ac.uk/~jreades/jaipur/population.csv.gz'\ndf = pd.read_csv(url)       # load a (remote) CSV\nprint(type(df))             # not a 'simple' data type\nprint(df.columns.to_list()) # column names\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n['State', 'District', 'Subdistt', 'Town/Village', 'Ward', 'EB', 'Level', 'Name', 'TRU', 'No_HH', 'TOT_P', 'TOT_M', 'TOT_F', 'P_06', 'M_06', 'F_06', 'P_SC', 'M_SC', 'F_SC', 'P_ST', 'M_ST', 'F_ST', 'P_LIT', 'M_LIT', 'F_LIT', 'P_ILL', 'M_ILL', 'F_ILL', 'TOT_WORK_P', 'TOT_WORK_M', 'TOT_WORK_F', 'MAINWORK_P', 'MAINWORK_M', 'MAINWORK_F', 'MAIN_CL_P', 'MAIN_CL_M', 'MAIN_CL_F', 'MAIN_AL_P', 'MAIN_AL_M', 'MAIN_AL_F', 'MAIN_HH_P', 'MAIN_HH_M', 'MAIN_HH_F', 'MAIN_OT_P', 'MAIN_OT_M', 'MAIN_OT_F', 'MARGWORK_P', 'MARGWORK_M', 'MARGWORK_F', 'MARG_CL_P', 'MARG_CL_M', 'MARG_CL_F', 'MARG_AL_P', 'MARG_AL_M', 'MARG_AL_F', 'MARG_HH_P', 'MARG_HH_M', 'MARG_HH_F', 'MARG_OT_P', 'MARG_OT_M', 'MARG_OT_F', 'MARGWORK_3_6_P', 'MARGWORK_3_6_M', 'MARGWORK_3_6_F', 'MARG_CL_3_6_P', 'MARG_CL_3_6_M', 'MARG_CL_3_6_F', 'MARG_AL_3_6_P', 'MARG_AL_3_6_M', 'MARG_AL_3_6_F', 'MARG_HH_3_6_P', 'MARG_HH_3_6_M', 'MARG_HH_3_6_F', 'MARG_OT_3_6_P', 'MARG_OT_3_6_M', 'MARG_OT_3_6_F', 'MARGWORK_0_3_P', 'MARGWORK_0_3_M', 'MARGWORK_0_3_F', 'MARG_CL_0_3_P', 'MARG_CL_0_3_M', 'MARG_CL_0_3_F', 'MARG_AL_0_3_P', 'MARG_AL_0_3_M', 'MARG_AL_0_3_F', 'MARG_HH_0_3_P', 'MARG_HH_0_3_M', 'MARG_HH_0_3_F', 'MARG_OT_0_3_P', 'MARG_OT_0_3_M', 'MARG_OT_0_3_F', 'NON_WORK_P', 'NON_WORK_M', 'NON_WORK_F']"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#summarise-a-data-frame",
    "href": "lectures/2.3-Numeric_Data.html#summarise-a-data-frame",
    "title": "Pandas",
    "section": "Summarise a Data Frame",
    "text": "Summarise a Data Frame\n\n\nStatistical summarisation.\n\ndf.describe()\n\n\n\n\n\n\n\n\nState\nDistrict\nSubdistt\nTown/Village\nWard\nEB\nNo_HH\nTOT_P\nTOT_M\nTOT_F\n...\nMARG_AL_0_3_F\nMARG_HH_0_3_P\nMARG_HH_0_3_M\nMARG_HH_0_3_F\nMARG_OT_0_3_P\nMARG_OT_0_3_M\nMARG_OT_0_3_F\nNON_WORK_P\nNON_WORK_M\nNON_WORK_F\n\n\n\n\ncount\n2561.0\n2561.0\n2561.000000\n2561.000000\n2561.000000\n2561.0\n2.561000e+03\n2.561000e+03\n2.561000e+03\n2.561000e+03\n...\n2561.000000\n2561.000000\n2561.000000\n2561.000000\n2561.000000\n2561.000000\n2561.000000\n2.561000e+03\n2.561000e+03\n2.561000e+03\n\n\nmean\n8.0\n110.0\n543.976962\n169642.691527\n2.311207\n0.0\n2.561084e+03\n1.430145e+04\n7.489366e+03\n6.812087e+03\n...\n13.509567\n8.444748\n3.001171\n5.443577\n66.545881\n27.736431\n38.809449\n9.049970e+03\n3.776792e+03\n5.273178e+03\n\n\nstd\n0.0\n0.0\n19.015389\n239933.613990\n8.628916\n0.0\n3.293322e+04\n1.815688e+05\n9.511259e+04\n8.645723e+04\n...\n189.164285\n106.745661\n38.197017\n69.027967\n851.119683\n350.089056\n516.492442\n1.158499e+05\n4.771246e+04\n6.835303e+04\n\n\nmin\n8.0\n110.0\n0.000000\n0.000000\n0.000000\n0.0\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\n25%\n8.0\n110.0\n542.000000\n79596.000000\n0.000000\n0.0\n9.500000e+01\n6.010000e+02\n3.100000e+02\n2.870000e+02\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n3.120000e+02\n1.570000e+02\n1.520000e+02\n\n\n50%\n8.0\n110.0\n545.000000\n80236.000000\n0.000000\n0.0\n1.700000e+02\n1.064000e+03\n5.530000e+02\n5.120000e+02\n...\n0.000000\n0.000000\n0.000000\n0.000000\n2.000000\n1.000000\n1.000000\n6.050000e+02\n2.870000e+02\n3.110000e+02\n\n\n75%\n8.0\n110.0\n548.000000\n80873.000000\n0.000000\n0.0\n3.170000e+02\n1.934000e+03\n1.010000e+03\n9.200000e+02\n...\n2.000000\n1.000000\n0.000000\n0.000000\n11.000000\n4.000000\n6.000000\n1.148000e+03\n5.280000e+02\n6.230000e+02\n\n\nmax\n8.0\n110.0\n550.000000\n800523.000000\n77.000000\n0.0\n1.177096e+06\n6.626178e+06\n3.468507e+06\n3.157671e+06\n...\n6775.000000\n4002.000000\n1405.000000\n2597.000000\n32114.000000\n13016.000000\n19098.000000\n4.161285e+06\n1.753560e+06\n2.407725e+06\n\n\n\n\n8 rows × 91 columns\n\n\n\n\nData types and memory usage.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2561 entries, 0 to 2560\nData columns (total 94 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   State           2561 non-null   int64 \n 1   District        2561 non-null   int64 \n 2   Subdistt        2561 non-null   int64 \n 3   Town/Village    2561 non-null   int64 \n 4   Ward            2561 non-null   int64 \n 5   EB              2561 non-null   int64 \n 6   Level           2561 non-null   object\n 7   Name            2561 non-null   object\n 8   TRU             2561 non-null   object\n 9   No_HH           2561 non-null   int64 \n 10  TOT_P           2561 non-null   int64 \n 11  TOT_M           2561 non-null   int64 \n 12  TOT_F           2561 non-null   int64 \n 13  P_06            2561 non-null   int64 \n 14  M_06            2561 non-null   int64 \n 15  F_06            2561 non-null   int64 \n 16  P_SC            2561 non-null   int64 \n 17  M_SC            2561 non-null   int64 \n 18  F_SC            2561 non-null   int64 \n 19  P_ST            2561 non-null   int64 \n 20  M_ST            2561 non-null   int64 \n 21  F_ST            2561 non-null   int64 \n 22  P_LIT           2561 non-null   int64 \n 23  M_LIT           2561 non-null   int64 \n 24  F_LIT           2561 non-null   int64 \n 25  P_ILL           2561 non-null   int64 \n 26  M_ILL           2561 non-null   int64 \n 27  F_ILL           2561 non-null   int64 \n 28  TOT_WORK_P      2561 non-null   int64 \n 29  TOT_WORK_M      2561 non-null   int64 \n 30  TOT_WORK_F      2561 non-null   int64 \n 31  MAINWORK_P      2561 non-null   int64 \n 32  MAINWORK_M      2561 non-null   int64 \n 33  MAINWORK_F      2561 non-null   int64 \n 34  MAIN_CL_P       2561 non-null   int64 \n 35  MAIN_CL_M       2561 non-null   int64 \n 36  MAIN_CL_F       2561 non-null   int64 \n 37  MAIN_AL_P       2561 non-null   int64 \n 38  MAIN_AL_M       2561 non-null   int64 \n 39  MAIN_AL_F       2561 non-null   int64 \n 40  MAIN_HH_P       2561 non-null   int64 \n 41  MAIN_HH_M       2561 non-null   int64 \n 42  MAIN_HH_F       2561 non-null   int64 \n 43  MAIN_OT_P       2561 non-null   int64 \n 44  MAIN_OT_M       2561 non-null   int64 \n 45  MAIN_OT_F       2561 non-null   int64 \n 46  MARGWORK_P      2561 non-null   int64 \n 47  MARGWORK_M      2561 non-null   int64 \n 48  MARGWORK_F      2561 non-null   int64 \n 49  MARG_CL_P       2561 non-null   int64 \n 50  MARG_CL_M       2561 non-null   int64 \n 51  MARG_CL_F       2561 non-null   int64 \n 52  MARG_AL_P       2561 non-null   int64 \n 53  MARG_AL_M       2561 non-null   int64 \n 54  MARG_AL_F       2561 non-null   int64 \n 55  MARG_HH_P       2561 non-null   int64 \n 56  MARG_HH_M       2561 non-null   int64 \n 57  MARG_HH_F       2561 non-null   int64 \n 58  MARG_OT_P       2561 non-null   int64 \n 59  MARG_OT_M       2561 non-null   int64 \n 60  MARG_OT_F       2561 non-null   int64 \n 61  MARGWORK_3_6_P  2561 non-null   int64 \n 62  MARGWORK_3_6_M  2561 non-null   int64 \n 63  MARGWORK_3_6_F  2561 non-null   int64 \n 64  MARG_CL_3_6_P   2561 non-null   int64 \n 65  MARG_CL_3_6_M   2561 non-null   int64 \n 66  MARG_CL_3_6_F   2561 non-null   int64 \n 67  MARG_AL_3_6_P   2561 non-null   int64 \n 68  MARG_AL_3_6_M   2561 non-null   int64 \n 69  MARG_AL_3_6_F   2561 non-null   int64 \n 70  MARG_HH_3_6_P   2561 non-null   int64 \n 71  MARG_HH_3_6_M   2561 non-null   int64 \n 72  MARG_HH_3_6_F   2561 non-null   int64 \n 73  MARG_OT_3_6_P   2561 non-null   int64 \n 74  MARG_OT_3_6_M   2561 non-null   int64 \n 75  MARG_OT_3_6_F   2561 non-null   int64 \n 76  MARGWORK_0_3_P  2561 non-null   int64 \n 77  MARGWORK_0_3_M  2561 non-null   int64 \n 78  MARGWORK_0_3_F  2561 non-null   int64 \n 79  MARG_CL_0_3_P   2561 non-null   int64 \n 80  MARG_CL_0_3_M   2561 non-null   int64 \n 81  MARG_CL_0_3_F   2561 non-null   int64 \n 82  MARG_AL_0_3_P   2561 non-null   int64 \n 83  MARG_AL_0_3_M   2561 non-null   int64 \n 84  MARG_AL_0_3_F   2561 non-null   int64 \n 85  MARG_HH_0_3_P   2561 non-null   int64 \n 86  MARG_HH_0_3_M   2561 non-null   int64 \n 87  MARG_HH_0_3_F   2561 non-null   int64 \n 88  MARG_OT_0_3_P   2561 non-null   int64 \n 89  MARG_OT_0_3_M   2561 non-null   int64 \n 90  MARG_OT_0_3_F   2561 non-null   int64 \n 91  NON_WORK_P      2561 non-null   int64 \n 92  NON_WORK_M      2561 non-null   int64 \n 93  NON_WORK_F      2561 non-null   int64 \ndtypes: int64(91), object(3)\nmemory usage: 1.8+ MB"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#accessing-a-series",
    "href": "lectures/2.3-Numeric_Data.html#accessing-a-series",
    "title": "Pandas",
    "section": "Accessing a Series",
    "text": "Accessing a Series\n\nprint(type(df['State']))            # type for column\nprint(type(df['State'].array))      # type for values\nprint(df['State'].array[:5])        # first five values\nprint(f\"1: {df['TOT_P'].mean()}\")   # summarise a series/column\nprint(f\"2: {df.TOT_P.mean():0.2f}\") # if no spaces in name\n\n&lt;class 'pandas.core.series.Series'&gt;\n&lt;class 'pandas.core.arrays.numpy_.NumpyExtensionArray'&gt;\n&lt;NumpyExtensionArray&gt;\n[8, 8, 8, 8, 8]\nLength: 5, dtype: int64\n1: 14301.45294806716\n2: 14301.45\n\n\n\nNotice that we’ve got two ways of accessing a pandas Series:\n\nThe dictionary-like way: df['Latitude']; this works for all columns, always.\nThe method-like way: df.Latitude; this works for ‘reading’ columns without spaces in their names."
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#jupyter-formatting",
    "href": "lectures/2.3-Numeric_Data.html#jupyter-formatting",
    "title": "Pandas",
    "section": "Jupyter Formatting",
    "text": "Jupyter Formatting\nPandas is also ‘Jupyter-aware’, meaning that output can displayed directly in Jupyter and Quarto in ‘fancy’ ways:\n\ndf[['District','TOT_F','F_SC','NON_WORK_M']].sample(5)\n\n\n\n\n\n\n\n\nDistrict\nTOT_F\nF_SC\nNON_WORK_M\n\n\n\n\n2053\n110\n280\n45\n166\n\n\n2141\n110\n517\n3\n344\n\n\n2482\n110\n571\n42\n335\n\n\n2491\n110\n26\n26\n10\n\n\n418\n110\n792\n168\n453"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#nosing-around",
    "href": "lectures/2.3-Numeric_Data.html#nosing-around",
    "title": "Pandas",
    "section": "Nosing Around",
    "text": "Nosing Around\ndf.head(3)                       # First 3 rows of df\ndf[['TOT_F','F_SC']].tail(3)     # Last 3 rows of selected columns\ndf.sample(frac=0.3)              # A random 30% sample\ndf.sample(3, random_state=42)    # A random sample of 3 with a seed\ndf.sample(3, random_state=42)    # Same sample!\n\nHead and tail are used on the Command Line. Random sampling with seeds is covered in my talk on Randomness. We’ve even got Lists of Lists, which is a really basic data structure!"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#data-frames-vs-series",
    "href": "lectures/2.3-Numeric_Data.html#data-frames-vs-series",
    "title": "Pandas",
    "section": "Data Frames vs Series",
    "text": "Data Frames vs Series\nPandas operates on two principles:\n\nAny operation on a Data Frame returns a Data Frame.\nAny operation on a Series returns a Series.\n\n\nWe’ll see in a moment why this is useful!"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#shallow-copies",
    "href": "lectures/2.3-Numeric_Data.html#shallow-copies",
    "title": "Pandas",
    "section": "‘Shallow’ Copies",
    "text": "‘Shallow’ Copies\nMore subtly, operations on a Series or Data Frame return a shallow copy, which is like a ‘view’ in a database…\n\nThe original is unchanged unless you specify inplace=True (where supported).\nAttempts to change a subset of the data frame will often trigger a SettingWithCopyWarning warning.\n\nIf you need a full copy then use the copy() method (e.g. df.copy() or df.Series.copy()).\n\n\nDataQuest has a nice overview of how SettingWithCopyWarning is triggered and what to do about it."
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#putting-these-ideas-together",
    "href": "lectures/2.3-Numeric_Data.html#putting-these-ideas-together",
    "title": "Pandas",
    "section": "Putting These Ideas Together",
    "text": "Putting These Ideas Together\n\n# Returns a series but not a column\nprint(type(df.TOT_F - 1))\n# Saves returned series as a new column\ndf['smaller'] = df.TOT_F - 1\nprint(df.smaller.head(3))\n# Returns a new data frame w/o the dropped column \nprint(type(df.drop(columns=['smaller'])))\n# Modifies df directly to drop the column\nprint(type(df.drop(columns=['smaller'], inplace=True)))\n# Try to modify a view of df (triggers warning)\ndf[df.TOT_F&gt;5000].TOT_F = 5000 \n\n&lt;class 'pandas.core.series.Series'&gt;\n0    3157670\n1    1511406\n2    1646263\nName: smaller, dtype: int64\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n&lt;class 'NoneType'&gt;"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#chaining",
    "href": "lectures/2.3-Numeric_Data.html#chaining",
    "title": "Pandas",
    "section": "Chaining",
    "text": "Chaining\nOperations on a Data Frame return a DataFrame and operations on a Series return a Series, allowing us to ‘chain’ steps together:\n\ndf.sort_values(by=['TOT_P','TOT_M'], ascending=False).head(20).sample(frac=0.5).median(numeric_only=True)\n\nState                 8.0\nDistrict            110.0\nSubdistt            545.0\nTown/Village          0.0\nWard                  0.0\n                   ...   \nMARG_OT_0_3_M      1071.5\nMARG_OT_0_3_F      1252.5\nNON_WORK_P       409191.5\nNON_WORK_M       166471.5\nNON_WORK_F       242720.0\nLength: 91, dtype: float64"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#selection",
    "href": "lectures/2.3-Numeric_Data.html#selection",
    "title": "Pandas",
    "section": "Selection",
    "text": "Selection\n# Returns a selection (Boolean series)\ndf['TOT_P']&gt;5000\n# Data frame of records matching selection\ndf[ df['TOT_P']&gt;5000 ]\n\n# Calculations on a slice (returns mean centroid!)\ndf[df['TOT_P']&gt;5000][['TOT_M','TOT_F']].mean()\nYou can link several conditions using & (and) and | (or).\n# Two conditions with a bit-wise AND\ndf[\n  (df['TOT_P']&gt;5000) & (df['TOT_F']&gt;5000)\n]"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#dealing-with-types",
    "href": "lectures/2.3-Numeric_Data.html#dealing-with-types",
    "title": "Pandas",
    "section": "Dealing with Types",
    "text": "Dealing with Types\nA Data Series can only be of one type:\n\n\n\n\n\n\n\n\nPandas Dtype\nPython Type\nUsage\n\n\n\n\nobject\nstr or mixed\nText or mixed columns (including arrays)\n\n\nint64\nint\nInteger columns\n\n\nfloat64\nfloat\nFloating point columns\n\n\nbool\nbool\nTrue/False columns\n\n\ndatetime64\nN/A (datetime)\nDate and time columns\n\n\ntimedelta[ns]\nN/A (datetime)\nDatetime difference columns\n\n\ncategory\nN/A (set)\nCategorical columns"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#changing-the-type",
    "href": "lectures/2.3-Numeric_Data.html#changing-the-type",
    "title": "Pandas",
    "section": "Changing the Type",
    "text": "Changing the Type\n\nprint(f\"Unique values: {df['TRU'].unique()}\")   # Find unique values\nprint(f\"Data type is: {df['TRU'].dtype.name}\")  # Confirm is 'object'\ndf['TRU'] = df['TRU'].astype('category')\nprint(f\"Data type now: {df['TRU'].dtype.name}\") # Confirm is 'category'\nprint(df['TRU'].describe()) # Category column info\n\nUnique values: ['Total' 'Rural' 'Urban']\nData type is: object\nData type now: category\ncount      2561\nunique        3\ntop       Rural\nfreq       2194\nName: TRU, dtype: object"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#datetime-data",
    "href": "lectures/2.3-Numeric_Data.html#datetime-data",
    "title": "Pandas",
    "section": "Datetime Data",
    "text": "Datetime Data\nWhat do we do here?\nprint(df.Date.dtype.name)\n# object\ndf.Date.to_list()[:3]\n# ['04/20/2019 11:00:00 PM', '12/02/2019 10:35:00 AM', '10/06/2019 04:50:00 PM']\nThis shows that Date is currently a string of dates+times.\nPandas handles date and times using a datetime type that also works as an index (more on these later):\ndf['dt'] = pd.to_datetime(df.Date.array, \n              format=\"%m/%d/%Y %H:%M:%S %p\")\nprint(df.dt.dtype.name)\n# datetime64[ns]\ndf.dt.to_list()[:3]\n# [Timestamp('2019-04-20 11:00:00'), Timestamp('2019-12-02 10:35:00'), Timestamp('2019-10-06 04:50:00')]\nThese follow the formatting conventions of strftime (string format time) for conversion."
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#datetime-formats",
    "href": "lectures/2.3-Numeric_Data.html#datetime-formats",
    "title": "Pandas",
    "section": "Datetime Formats",
    "text": "Datetime Formats\nExamples of strftime conventions include:\n\n\n\nFormat\nApplies To\n\n\n\n\n%d\n2-digit day\n\n\n%m\n2-digit month\n\n\n%y\n2-digit year\n\n\n%Y\n4-digit year\n\n\n%p\nAM/PM\n\n\n\nSo that is why:\npd.to_datetime(df.Date.array, format=\"%m/%d/%Y %H:%M:%S %p\")\nNote the other things happening here:\n\npd.to_datetime(...) is not a method, it’s a function from the pandas package.\ndf.Date.array (and df.Date.to_numpy() and df.Data.tolist()) gives access to the data directly, whereas df.Date gives access to the Series."
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#tidying-up",
    "href": "lectures/2.3-Numeric_Data.html#tidying-up",
    "title": "Pandas",
    "section": "Tidying Up",
    "text": "Tidying Up\nThis is one way, there are many options and subtleties…\n\n# Fix categories\nmapping = {}\n\n# df['Primary Type'].unique().to_list() also works\nfor x in df['TRU'].cat.categories.to_list():\n  mapping[x]=x.lower()\n\n# And update\ndf['TRU'] = df['TRU'].cat.rename_categories(mapping)\n\nHow would you work out what this code does? 1\nThere are at least two ways: 1) print out mapping; 2) before running the code comment out the ‘update’ line and print out x and x.title(); 3) search for title python."
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#dealing-with-money",
    "href": "lectures/2.3-Numeric_Data.html#dealing-with-money",
    "title": "Pandas",
    "section": "Dealing with Money",
    "text": "Dealing with Money\nYou may encounter currency treated as a string, instead of a number. Normally, this is because of the way that the data is formatted (e.g. ‘₹1.5 lakh’) To deal with pricing information treated as a string:\n# You would need a function to deal with lakh and crore\ndf['price'].str.replace('₹','').str.\\\n            replace(',','').astype(float)\nMany more examples accessible via Google!\n\nAnother thing you might notice here: adding .cat allows us to access category methods for the Series; adding .str allows us to access string methods for the Series."
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#dropping-rows-and-columns",
    "href": "lectures/2.3-Numeric_Data.html#dropping-rows-and-columns",
    "title": "Pandas",
    "section": "Dropping Rows and Columns",
    "text": "Dropping Rows and Columns\nThere are multiple ways to drop ‘stuff’:\n\ndf2 = df.copy()\nprint(f\"The data frame has {df2.shape[0]:,} rows and {df2.shape[1]:,} cols.\")\ndf2.drop(index=range(5,1000), inplace=True) # Row 'numbers' or index values\nprint(f\"The data frame has {df2.shape[0]:,} rows and {df2.shape[1]:,} cols.\")\ndf2.drop(columns=['TOT_P'], inplace=True)   # Column name(s)\nprint(f\"The data frame has {df2.shape[0]:,} rows and {df2.shape[1]:,} cols.\")\n\nThe data frame has 2,561 rows and 94 cols.\nThe data frame has 1,566 rows and 94 cols.\nThe data frame has 1,566 rows and 93 cols.\n\n\n\nWhy might you want the default to not be in_place?\nThere is also df.dropna() which can apply to rows or columns with NULL or np.nan values."
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#accessing-data-by-location",
    "href": "lectures/2.3-Numeric_Data.html#accessing-data-by-location",
    "title": "Pandas",
    "section": "Accessing Data by Location",
    "text": "Accessing Data by Location\n\n\n\n\n\n\n\n\n\n\nIndex\n0\n1\n2\n3\n\n\n\n\n\nID\nCase Number\nDate\nPrimary Type\n\n\n0\n11667185\nJC237601\n04/20/2020 11:00:00PM\nBURGLARY\n\n\n1\n11998178\nJC532226\n12/02/2020 10:35:00AM\nDECEPTIVE PRACTICE\n\n\n2\n11852571\nJC462365\n10/06/2020 04:50:00PM\nBATTERY\n\n\n\nWe can interact with rows and columns by position or name:\ndf.iloc[0:2,0:2] # List selection! (':' means 'all')\ndf.loc[0:2,['ID','Case Number']] # Dict selection\nThese actually return different results because of the index:\n\ndf.loc returns the rows labeled 0, 1, and 2 ([0..2]), whereas\ndf.iloc returns the range 0..2 ([0..2))!"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#indexes",
    "href": "lectures/2.3-Numeric_Data.html#indexes",
    "title": "Pandas",
    "section": "Indexes",
    "text": "Indexes\nSo by default, pandas creates a row index index whose values are 0..n and column index whose values are the column names. You will see this if you select a sample:\n\ndf.sample(2)\n\n\n\n\n\n\n\n\nState\nDistrict\nSubdistt\nTown/Village\nWard\nEB\nLevel\nName\nTRU\nNo_HH\n...\nMARG_AL_0_3_F\nMARG_HH_0_3_P\nMARG_HH_0_3_M\nMARG_HH_0_3_F\nMARG_OT_0_3_P\nMARG_OT_0_3_M\nMARG_OT_0_3_F\nNON_WORK_P\nNON_WORK_M\nNON_WORK_F\n\n\n\n\n1506\n8\n110\n546\n80240\n0\n0\nVILLAGE\nBalloopura\nrural\n121\n...\n0\n0\n0\n0\n0\n0\n0\n499\n203\n296\n\n\n248\n8\n110\n539\n79208\n0\n0\nVILLAGE\nKhera\nrural\n162\n...\n0\n0\n0\n0\n2\n2\n0\n588\n319\n269\n\n\n\n\n2 rows × 94 columns\n\n\n\nThe left-most column is the index. So Name is now the index and is no longer a column: notice that the number of columns has changed.\n\ndf.set_index('Name', inplace=True)\ndf.sample(2)\n\n\n\n\n\n\n\n\nState\nDistrict\nSubdistt\nTown/Village\nWard\nEB\nLevel\nTRU\nNo_HH\nTOT_P\n...\nMARG_AL_0_3_F\nMARG_HH_0_3_P\nMARG_HH_0_3_M\nMARG_HH_0_3_F\nMARG_OT_0_3_P\nMARG_OT_0_3_M\nMARG_OT_0_3_F\nNON_WORK_P\nNON_WORK_M\nNON_WORK_F\n\n\nName\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKishanpura\n8\n110\n549\n80831\n0\n0\nVILLAGE\nrural\n216\n1151\n...\n4\n0\n0\n0\n5\n2\n3\n631\n330\n301\n\n\nRampura @ Bariyan\n8\n110\n543\n79786\n0\n0\nVILLAGE\nrural\n246\n1634\n...\n0\n0\n0\n0\n106\n2\n104\n923\n462\n461\n\n\n\n\n2 rows × 93 columns"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#indexes-contd-1",
    "href": "lectures/2.3-Numeric_Data.html#indexes-contd-1",
    "title": "Pandas",
    "section": "Indexes (cont’d 1)",
    "text": "Indexes (cont’d 1)\nSo now we can pull the data for a single ward like this:\n\ndf.loc['Jaipur (M Corp.) (Part) WARD NO.-0002',:]\n\nState                 8\nDistrict            110\nSubdistt            546\nTown/Village     800522\nWard                  2\n                  ...  \nMARG_OT_0_3_M        64\nMARG_OT_0_3_F        59\nNON_WORK_P        45438\nNON_WORK_M        17962\nNON_WORK_F        27476\nName: Jaipur (M Corp.) (Part) WARD NO.-0002, Length: 93, dtype: object"
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#indexes-contd-2",
    "href": "lectures/2.3-Numeric_Data.html#indexes-contd-2",
    "title": "Pandas",
    "section": "Indexes (cont’d 2)",
    "text": "Indexes (cont’d 2)\nAnd we can pull data for a range of wards like this:\n\ndf.loc['Jaipur (M Corp.) (Part) WARD NO.-0002':'Jaipur (M Corp.) (Part) WARD NO.-0004',\n  'TOT_P':'F_06']\n\n\n\n\n\n\n\n\nTOT_P\nTOT_M\nTOT_F\nP_06\nM_06\nF_06\n\n\nName\n\n\n\n\n\n\n\n\n\n\nJaipur (M Corp.) (Part) WARD NO.-0002\n65260\n34942\n30318\n8296\n4597\n3699\n\n\nJaipur (M Corp.) (Part) WARD NO.-0003\n39281\n21036\n18245\n4944\n2661\n2283\n\n\nJaipur (M Corp.) (Part) WARD NO.-0004\n40485\n21147\n19338\n5086\n2771\n2315\n\n\n\n\n\n\n\nMnemonic: we used iloc to select rows/cols based on integer location and we use loc to select rows/cols based on name location.\nP.S. You can reset the data frame using df.reset_index(inplace=True)."
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#saving",
    "href": "lectures/2.3-Numeric_Data.html#saving",
    "title": "Pandas",
    "section": "Saving",
    "text": "Saving\nPandas can write to a wide range of file types, including:\n\n\n\nCommand\nSaved As…\n\n\n\n\ndf.to_csv(&lt;path&gt;)\nCSV file. But note the options to change sep (default is ',') and to suppress index output (index=False).\n\n\ndf.to_excel(&lt;path&gt;)\nXLSX file. But note the options to specify a sheet_name, na_rep, and so on, as well as to suppress the index (index=False).\n\n\ndf.to_parquet(&lt;path&gt;)\nDirectly usable by many languages. Requires pyarrow to be installed to access the options.\n\n\ndf.to_latex(&lt;path&gt;))\nWrite a LaTeX-formatted table to a file. Display requires booktabs. Could do copy+paste with print(df.to_latex()).\n\n\ndf.to_markdown(&lt;path&gt;)\nWrite a Markdown-formatted table to a file. Requires tabulate. Could do copy+paste with print(df.to_markdown()).\n\n\n\nIn most cases compression is detected automatically (e.g. df.to_csv('file.csv.gz')) but you can also specify it (e.g. df.to_csv('file.csv.gz', compression='gzip')).1\nFor instance, a bit.ly link to a Gzipped file requires compression='gzip' because there’s nothing in the link itself to tell Pandas what to expect."
  },
  {
    "objectID": "lectures/2.3-Numeric_Data.html#resources",
    "href": "lectures/2.3-Numeric_Data.html#resources",
    "title": "Pandas",
    "section": "Resources",
    "text": "Resources\n\nData Cleaning with Numpy and Pandas\nPandas dtypes\nThe Index Explained\nUsing Pandas iloc\nA Clear Explanation of the Pandas Index\nUfuncs and Apply"
  },
  {
    "objectID": "practicals/Report.html#introduction",
    "href": "practicals/Report.html#introduction",
    "title": "A Project",
    "section": "",
    "text": "Some content here."
  },
  {
    "objectID": "practicals/Report.html#another-heading",
    "href": "practicals/Report.html#another-heading",
    "title": "A Project",
    "section": "Another Heading",
    "text": "Another Heading\nThe ward with the highest concentration of plastic roofing is No. 47.\nThere is also an inline version (e.g. The ward is No. 47), but it doesn’t (consistently) work for me so you might need to play about a bit…"
  },
  {
    "objectID": "practicals/Key_Concepts.html",
    "href": "practicals/Key_Concepts.html",
    "title": "Key Concepts",
    "section": "",
    "text": "In this notebook we are going to (briefly) look at two key concepts in Python (lists and dictionaries) as well as the basics of something called a package. This will hopefully help a lot with the content over the next three days!"
  },
  {
    "objectID": "practicals/Key_Concepts.html#the-task",
    "href": "practicals/Key_Concepts.html#the-task",
    "title": "Key Concepts",
    "section": "The Task",
    "text": "The Task\nOur basic task is to read a CSV file from a server and turn it into ‘data’ that we can use. This might sound hard. It is hard when you’re just starting out in programming. But it is not hard for a computer… iff we can figure out what to tell it to do and make use of work that other people have done for us!\n\nBreak Down the Problem\n\nStep 1. Analyse the Problem\nWe don’t write programs like we write essays: writing a whole lot of code and then hoping for the best when we hit ‘run’. You want to break it down into simple steps, and then tick them off one by one. Doing this gets easier as you become more familiar with programming.\nSo for this problem we might start with:\n\nFind the data\nDownload the data\nRead the data\nLoad the data\n\nWe might or might not need all of these steps. Or some steps might be easy, while others are hard! But now we can tackle each of those in turn: get the first bit working, then add the second bit, etc. It’s just like using Lego: you take the same pieces and assemble them in different ways to produce different things.\n\n\nStep 2. Functions & Packages\nSome steps in a program are done so many times by so many people that, eventually, someone writes a package that bundles up those operations into something easy to use. Packages can help us to achieve quite a lot very quickly since we just use someone else’s code. Often, if you’re not sure where to start, Google (or StackOverflow) is the place to go:\nhow to read text file on web server python\nBoom!"
  },
  {
    "objectID": "practicals/Key_Concepts.html#reading-a-remote-file",
    "href": "practicals/Key_Concepts.html#reading-a-remote-file",
    "title": "Key Concepts",
    "section": "Reading a Remote File",
    "text": "Reading a Remote File\nSo, we are going to download a file, but we aren’t going to do antything else. This is step #1, then we tackle the rest of the steps!\nBecause we’re accessing data from a ‘URL’ we need to use the urlopen function from the urllib.request package. If you’re wondering how we know to use this function and package, you might google something like: read remote csv file python3 which in turn might get you to a StackOverflow question and answer like this.\n\n\n\nfrom urllib.request import urlopen\nhelp(urlopen)\n\nHelp on function urlopen in module urllib.request:\n\nurlopen(url, data=None, timeout=&lt;object object at 0x1035e48c0&gt;, *, cafile=None, capath=None, cadefault=False, context=None)\n    Open the URL url, which can be either a string or a Request object.\n\n    *data* must be an object specifying additional data to be sent to\n    the server, or None if no such data is needed.  See Request for\n    details.\n\n    urllib.request module uses HTTP/1.1 and includes a \"Connection:close\"\n    header in its HTTP requests.\n\n    The optional *timeout* parameter specifies a timeout in seconds for\n    blocking operations like the connection attempt (if not specified, the\n    global default timeout setting will be used). This only works for HTTP,\n    HTTPS and FTP connections.\n\n    If *context* is specified, it must be a ssl.SSLContext instance describing\n    the various SSL options. See HTTPSConnection for more details.\n\n    The optional *cafile* and *capath* parameters specify a set of trusted CA\n    certificates for HTTPS requests. cafile should point to a single file\n    containing a bundle of CA certificates, whereas capath should point to a\n    directory of hashed certificate files. More information can be found in\n    ssl.SSLContext.load_verify_locations().\n\n    The *cadefault* parameter is ignored.\n\n\n    This function always returns an object which can work as a\n    context manager and has the properties url, headers, and status.\n    See urllib.response.addinfourl for more detail on these properties.\n\n    For HTTP and HTTPS URLs, this function returns a http.client.HTTPResponse\n    object slightly modified. In addition to the three new methods above, the\n    msg attribute contains the same information as the reason attribute ---\n    the reason phrase returned by the server --- instead of the response\n    headers as it is specified in the documentation for HTTPResponse.\n\n    For FTP, file, and data URLs and requests explicitly handled by legacy\n    URLopener and FancyURLopener classes, this function returns a\n    urllib.response.addinfourl object.\n\n    Note that None may be returned if no handler handles the request (though\n    the default installed global OpenerDirector uses UnknownHandler to ensure\n    this never happens).\n\n    In addition, if proxy settings are detected (for example, when a *_proxy\n    environment variable like http_proxy is set), ProxyHandler is default\n    installed and makes sure the requests are handled through the proxy.\n\n\n\n\nAs you can see, there is lot of information here about how things work. A lot of it won’t make much sense at the moment. That’s ok. Some of this doesn’t make much sense to me, but that’s because this is the full documentation trying to cover all the bases. You don’t need to read every line of this, what you are looking is information about things like the ‘signature’ (what parameters the function accepts) and its output. Of course, you can also just Google it!\n\n\n\n\n\n\nTip\n\n\n\nRemember that you can use dir(...) and help(...) to investigate what a package offers.\n\n\nBefore you start working on the code, why not open the data file directly in your browser? It’s pretty small, and it will give you a sense of what is going on.\n\nfrom urllib.request import URLError\nfrom urllib.request import urlopen\n\nurl = 'https://orca.casa.ucl.ac.uk/~jreades/jaipur/Wikipedia-Cities-simple.csv'\n\n# Read the URL into variable called 'response'\n# using the function that we imported above\ntry:\n    response = urlopen(url)\nexcept URLError as e:\n    print(\"Unable to connect to URL!\")\n    print(e)\n\n# Now read from the stream, decoding so that we get actual text\nraw = response.read()\n\n# You might want to explore what `__class__` and `__name__`\n# are doing, but basically the give us a way of finding out what\n# is 'behind' more complex variables\n\nprint(f\"'raw' variable is of type: '{raw.__class__.__name__}'.\")\nprint(f\"Raw content is:\\n{raw[:75]}...\\n\")\n\ndata = raw.decode('utf-8')\n\nprint(f\"'data' variable is of type: '{data.__class__.__name__}'.\")\nprint(f\"Decoded content is:\\n{data[:75]}...\")\n\n'raw' variable is of type: 'bytes'.\nRaw content is:\nb'City,Population,Latitude,Longitude\\r\\nPerth,45770,56.39583,-3.43333\\r\\nArmagh,1'...\n\n'data' variable is of type: 'str'.\nDecoded content is:\nCity,Population,Latitude,Longitude\nPerth,45770,56.39583,-3.43333\nArmagh,1...\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that the raw data has the format b'...' with all of the data seemingly on one line, while the decoded version in data is ‘correctly’ structured with lines! The ‘raw’ data is in bytecode format which is not, strictly, a string. It only becomes a string when we ‘decode it’ to utf-8 (which is the ‘encoding’ of text that supports most human languages). While the computer doesn’t particularly care, we do!\n\n\nRemember that you can treat strings as lists, so when we print below we cut off the output using the list[:&lt;Some Number&gt;] syntax.\n\nprint(f\"There are {len(data)} characters in the data variable.\")\nprint(f\"The first 125 characters are: '{data[:125]}'\") # Notice that '\\n' count here!\n\nThere are 352 characters in the data variable.\nThe first 125 characters are: 'City,Population,Latitude,Longitude\nPerth,45770,56.39583,-3.43333\nArmagh,14777,54.3499,-6.6546\nDundee,147268,56.462,-2.9707'\n\n\nSo this is definitely text, but it doesn’t (yet) look entirely like the data we see because it’s still just one long string, and not data which has individual records on each line. To split the text into individual lines, we can use the handily named .splitlines() method (more on methods below):\n\nrows = data.splitlines()\nprint(f\"'rows' variable is of type: {rows.__class__.__name__}'.\")\n\n'rows' variable is of type: list'.\n\n\nNote now, how the data variable has type list. So to view the data as we see them in the original online file, we can now use a for loop to print out each element of the list (each element being a row of the original online file):\n\nprint(f\"There are {len(rows)} rows of data.\")\nprint(\"\\n\".join(rows[0:2])) # New syntax alert! notice we can *join* list elements\n\nThere are 11 rows of data.\nCity,Population,Latitude,Longitude\nPerth,45770,56.39583,-3.43333\n\n\nThat’s a little hard to read, though something has clearly changed. Let’s try printing the last row:\n\nprint(rows[-1])\n\nBangor,18808,53.228,-4.128\n\n\nCongratulations! You’ve now read a text file sitting on a server in, I think, Canada and Python didn’t care. You’ve also converted a plain-text file to a row-formatted list."
  },
  {
    "objectID": "practicals/Key_Concepts.html#text-into-data",
    "href": "practicals/Key_Concepts.html#text-into-data",
    "title": "Key Concepts",
    "section": "Text into Data",
    "text": "Text into Data\nWe now need to work on turning the list into useful data. We got partway there by splitting on line-breaks (splitlines()), but now we need to get columns for each line. You’ll notice that we are dealing with a CSV (Comma-Separated Value) file and that the format looks quite simple… So, in theory, to turn this into data we ‘just’ need to split each row into separate fields using the commas.\nThere’s a handy function associated with strings called split:\n\ntest = rows[-1].split(',')\nprint(test)\nprint(f\"The population of {test[0]} is {int(test[1]):,}\")\n\n['Bangor', '18808', '53.228', '-4.128']\nThe population of Bangor is 18,808\n\n\nI’d say that we’re now getting quite close to something that looks like ‘real data’: I know how to convert a raw response from a web server into a string, to split that string into rows, and can even access individual elements from a row!"
  },
  {
    "objectID": "practicals/Key_Concepts.html#the-advantages-of-a-package",
    "href": "practicals/Key_Concepts.html#the-advantages-of-a-package",
    "title": "Key Concepts",
    "section": "The Advantages of a Package",
    "text": "The Advantages of a Package\nThere are two problems to the data.splitlines() and row.split(',') approach! One of them is visible (though not obvious) in the examples above, the other is not.\n\n10 and '10' are not the same thing. To comma-format the population of Sheffield you’ll see that I had to do int(...) in order to turn '685368' into a number. So our approach so far doesn’t know anything about the type of data we’re working with.\nWe are also implicitly assuming that commas can only appear at field boundaries (i.e. that they can only appear to separate one column of data from the next). In other words, just using split(',') doesn’t work if any of the fields can themselves contain a comma!\nThere’s actually a third potential issue, but it’s so rare that we would need to take a completely different approach to deal with it: we are also assuming that newlines (\\n) can only appear at record boundaries (i.e. that the can only appear to separate one row of data from the next). In those cases, using splitlines() also doesn’t work, but this situation is (thankfully) very rare indeed.\n\nThis is where using code that someone else who is much more interested (and knowledgeable) has written and contributed is helpful: we don’t need to think through how to deal with this sort of thing ourselves, we can just find a library that does what we need and make use of its functionality. I’ve given you the skeleton of the answer below, but you’ll need to do a little Googling to find out how to \"read csv python\".\nNote: For now just focus on problem #2.\n\nfrom urllib.request import urlopen\nimport csv\n\nresponse = urlopen(url)\nraw = response.read()\n\n# Now take the raw data, decode it, and then\n# pass it over to the CSV reader function\ncsvfile  = csv.reader(raw.decode('utf-8').splitlines()) \n\nurlData = [] # Somewhere to store the data\nfor row in csvfile:              \n    urlData.append( row )\n\nprint(\"urlData has \" + str(len(urlData)) + \" rows and \" + str(len(urlData[0])) + \" columns.\")\nprint(urlData[-1]) # Check it worked!\n\nurlData has 11 rows and 4 columns.\n['Bangor', '18808', '53.228', '-4.128']\n\n\nIf it worked, then you should have this output:\n\n\nurlData has 11 rows and 4 columns.\n['Bangor', '18808', '53.228', '-4.128']\n\n\nTo you that might look a lot worse that the data that you originally had, but to a computer that list-of-lists is something it can work with; check it out:\n\nfor u in urlData[1:6]: # For each row in the first 5 items in list\n    print(f\"The city of '{u[0]}' has a population of {int(u[1]):,}\") # Print out the name and pop\n\nThe city of 'Perth' has a population of 45,770\nThe city of 'Armagh' has a population of 14,777\nThe city of 'Dundee' has a population of 147,268\nThe city of 'Colchester' has a population of 194,706\nThe city of 'Salisbury' has a population of 40,302\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhy did I use urlData[1:] instead of urlData?\nIf you print urlData[0] you’ll see that this is the ‘header’ row that tells us what each column contains! So if we try to convert the column name to an integer (int(u[1])) we will get an error!\nThe advantage of using the csv library over plain old string.split is that the csv library knows how to deal with fields that contain commas (e.g. \"Cardfiff, Caerdydd\" or \"An Amazing 4 Bedroom Home, Central London, Sleeps 12\") and so is much more flexible and consistent that our naive split approach.\n\n\nLet’s try this with a ‘bigger’ data set… In an ideal world, the ‘power’ of code is that once we’ve solved the problem once, we’ve solved it more generally as well. So let’s try with the ‘scaled-up’ data set and see waht happens!\n\nfrom urllib.request import urlopen\nimport csv\n\nurl = \"https://orca.casa.ucl.ac.uk/~jreades/jaipur/Wikipedia-Cities.csv\"\nresponse = urlopen(url)\nraw = response.read()\n\ncsvfile = csv.reader(raw.decode('utf-8').splitlines())\n\nurlData = [] # Somewhere to store the data\n\nfor row in csvfile:              \n    urlData.append( row )\n\nprint(f\"urlData has {len(urlData)} rows and {len(urlData[0])} columns.\")\n\nfor u in urlData[70:]:  # For each row in the list\n    print(f\"The city of '{u[0]}' has a population of {u[1]}\") # Print out the name and pop\n\nurlData has 75 rows and 7 columns.\nThe city of 'Winchester' has a population of South East\nThe city of 'Wolverhampton' has a population of West Midlands\nThe city of 'Worcester' has a population of West Midlands\nThe city of 'Wrexham, Wrecsam' has a population of Wales\nThe city of 'York' has a population of Yorkshire and the Humber\n\n\n\n\n\n\n\n\nWhat mistake have I made here?\n\n\n\n\n\nI have assumed that, just because the files have similar names, they must also have similar layouts!\n\nprint(f\"The URL's data labels are: {', '.join(urlData[0])}\")\n\nThe URL's data labels are: City, Region, Founded, Population, URL, Longitude, Latitude"
  },
  {
    "objectID": "practicals/Key_Concepts.html#insight",
    "href": "practicals/Key_Concepts.html#insight",
    "title": "Key Concepts",
    "section": "Insight!",
    "text": "Insight!\nSo, although the code was basically the same for both of these files (good), we would need to change quite a bit in order to print out the same information from different versions of the same data. So our code is rather brittle.\nOne of the issues is that our instincts about how to manage data doesn’t align with how the computer can most efficiently manage it. We make the mistake of thinking that the computer needs to do things that same way that we do when reading text and so assume that we need to:\n\nRepresent the rows as a list.\nRepresent the columns as a list for each row.\n\nThis thinking suggests that the ‘right’ data structure would clearly be a list-of-lists (LoLs!), but if you understand what happened here then the next section will make a lot more sense!"
  },
  {
    "objectID": "practicals/Key_Concepts.html#understanding-whats-an-appropriate-data-structure",
    "href": "practicals/Key_Concepts.html#understanding-whats-an-appropriate-data-structure",
    "title": "Key Concepts",
    "section": "Understanding What’s an ‘Appropriate’ Data Structure",
    "text": "Understanding What’s an ‘Appropriate’ Data Structure\nIf you stop to think about it, then our list-of-lists approach to the data isn’t very easy to navigate. Notice that if the position or name of a column changes then we need to change our program every time we re-run it! It’s not very easy to read either since we don’t really know what u[5] is supposed to be. That way lies all kinds of potential errors!\nAlso consider that, in order to calculate out even a simple aggregate such as the sum of a field for all rows we need to step through a lot of irrelevant data as well: we have to write a for loop and then step through each row with an ‘accumulator’ (somewhere to store the total). That’s slow.\nThat doesn’t make much sense since this should all be easier and faster in Python than in Excel, but right now it’s harder, and quite possibly slower as well! So how does the experienced programmer get around this? ‘Simple’ (i.e. neither simple, nor obvious, until you know the answer): she realises that the data is organised the wrong way! We humans tend to think in rows of data: this apartment has the following attributes (price, location, etc.), or that city has the following attributes (population, location). We read across the row because that’s the easiest way for us to think about it. But, in short, a list-of-lists does not seem to be the right way to store this data!\nCrucially, a computer doesn’t have to work that way. For a computer, it’s as easy to read down a column as it is to read across a row. In fact, it’s easier, because each column has the same type of data: one column contains names (strings), another column contains prices (integers), and other columns contain other types of data (floats, etc.). Better still, the order of the columns often doesn’t matter as long as we know what the columns are called: it’s easier to ask for the ‘description column’ than it is to ask for the 6th column since, for all we know, the description column might be in a different place for different files but they are all (relatively) likely to use the ‘description’ label for the column itself.\n\nA Dictionary of Lists to the Rescue\nSo, if we don’t care about column order, only row order, then a dictionary of lists would be a nice way to handle things. And why should we care about column order? With our CSV files above we already saw what a pain it was to fix things when the layout of the columns changed from one data set to the next. If, instead, we can just reference the ‘description’ column then it doesn’t matter where that column actually is. Why is that?\nWell, here are the first four rows of data from a list-of-lists for city sizes:\nmyData = [\n  ['id', 'Name', 'Rank', 'Longitude', 'Latitude', 'Population'], \n  ['1', 'Greater London', '1', '-18162.92767', '6711153.709', '9787426'], \n  ['2', 'Greater Manchester', '2', '-251761.802', '7073067.458', '2553379'], \n  ['3', 'West Midlands', '3', '-210635.2396', '6878950.083', '2440986']\n]\nNow, here’s how it would look as a dictionary of lists organised by column, and not by row:\n\nmyData = {\n    'id'         : [0, 1, 2, 3, 4, 5],\n    'Name'       : ['London', 'Manchester', 'Birmingham','Edinburgh','Inverness','Lerwick'],\n    'Rank'       : [1, 2, 3, 4, 5, 6],\n    'Longitude'  : [-0.128, -2.245, -1.903, -3.189, -4.223, -1.145],\n    'Latitude'   : [51.507, 53.479, 52.480, 55.953, 57.478, 60.155],\n    'Population' : [9787426, 2705000, 1141816, 901455, 70000, 6958],\n}\n\nprint(myData['Name'])\nprint(myData['Population'])\n\n['London', 'Manchester', 'Birmingham', 'Edinburgh', 'Inverness', 'Lerwick']\n[9787426, 2705000, 1141816, 901455, 70000, 6958]\n\n\nWhat does this do better? Well, for starters, we know that everything in the ‘Name’ column will be a string, and that everything in the ‘Longitude’ column is a float, while the ‘Population’ column contains integers. So that’s made life easier already, but the real benefit is coming up…\n\n\nBehold the Power of the DoL\nNow let’s look at what you can do with this… but first we need to import one more package that you’re going to see a lot over the rest of term: numpy (Numerical Python), which is used so much that most people simply refer to it as np. This is a huge package in terms of features, but right now we’re interested only in the basic arithmatic functions: mean, max, and min.\n\n\n\n\n\n\nWe’ll step through most of these in detail below.\n\n\n\n\n\n\n\n\n\nFind the latitude of Manchester:\n\ncity = \"Manchester\"\nlat = myData['Latitude'][ myData['Name'].index(city) ]\nprint(f\"{city}'s latitude is {lat}\")\n\nManchester's latitude is 53.479\n\n\nPrint the location of Lerwick:\n\ncity = \"Lerwick\"\nprint(f\"The town of {city} can be found at \" + \n      f\"{abs(myData['Longitude'][myData['Name'].index(city)])}ºW, {myData['Latitude'][myData['Name'].index(city)]}ºN\")\n\nThe town of Lerwick can be found at 1.145ºW, 60.155ºN\n\n\nFind the easternmost city:\n\ncity = myData['Name'][ myData['Longitude'].index( max(myData['Longitude']) ) ]\nprint(f\"The easternmost city is: {city}\")\n\nThe easternmost city is: London\n\n\nFind the mean population of the cities using a handy package called numpy:\n\nimport numpy as np\nmean = np.mean(myData['Population'])\nprint(f\"The mean population is: {mean}\")\n\nThe mean population is: 2435442.5\n\n\n\n\n\n\n\n\nWarning\n\n\n\nStop! Look closely at what is going on. There’s a lot of content to process in the code above, so do not rush blindly on if this is confusing. Try pulling it apart into pieces and then reassemble it. Start with the bits that you understand and then add complexity.\n\n\nWe’ll go through each one in turn, but they nearly all work in the same way and the really key thing is that you’ll notice that we no longer have any loops (which are slow) just index or np.&lt;function&gt; (which is very fast).\n\n\nThe Population of Manchester\nThe code can look pretty daunting, so let’s break it down into two parts. What would you get if you ran just this code?\n\nmyData['Population'][1]\n\n2705000\n\n\nRemember that this is a dictionary-of-lists (DoL). So, Python first looks for a key named Population in the myData dictionary. It finds out that the value associated with this key is a list and in this example, it just pulls out the second value (index 1). Does that part make sense?\nNow, to the second part:\n\nmyData['Name'].index('Manchester')\n\n1\n\n\nHere we look in the dictionary for the key Name and find that that’s also a list. All we’re doing here is asking Python to find the index of ‘Manchester’ for us in that list. And myData['Name'].index('Manchester') gives us back a 1, so instead of just writing myData['Population'][1] we can replace the 1 with myData['Name'].index('Manchester')! Crucially, notice the complete absence of a for loop?\nDoes that make sense? If it does then you should be having a kind of an 🤯 moment because what we’ve done by taking a column view, rather than a row view, is to make Python’s index() command do the work for us. Instead of having to look through each row for a field that matches ‘Name’ and then check to see if it’s ‘Manchester’, we’ve pointed Python at the right column immediately and asked it to find the match (which it can do very quickly). Once we have a match then we also have the row number to go and do the lookup in the ‘Population’ column because the index is the row number!\n\n\nThe Easternmost City\nWhere this approach really comes into its own is on problems that involve maths. To figure out the easternmost city in this list we need to find the maximum Longitude and then use that value to look up the city name. So let’s do the same process of pulling this apart into two steps. Let start with the easier bit:\n\nmyData['Name'][0]\n\n'London'\n\n\nThat would give us the name of a city, but we don’t just want the first city in the list, we want the one with the maximum longitude. To achieve that we need to somehow replace the 0 with the index of the maximum longitude. Let’s break this down further:\n\nWe first need to find the maximum longitude.\nWe then need to find the index of that maximum longitude.\n\nSo Step 1 would be:\n\nmax_lon = max(myData['Longitude'])\n\nBecause the max(...) helps us to find the maximum longitude in the Longitude list. Now that we have that we can proceed to Step 2:\n\nmyData['Longitude'].index(max_lon)\n\n0\n\n\nSo now we ask Python to find the position of max_lon in the list. But rather than doing this in two steps we can combine into one if we write it down to make it easier to read:\n\nmyData['Longitude'].index(\n    max(myData['Longitude'])\n)\n\n0\n\n\nThere’s the same .index which tells us that Python is going to look for something in the list associated with the Longitude key. All we’ve done is change what’s inside that index function to max(myData['Longitude']). This is telling Python to find the maximum value in the myData['Longitude'] list. So to explain this in three steps, what we’re doing is:\n\nFinding the maximum value in the Longitude column (we know there must be one, but we don’t know what it is!),\nFinding the index (position) of that maximum value in the Longitude column (now that we know what the value is!),\nUsing that index to read a value out of the Name column.\n\nI am a geek, but that’s pretty cool, right? In one line of code we managed to quickly find out where the data we needed was even though it involved three discrete steps. Think about how much work you’d have to do if you were still thinking in rows, not columns!\n\n\nThe Location of Lerwick\nLerwick is a small town in the Shetlands, way up to the North of mainland U.K. and somewhere I’ve wanted to go ever since I got back from Orkney–but then I spent my honeymoon in the far North of Iceland, so perhaps I just don’t like being around lots of people… 🙃\nAnyway, this one might be a tiny bit easier conceptually than the other problems, except that I’ve deliberately used a slightly different way of showing the output that might be confusing:\nPrint the location of Lerwick:\n\ncity = \"Lerwick\"\nprint(f\"The town of {city} can be found at \" + \n      f\"{abs(myData['Longitude'][myData['Name'].index(city)])}ºW, {myData['Latitude'][myData['Name'].index(city)]}ºN\")\n\nThe town of Lerwick can be found at 1.145ºW, 60.155ºN\n\n\nThe first thing to do is to pull apart the print statement: you can see that this is actually just two ‘f-strings’ joined by a +–having that at the end of the line tells Python that it should carry on to the next line. That’s a handy way to make your code a little easier to read. If you’re creating a list and it’s getting a little long, then you can also continue a line using a , as well!\n\n1. The first f-string\nThe first string will help you to make sense of the second: f-strings allow you to ‘interpolate’ a variable into a string directly rather than having to have lots of str(x) + \" some text \" + str(y). You can write f\"{x} some text {y}\" and Python will automatically convert the variables x and y to strings and replace {x} with the value of x and {y} with the value of y.\nSo here f\"The town of {city} can be found at \" becomes f\"The town of Lerwick can be found at \" because {city} is replaced by the value of the variable city. This makes for code that is easier for humans to read and so I’d consider that a good thing.\n\n\n2. The second f-string\nThis one is hard because there’s just a lot of code there. But, again, if we start with what we recognise that it gets just a little bit more manageable… Also, it stands to reason that the only difference between the two outputs is that one asks for the ‘Longitude’ and the other for the ‘Latitude’. So if you can make sense of one you have automatically made sense of the other and don’t need to work it all out.\nLet’s start with a part that you might recognise:\n\nmyData['Name'].index(city)\n\n5\n\n\nYou’ve got this. This is just asking Python to work out the index of Lerwick (because city = 'Lerwick'). So it’s a number. 5 in this case. And we can then think, ’OK so what does this return:\n\nmyData['Longitude'][5]\n\n-1.145\n\n\nAnd the answer is -1.145. That’s the Longitude of Lerwick! There’s just one last thing: notice that we’re talking about degrees West here. So the answer isn’t a negative (because negative West degrees would be East!), it’s the absolute value. And that is the final piece of the puzzle: abs(...) gives us the absolute value of a number!\n\nhelp(abs)\n\nHelp on built-in function abs in module builtins:\n\nabs(x, /)\n    Return the absolute value of the argument.\n\n\n\n\n\n\nThe Average City Size\nHere we’re going to ‘cheat’ a little bit: rather than writing our own function, we’re going to import a package and use someone else’s function. The numpy package contains a lot of useful functions that we can call on (if you don’t believe me, add “dir(np)” on a new line after the import statement), and one of them calculates the average of a list or array of data.\n\nprint(f\"The mean population is {np.mean(myData['Population'])}\")\n\nThe mean population is 2435442.5\n\n\nThis is where our new approach really comes into its own: because all of the population data is in one place (a.k.a. a series or column), we can just throw the whole list into the np.mean function rather than having to use all of those convoluted loops and counters. Simples, right?\nNo, not simple at all, but we’ve come up with a way to make it simple.\n\n\nRecap!\nSo the really clever bit in all of this isn’t switching from a list-of-lists to a dictionary-of-lists, it’s recognising that the dictionary-of-lists is a better way to work with the data that we’re trying to analyse and that that there are useful functions that we can exploit to do the heavy lifting for us. Simply by changing the way that we stored the data in a ‘data structure’ (i.e. complex arrangement of lists, dictionaries, and variables) we were able to do away with lots of for loops and counters and conditions, and reduce many difficult operations to something that could be done on one line!"
  },
  {
    "objectID": "practicals/Key_Concepts.html#brain-teaser",
    "href": "practicals/Key_Concepts.html#brain-teaser",
    "title": "Key Concepts",
    "section": "Brain Teaser",
    "text": "Brain Teaser\n\n\n\n\n\n\nDifficulty: 🤯.\n\n\n\n\n\n\n\n\n\nWhy not have a stab at writing the code to print out the 4th most populous city? This can still be done on one line, though you might want to start by breaking the problem down:\n\nHow do I find the 4th largest value in a list?\nHow do I find the index of the 4th largest value in a list?\nHow do I use that to look up the name associated with that index?\n\nYou’ve already done #2 and #3 above so you’ve solved that problem. If you can solve #1 then the rest should fall into place.\n\n\n\n\n\n\nTip\n\n\n\nYou don’t want to use &lt;list&gt;.sort() because that will sort your data in place and break the link between the indexes across the ‘columns’; you want to research the function sorted(&lt;list&gt;) where &lt;list&gt; is the variable that holds your data and sorted(...) just returns whatever you pass it in a sorted order without changing the original list. You’ll see why this matters if you get the answer… otherwise, wait a few days for the answers to post."
  },
  {
    "objectID": "practicals/Key_Concepts.html#bringing-it-all-together",
    "href": "practicals/Key_Concepts.html#bringing-it-all-together",
    "title": "Key Concepts",
    "section": "Bringing it all together…",
    "text": "Bringing it all together…\nConceptually, this is one of the hardest practicals in the entire term because it joins up so many of the seemingly simple ideas that you covered in Code Camp into a very complex ‘stew’ – all our basic ingredients (lists, dictionaries, etc.) have simmered for a bit, been stirred up together, and become something entirely new and more complex.\nSo if this practical doesn’t make sense to you on the first runthrough, I’d suggest going back through the second half of the practical again in a couple of days’ time – that will give your brain a little time to wrap itself around the basics before you throw the hard stuff at it again. Don’t panic if it doesn’t all make sense on the second runthrough either – this is like a language, you need to practice! With luck, the second time you went through this code a little bit more made sense. If you need to do it a third time you’ll find that even more makes sense… and so on.\nThis is a very challenging notebook because it takes you both through the process of building a function incrementally and through a ‘simple’ example of how Python classes actually work. You will need to understand these two very different elements in order to make the most of the remaining 6 weeks of term, because we both improve our code incrementally and make use of objects and their inheritances extensively. You also get an extra chance to revisit the differences between LoLs and DoLs because you will undoubtedly encounter and make use of these data structures even after you become a skillfull Python programmer.\n\n\n\n\n\n\nWarning\n\n\n\nThis is a very challenging practical and you should do your best to ensure that you actually understand what you have done and why."
  },
  {
    "objectID": "practicals/Key_Concepts.html#the-way-that-doesnt-work",
    "href": "practicals/Key_Concepts.html#the-way-that-doesnt-work",
    "title": "Key Concepts",
    "section": "The Way That Doesn’t Work",
    "text": "The Way That Doesn’t Work\nRecall that this is how four rows of ‘data’ for city sizes organised by row as a list-of-lists look:\n\nmyData = [\n    ['id', 'Name', 'Rank', 'Longitude', 'Latitude', 'Population'], \n    ['1', 'Greater London', '1', '-18162.92767', '6711153.709', '9787426'], \n    ['2', 'Greater Manchester', '2', '-251761.802', '7073067.458', '2553379'], \n    ['3', 'West Midlands', '3', '-210635.2396', '6878950.083', '2440986']\n]\n\nTo print out a list of every city in the data set when we don’t know where the Name column is in the file we have to jump through some hoops:\n\ncities = []\n\ncol    = myData[0].index('Name')\nfor i in range(1, len(myData)):\n    cities.append(myData[i][col])\n\nprint(\"The cities in the data set are: \" + \", \".join(cities))\n\nThe cities in the data set are: Greater London, Greater Manchester, West Midlands\n\n\nAnd it’s the same kind of faff if we want to find out if Edinburgh is included in the data set:\n\ncol   = myData[0].index('Name')\nfound = False\nfor i in range(1, len(myData)):\n    if myData[i][col] == 'Edinburgh':\n        print(\"Found Edinburgh in the data set!\")\n        found = True\n        break\n\nif found == False:\n    print(\"Didn't find Edinburgh in the data set.\")\n\nDidn't find Edinburgh in the data set."
  },
  {
    "objectID": "practicals/Key_Concepts.html#the-way-that-does-work",
    "href": "practicals/Key_Concepts.html#the-way-that-does-work",
    "title": "Key Concepts",
    "section": "The Way That Does Work",
    "text": "The Way That Does Work\nCompare that code to how it works for a dictionary-of-lists organised by column. Now try printing out the cities in the data:\n\nmyData = {\n    'id'         : [0, 1, 2, 3, 4, 5],\n    'Name'       : ['Greater London', 'Greater Manchester', 'Birmingham','Edinburgh','Inverness','Lerwick'],\n    'Rank'       : [1, 2, 3, 4, 5, 6],\n    'Longitude'  : [-0.128, -2.245, -1.903, -3.189, -4.223, -1.145],\n    'Latitude'   : [51.507, 53.479, 52.480, 55.953, 57.478, 60.155],\n    'Population' : [9787426, 2705000, 1141816, 901455, 70000, 6958],\n}\n\nTo print out a list of every city in the data set:\n\nprint(\", \".join(myData['Name']))\n\nGreater London, Greater Manchester, Birmingham, Edinburgh, Inverness, Lerwick\n\n\nTo find out if Edinburgh is included in the list of data:\n\nif 'Edinburgh' in myData['Name']:\n    print(\"Found Edinburgh in the data set!\")\nelse:\n    print(\"Didn't find Edinburgh in the data set.\")\n\nFound Edinburgh in the data set!\n\n\nSee how even basic questions like “Is Edinburgh in our list of data?” are suddenly easy to answer? We no longer need to loop over the entire data set in order to find one data point. In addition, we know that everything in the ‘Name’ column will be a string, and that everything in the ‘Longitude’ column is a float, while the ‘Population’ column contains integers. So that’s made life easier already. But let’s test this out and see how it works."
  },
  {
    "objectID": "practicals/Key_Concepts.html#calculate-mean",
    "href": "practicals/Key_Concepts.html#calculate-mean",
    "title": "Key Concepts",
    "section": "Calculate Mean",
    "text": "Calculate Mean\nLet’s start by calculating the sample mean (use Google: Python numpy mean...):\n\nimport numpy as np\n# Use numpy functions to calculate mean and standard deviation\nmean = np.mean(myData['Population'])\nprint(f\"City distribution has a mean of {mean:,.0f}.\")\n\nCity distribution has a mean of 2,435,442."
  },
  {
    "objectID": "practicals/Key_Concepts.html#calculate-standard-deviation",
    "href": "practicals/Key_Concepts.html#calculate-standard-deviation",
    "title": "Key Concepts",
    "section": "Calculate Standard Deviation",
    "text": "Calculate Standard Deviation\nNow let’s do the standard deviation:\n\n# Use numpy functions to calculate mean and standard deviation\nstd  = np.std(myData['Population'])\nprint(f\"City distribution has a standard deviation of {std:,.2f}.\")\n\nCity distribution has a standard deviation of 3,406,947.93.\n\n\nSo the numpy package gives us a way to calculate the mean and standard deviation quickly and without having to reinvent the wheel. The other potentially new thing here is {std:,.2f}. This is about string formatting and the main thing to recognise is that this means ‘format this float with commas separating the thousands/millions and 2 digits to the right’. The link I’ve provided uses the slightly older approach of &lt;str&gt;.format() but the formatting approach is the same."
  },
  {
    "objectID": "practicals/Key_Concepts.html#for-loops-without-for-loops",
    "href": "practicals/Key_Concepts.html#for-loops-without-for-loops",
    "title": "Key Concepts",
    "section": "For Loops Without For Loops",
    "text": "For Loops Without For Loops\n\n\n\n\n\n\nDifficulty level: Medium.\n\n\n\n\n\n\n\n\n\nNow we’re going to see something called a List Comprehension.\nIn Python you will see code like this a lot: [x for x in list]. This syntax is known as a ‘list comprehension’ and is basically a for loop on one line with the output being assigned to a list. So we can apply an operation (converting to a string, subtracting a value, etc.) to every item in a list without writing out a full for loop.\nHere’s a quick example just to show you what’s going on:\n\ndemo = range(0,10) # &lt;- a *range* of numbers between 0 and 9 (stop at 10)\nprint([x**2 for x in demo]) # square every element of demo\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nNow let’s apply this to our problem. We calculated the the mean and standard deviation above, so now we want to apply the z-score formula to every element of the Population list… Remember that the format for the z-score (when dealing with a sample) is:\n\\[\nz = \\frac{x - \\bar{x}}{s}\n\\]\nAnd the population standard deviation (by which I mean, if you are dealing with all the data, and not a subsample as we are here) is:\n\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\n\nrs = [(x - mean)/std for x in myData['Population']] # rs == result set\nprint([f\"{x:.3f}\" for x in rs])\n\n['2.158', '0.079', '-0.380', '-0.450', '-0.694', '-0.713']"
  },
  {
    "objectID": "practicals/Key_Concepts.html#appending",
    "href": "practicals/Key_Concepts.html#appending",
    "title": "Key Concepts",
    "section": "Appending",
    "text": "Appending\n\n\n\n\n\n\nDifficulty level: trivial\n\n\n\n\n\n\n\n\n\nAnd now let’s add it to the data set:\n\nmyData['Std. Population'] = rs\nprint(myData['Std. Population'])\n\n[2.1579383252868527, 0.0791199354729932, -0.3797024575689938, -0.45025269939207097, -0.6942995760276591, -0.7128035277711219]\n\n\nAnd just to show how everything is in a single data structure:\n\nfor c in myData['Name']:\n    idx = myData['Name'].index(c)\n    print(f\"{c} has a population of {myData['Population'][idx]:,} and standardised score of {myData['Std. Population'][idx]:.3f}\")\n\nLondon has a population of 9,787,426 and standardised score of 2.158\nManchester has a population of 2,705,000 and standardised score of 0.079\nBirmingham has a population of 1,141,816 and standardised score of -0.380\nEdinburgh has a population of 901,455 and standardised score of -0.450\nInverness has a population of 70,000 and standardised score of -0.694\nLerwick has a population of 6,958 and standardised score of -0.713"
  },
  {
    "objectID": "practicals/Key_Concepts.html#downloading-from-a-url",
    "href": "practicals/Key_Concepts.html#downloading-from-a-url",
    "title": "Key Concepts",
    "section": "Downloading from a URL",
    "text": "Downloading from a URL\nLet’s focus on the first part first because that’s the precondition for everything else. If we can get the ‘download a file from a URL’ working then the rest will gradually fall into place through iterative improvments!\n\nFinding an Existing Answer\n\n\n\n\n\n\nDifficulty level: Low\n\n\n\n\n\n\n\n\n\nFirst, let’s be sensibly lazy–we’ve already written code to read a file from the Internet and turn it into a list of lists. So I’ve copy+pasted that into the code block below since we’re going to start from this point; however, just to help you check your own understanding, I’ve removed a few bits and replaced them with ??. Sorry, it’s good practice. 😈\n\nQuestionAnswer\n\n\nfrom urllib.request import urlopen\nimport csv\n\nurl = \"https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv\"\n\nurlData = [] # Somewhere to store the data\n\nresponse = urlopen(url) # Get the data using the urlopen function\ncsvfile  = csv.reader(response.read().decode('utf-8').splitlines()) # Pass it over to the reader\n\nfor row in csvfile:\n    urlData.append(??)\n\nprint(\"urlData has \" + str(len(urlData)) + \" rows and \" + str(len(urlData[0])) + \" columns.\")\nprint(urlData[-1]) # Check it worked!\nYou should get:\nurlData has 11 rows and 4 columns.  [‘Bangor’, ‘18808’, ‘53.228’, ‘-4.128’]\n\n\nfrom urllib.request import urlopen\nimport csv\n\nurl = \"https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv\"\n\nurlData = [] # Somewhere to store the data\n\nresponse = urlopen(url) # Get the data using the urlopen function\ncsvfile  = csv.reader(response.read().decode('utf-8').splitlines()) # Pass it over to the reader\n\nfor row in csvfile:\n    urlData.append(row)\n\nprint(\"urlData has \" + str(len(urlData)) + \" rows and \" + str(len(urlData[0])) + \" columns.\")\nprint(urlData[-1]) # Check it worked!\n\n\n\n\n\nGetting Organised\n\n\n\n\n\n\nDifficulty level: Low\n\n\n\n\n\n\n\n\n\nLet’s take the code above and modify it so that it is:\n\nA function that takes two arguments: a URL; and a destination filename.\nImplemented as a function that checks if a file exists already before downloading it again.\n\nYou will find that the os module helps here because of the path function. And you will need to Google how to test if a file exists. I would normally select a StackOverflow link in the results list over anything else because there will normally be an explanation included of why a particular answer is a ‘good one’. I also look at which answers got the most votes (not always the same as the one that was the ‘accepted answer’). In this particular case, I also found this answer useful.\nI would start by setting my inputs:\n\nimport os\nurl = \"https://raw.githubusercontent.com/jreades/fsds/master/data/src/Wikipedia-Cities-simple.csv\"\nout = os.path.join('data','Wikipedia-Cities.csv') # Print `out` if you aren't sure what this has done!\n\n\n\nSketching the Function\n\n\n\n\n\n\nDifficulty level: Low, if you’ve watched the videos…\n\n\n\n\n\n\n\n\n\nThen I would sketch out how my function will work using comments. And the simplest thing to start with is checking whether the file has already been downloaded:\n\nQuestionAnswer\n\n\nfrom urllib.request import urlopen\n\ndef get_url(src, dest):\n    \n    # Check if dest exists -- if it does\n    # then we can skip downloading the file,\n    # otherwise we have to download it!\n    if os.path.isfile(??):\n        print(f\"{dest} found!\")\n    else:\n        print(f\"{dest} *not* found!\")\n        \nget_url(url, out)\n\n\nfrom urllib.request import urlopen\n\ndef get_url(src, dest):\n\n    # Check if dest exists -- if it does\n    # then we can skip downloading the file,\n    # otherwise we have to download it!\n    if os.path.isfile(dest):\n        print(f\"{dest} found!\")\n    else:\n        print(f\"{dest} *not* found!\")\n\nget_url(url, out)\ndata/Wikipedia-Cities.csv found!\n\n\n\n\n\nFleshing Out the Function\n\n\n\n\n\n\nDifficulty level: Medium\n\n\n\n\n\nIf you really explore what’s going on in the function rather than just running it and moving on.\n\n\n\nI would then flesh out the code so that it downloads the file if it isn’t found and then, either way, returns the local file path for our CSV reader to extract:\n\ndef get_url(src, dest):\n    \n    # Check if dest does *not* exist -- that\n    # would mean we had to download it!\n    if os.path.isfile(dest):\n        print(f\"{dest} found locally!\")\n    else:\n        print(f\"{dest} not found, downloading!\")\n        \n        # Get the data using the urlopen function\n        response = urlopen(src) \n        filedata = response.read().decode('utf-8')\n        \n        # Extract the part of the dest(ination) that is *not*\n        # the actual filename--have a look at how \n        # os.path.split works using `help(os.path.split)`\n        path = list(os.path.split(dest)[:-1])\n        \n        # Create any missing directories in dest(ination) path\n        # -- os.path.join is the reverse of split (as you saw above)\n        # but it doesn't work with lists... so I had to google how \n        # to use the 'splat' operator! os.makedirs creates missing \n        # directories in a path automatically.\n        if len(path) &gt;= 1 and path[0] != '':\n            os.makedirs(os.path.join(*path), exist_ok=True)\n        \n        with open(dest, 'w') as f:\n            f.write(filedata)\n            \n        print(f\"Data written to {dest}!\")\n    \n    return dest\n        \n# Using the `return contents` line we make it easy to \n# see what our function is up to.\nsrc = get_url(url, out)\n\ndata/Wikipedia-Cities.csv found locally!"
  },
  {
    "objectID": "practicals/Key_Concepts.html#decorating",
    "href": "practicals/Key_Concepts.html#decorating",
    "title": "Key Concepts",
    "section": "Decorating!",
    "text": "Decorating!\nLet’s now look into simplifying this code using a dectorator! Our function has become a bit unwieldy and we want to look at how we can simplify that.\nThe ‘obvious’ (i.e. not obvious) way to do this is to implement the check for a local copy as a decorator on the downloading function. So we have a function that downloads, and a decorator function that checks if the download should even be triggered.\n\nfrom functools import wraps\ndef check_cache(f):\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        src  = args[0]\n        dest = args[1]\n        if os.path.isfile(dest):\n            print(f\"{dest} found locally!\")\n            return(dest)\n        else:\n            print(f\"{dest} not found, downloading!\")\n            return(f(src, dest))\n    return wrapper\n\n@check_cache\ndef get_url(src, dest):    \n    # Get the data using the urlopen function\n    response = urlopen(src) \n    filedata = response.read().decode('utf-8')\n     \n    # Extract the part of the dest(ination) that is *not*\n    # the actual filename--have a look at how \n    # os.path.split works using `help(os.path.split)`\n    path = list(os.path.split(dest)[:-1])\n     \n    # Create any missing directories in dest(ination) path\n    # -- os.path.join is the reverse of split (as you saw above)\n    # but it doesn't work with lists... so I had to google how \n    # to use the 'splat' operator! os.makedirs creates missing \n    # directories in a path automatically.\n    if len(path) &gt;= 1 and path[0] != '':\n        os.makedirs(os.path.join(*path), exist_ok=True)\n     \n    with open(dest, 'w') as f:\n        f.write(filedata)\n         \n    print(f\"Data written to {dest}!\")\n    \n    return dest\n        \n# Using the `return contents` line we make it easy to \n# see what our function is up to.\nsrc = get_url(url, out)\n\ndata/Wikipedia-Cities.csv found locally!\n\n\nI’m not going to pretend that’s the best use of a decorator, but it does neatly separate the downloading function from the caching function. In fact, there is already a cache decorator and some of these have unlimited capacity; however, they are intended to run in a ‘live’ context, so you’d still need to download the file again any time you start a new notebook or restart Docker. This caching function saves the actual data locally to dest.\n\n\n\n\n\n\nStop!\n\n\n\nIt really would be a good idea to put in the effort to make sense of how this function works. There is a lot going on here and understanding how this function works will help you to understand how to code. You should notice that we don’t try to check if the data file contains any useful data! So if you download or create an empty file while testing, you won’t necessarily get an error until you try to turn it into data afterwards!"
  },
  {
    "objectID": "practicals/Key_Concepts.html#parse-the-csv-file",
    "href": "practicals/Key_Concepts.html#parse-the-csv-file",
    "title": "Key Concepts",
    "section": "Parse the CSV File",
    "text": "Parse the CSV File\n\n\n\n\n\n\nDifficulty: Low\n\n\n\n\n\n\n\n\n\nNow we turn to the next task: parsing the file if it’s a CSV. This implies that it might not be so that’s something we should also consider!\n\nQuestionAnswer\n\n\nimport csv\n\ndef read_csv(src):\n    \n    csvdata = []\n    with open(src, 'r') as f:\n        csvr = csv.??(f)\n        \n        for r in csvr:\n            csvdata.append(??)\n    \n    # Return list of lists\n    return ??\n\nread_csv(src)\n#read_csv('foo.bar') # &lt;- Notice what happens if you try to run this code\n#read_csv('Practical-04-Objects-Answers.ipynb') # Or this code!\n\n\nimport csv\n\ndef read_csv(src):\n\n    csvdata = []\n    with open(src, 'r') as f:\n        csvr = csv.reader(f)\n\n        for r in csvr:\n            csvdata.append(r)\n\n    # Return list of lists\n    return csvdata\n\nread_csv(src)\n#read_csv('foo.bar') # &lt;- Notice what happens if you try to run this code\n#read_csv('Practical-04-Objects-Answers.ipynb') # Or this code!\n[['City', 'Population', 'Latitude', 'Longitude'],\n ['Perth', '45770', '56.39583', '-3.43333'],\n ['Armagh', '14777', '54.3499', '-6.6546'],\n ['Dundee', '147268', '56.462', '-2.9707'],\n ['Colchester', '194706', '51.88861', '0.90361'],\n ['Salisbury', '40302', '51.07', '-1.79'],\n ['Portsmouth', '205056', '50.80583', '-1.08722'],\n ['Wakefield', '325837', '53.683', '-1.499'],\n ['Bradford', '522452', '53.792', '-1.754'],\n ['Lancaster', '138375', '54.047', '-2.801'],\n ['Bangor', '18808', '53.228', '-4.128']]\n\n\n\nYou should get:\n [[‘City’, ‘Population’, ‘Latitude’, ‘Longitude’], [‘Perth’, ‘45770’, ‘56.39583’, ‘-3.43333’], [‘Armagh’, ‘14777’, ‘54.3499’, ‘-6.6546’], [‘Dundee’, ‘147268’, ‘56.462’, ‘-2.9707’], [‘Colchester’, ‘194706’, ‘51.88861’, ‘0.90361’], [‘Salisbury’, ‘40302’, ‘51.07’, ‘-1.79’], [‘Portsmouth’, ‘205056’, ‘50.80583’, ‘-1.08722’], [‘Wakefield’, ‘325837’, ‘53.683’, ‘-1.499’], [‘Bradford’, ‘522452’, ‘53.792’, ‘-1.754’], [‘Lancaster’, ‘138375’, ‘54.047’, ‘-2.801’], [‘Bangor’, ‘18808’, ‘53.228’, ‘-4.128’]]"
  },
  {
    "objectID": "practicals/Key_Concepts.html#convert-the-csv-into-a-dol",
    "href": "practicals/Key_Concepts.html#convert-the-csv-into-a-dol",
    "title": "Key Concepts",
    "section": "Convert the CSV into a DoL",
    "text": "Convert the CSV into a DoL\n\n\n\n\n\n\nDifficulty: Medium.\n\n\n\n\n\n\n\n\n\nNow we can focus on converting the CSV data to a dictionary-of-lists! We’re going to start with the same function name but expand what the function does. This kind of iteration is common in software development.\n\nQuestionAnswer\n\n\ndef read_csv(src):\n    \n    csvdata = {} # An empty dictionary-of-lists\n    \n    with open(??, 'r') as f:\n        csvr = csv.reader(f)\n        \n        # Read in our column names and\n        # initialise the dictionary-of-lists\n        csvcols = next(csvr) \n        for c in csvcols:\n            csvdata[c] = []\n        \n        # Notice this code is still the same, \n        # we just used next(csvr) to get the \n        # header row first!\n        for r in ??: \n            # Although you can often assume that the order \n            # of the keys is the same, Python doesn't \n            # guarantee it; this way we will always make\n            # the correct assignment.\n            for idx, c in enumerate(csvcols):\n                csvdata[??].append(r[idx])\n    \n    # Return dictionary of lists\n    return csvdata\n\nread_csv(src)\n\n\nimport csv\n\ndef read_csv(src):\n\n    csvdata = {} # An empty dictionary-of-lists\n\n    with open(src, 'r') as f:\n        csvr = csv.reader(f)\n\n        # Read in our column names and\n        # initialise the dictionary-of-lists\n        csvcols = next(csvr)\n        for c in csvcols:\n            csvdata[c] = []\n\n        # Notice this code is still the same,\n        # we just used next(csvr) to get the\n        # header row first!\n        for r in csvr:\n            # Although you can often assume that the order\n            # of the keys is the same, Python doesn't\n            # guarantee it; this way we will always make\n            # the correct assignment.\n            for idx, c in enumerate(csvcols):\n                csvdata[c].append(r[idx])\n\n    # Return dictionary of lists\n    return csvdata\n\nread_csv(src)\n{'City': ['Perth',\n  'Armagh',\n  'Dundee',\n  'Colchester',\n  'Salisbury',\n  'Portsmouth',\n  'Wakefield',\n  'Bradford',\n  'Lancaster',\n  'Bangor'],\n 'Population': ['45770',\n  '14777',\n  '147268',\n  '194706',\n  '40302',\n  '205056',\n  '325837',\n  '522452',\n  '138375',\n  '18808'],\n 'Latitude': ['56.39583',\n  '54.3499',\n  '56.462',\n  '51.88861',\n  '51.07',\n  '50.80583',\n  '53.683',\n  '53.792',\n  '54.047',\n  '53.228'],\n 'Longitude': ['-3.43333',\n  '-6.6546',\n  '-2.9707',\n  '0.90361',\n  '-1.79',\n  '-1.08722',\n  '-1.499',\n  '-1.754',\n  '-2.801',\n  '-4.128']}\n\n\n\nYou should get something that starts:\n\n\n{'City': ['Perth', 'Armagh', 'Dundee', 'Colchester', 'Salisbury', 'Portsmou..."
  },
  {
    "objectID": "practicals/Key_Concepts.html#adding-docstring",
    "href": "practicals/Key_Concepts.html#adding-docstring",
    "title": "Key Concepts",
    "section": "Adding Docstring",
    "text": "Adding Docstring\n\n\n\n\n\n\nDifficulty: Low\n\n\n\n\n\n\n\n\n\nWe’ve assumed that the first row of our data set is always a header (i.e. list of column names). If it’s not then this code is going to have problems. A robust function would allow us to specify column names, skip rows, etc. when we create the data structure, but let’s not get caught up in that level of detail. Notice that I’ve also, for the first time:\n\nUsed the docstring support offered by Python. You’ll be able to use help(...) and get back the docstring help!\nProvided hints to Python about the expected input and output data types. This can help to ensure consistency and is also critical in testing / continuous integration when working with others on a codebase.\n\n\ndef read_csv(src:str) -&gt; dict:\n    \"\"\"\n    Converts a CSV file to a dictionary-of-lists (dol),\n    using the first row to create column names.\n    \n    param src: a local CSV file\n    returns: a dictionary-of-lists\n    \"\"\"\n    csvdata = {} # An empty dictionary-of-lists\n    \n    with open(src, 'r') as f:\n        csvr = csv.reader(f)\n        \n        # Read in our column names and\n        # initialise the dictionary-of-lists\n        csvcols = next(csvr) \n        for c in csvcols:\n            csvdata[c] = []\n        \n        # Notice this code is still the same, \n        # we just used next(csvr) to get the \n        # header row first!\n        for r in csvr: \n            # Although you can often assume that the order \n            # of the keys is the same, Python doesn't \n            # guarantee it; this way we will always make\n            # the correct assignment.\n            for idx, c in enumerate(csvcols):\n                csvdata[c].append(r[idx])\n    \n    # Return dictionary of lists\n    return csvdata\n\nds = read_csv(src)\n\n\nhelp(read_csv)\n\nHelp on function read_csv in module __main__:\n\nread_csv(src: str) -&gt; dict\n    Converts a CSV file to a dictionary-of-lists (dol),\n    using the first row to create column names.\n\n    param src: a local CSV file\n    returns: a dictionary-of-lists\n\n\n\n\nprint(\"Columns are: \" + \", \".join(ds.keys()))\nprint(f\"First two cities are: {ds['City'][:2]}\")\nprint(f\"First two populations are: {ds['Population'][:2]}\")\nprint(f\"First two latitudes are: {ds['Latitude'][:2]}\")\n\nColumns are: City, Population, Latitude, Longitude\nFirst two cities are: ['Perth', 'Armagh']\nFirst two populations are: ['45770', '14777']\nFirst two latitudes are: ['56.39583', '54.3499']"
  },
  {
    "objectID": "practicals/Key_Concepts.html#creating-a-package",
    "href": "practicals/Key_Concepts.html#creating-a-package",
    "title": "Key Concepts",
    "section": "Creating a Package",
    "text": "Creating a Package\nWe’re not going to tackle this now, but it’s important that you understand how what we’ve done connects to what we’re about to do, and the concept of a package is the bridge. We’ve already covered this in the pre-recorded lectures, but if you want to actually try to create your own package, the simplest way to do this is to:\n\nCopy the read_csv into a new file called, for instance, utils.py.\nMake sure you delete this function from the current ‘namespace’ (del(read_csv)) by which I mean that the read_csv function no longer exists (running help(read_csv) should give you an error!).\nTry importing the function from the file: from utils import read_csv and run the help(read_csv) code again.\n\nAssuming that you’ve done everything correctly, we’ve now brought in code from another file without having to write it into our main Python script file. In Python, many of the most complex libraries are spread across the equivalent of many utils.py files, but on top of that when we import and run them they are also creating objects from classes defined in those files.\nWhat we now want to do is use a fairly simple example using different ‘shapes’ (pyramids, cubes, etc.) that allow us to explore how classes work through inheritance from parents and can extend of overwrite the functionality provided by the parent class. We’ll need this understanding in order to grasp how Pandas and GeoPandas work specifically, but also how Python works more generally."
  },
  {
    "objectID": "practicals/Key_Concepts.html#abstract-base-class",
    "href": "practicals/Key_Concepts.html#abstract-base-class",
    "title": "Key Concepts",
    "section": "Abstract Base Class",
    "text": "Abstract Base Class\nThis class appears to do very little, but there are two things to notice:\n\nIt provides a constructor (__init__) that sets the shape_type to the name of the class automatically (so a square object has shape_type='Square') and it stores the critical dimension of the shape in self.dim.\nIt provides methods (which only raise exceptions) that will allow one shape to be used in the place of any other shape that inherits from shape.\n\n\n# Base class shape\nclass shape(object): # Inherit from base class \n    def __init__(self, dimension:float=None):\n        self.shape_type = self.__class__.__name__.capitalize()\n        self.dim = dimension\n        return\n    \n    def diameter(self):\n        raise Exception(\"Unimplmented method error.\")\n    \n    def volume(self):\n        raise Exception(\"Unimplmented method error.\")\n    \n    def surface(self):\n        raise Exception(\"Unimplmented method error.\")\n        \n    def type(self):\n        return(self.shape_type)\n\nWe can now create a new shape object (an instance of the shape class) but we can’t do much that is useful with it:\n\ns = shape(15)\n\ntry: \n    print(f\"I am a {s.type()}\")\n    print(f\"My volume is {s.volume()}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\nI am a Shape\nError: Unimplmented method error."
  },
  {
    "objectID": "practicals/Key_Concepts.html#cube",
    "href": "practicals/Key_Concepts.html#cube",
    "title": "Key Concepts",
    "section": "Cube",
    "text": "Cube\nImplements a cube:\n\nThe diameter of the cube is given by the Pythagorean formula for the length of the hypotenuse in 3D between opposing corners: \\(\\sqrt{d^{2} + d^{2} + d^{2}}\\) which we can reduce to \\(\\sqrt{3 d^{2}}\\).\nA cube’s volume is given by \\(d^{3}\\).\nA cube’s surface area will be the sum of its six faces: \\(6d^{2}\\).\n\n\nQuestionAnswer\n\n\nCan you work out the missing elements that will allow you to create a cube class?\n# Cube class\nclass cube(shape): # Inherit from shape \n    def __init__(self, dim:float):\n        super().__init__(dim)\n        return\n    \n    def diameter(self):\n        return (3 * self.??**2)**(1/2)\n    \n    def volume(self):\n        return self.dim**3\n    \n    def surface(self):\n        return ??*(self.dim**2)\n\n# If you've done everything correctly then\n# you will no longer get an error here...\ns = cube(15)\n\ntry: \n    print(f\"I am a {s.type()}\")\n    print(f\"My volume is {s.volume()}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n\n# Cube class\nclass cube(shape): # Inherit from shape\n    def __init__(self, dim:float):\n        super().__init__(dim)\n        return\n\n    def diameter(self):\n        return (3 * self.dim**2)**(1/2)\n\n    def volume(self):\n        return self.dim**3\n\n    def surface(self):\n        return 6*(self.dim**2)\n\n# If you've done everything correctly then\n# you will no longer get an error here...\ns = cube(15)\n\ntry: \n    print(f\"I am a {s.type()}\")\n    print(f\"My volume is {s.volume()}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\nI am a Cube\nMy volume is 3375"
  },
  {
    "objectID": "practicals/Key_Concepts.html#sphere",
    "href": "practicals/Key_Concepts.html#sphere",
    "title": "Key Concepts",
    "section": "Sphere",
    "text": "Sphere\nImplements a sphere:\n\nThe diameter is twice the critical dimension (radius): \\(2r\\).\nThe volume is \\(\\frac{4}{3} \\pi r^{3}\\).\nThe surface area will be \\(4 \\pi r^{2}\\).\n\nIf we were writing something more general, we’d probably have spheres as a special case of an ellipsoid!\n\nQuestionAnswer\n\n\nCan you work out the missing elements that will allow you to create a cube class?\n# Sphere class\nfrom math import pi\nclass sphere(shape): # Inherit from shape\n    def __init__(self, dim:float):\n        # Something...\n\n    def diameter(self):\n        # Something...\n\n    def volume(self):\n        # Something\n\n    def surface(self):\n        # Something\n\n# If you've done everything correctly then\n# you will no longer get an error here...\ns = sphere(15)\n\ntry: \n    print(f\"I am a {s.type()}\")\n    print(f\"My volume is {s.volume()}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\n\nfrom math import pi\n# Sphere class\nclass sphere(shape): # Inherit from shape\n    def __init__(self, dim:float):\n        super().__init__(dim)\n        return\n\n    def diameter(self):\n        return self.dim*2\n\n    def volume(self):\n        return (4/3) * pi * self.dim**3\n\n    def surface(self):\n        return 4 * pi * (self.dim**2)\n\n# If you've done everything correctly then\n# you will no longer get an error here...\ns = sphere(15)\n\ntry: \n    print(f\"I am a {s.type()}\")\n    print(f\"My volume is {s.volume()}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\nI am a Sphere\nMy volume is 14137.166941154068"
  },
  {
    "objectID": "practicals/Key_Concepts.html#regular-pyramid",
    "href": "practicals/Key_Concepts.html#regular-pyramid",
    "title": "Key Concepts",
    "section": "Regular Pyramid",
    "text": "Regular Pyramid\nWe’re taking this to be a regular pyramid where all sides are equal:\n\nThe diameter is a line drawn across the base between opposing corners of the base so it’s just \\(\\sqrt{d^{2} + d^{2}}\\).\nThe volume is given by \\(V = b * h / 3\\) (where \\(b\\) is the area of the base, which in this case becomes \\(d^{2} * h/3\\)).\nThe surface area will be the base + 4 equilateral triangles: \\(d^{2} + 4 (d^{2}\\sqrt{3}/4)\\) which we can reduce to \\(d^{2} + d^{2}\\sqrt{3}\\)\n\nBut this requires a height method that is specific to pyramids:\n\nThe height is taken from the centre of the pyramid (which will be half the length of the hypotenuse for two edges): \\(l = \\sqrt{d{^2} + d^{2}}\\) and the long side (\\(d\\) again) which gives us \\(\\sqrt{l/2 + d^{2}}\\).\n\n\n\n\n\n\n\nClass Variables\n\n\n\nNote that this has a class variable called has_mummies since Egyptian regular pyramids are plagued by them! This class variable is set automatically for all instances of the pyramid class. Changing this variable can have weird effects so they’re not often changed.\n\n\n\n# Pyramid class\nclass pyramid(shape): # Inherit from shape\n\n    has_mummies = True # This is for *all* regular pyramids\n\n    def __init__(self, dim:float):\n        super().__init__(dim)\n        self.shape_type = 'Regular Pyramid'\n        return\n\n    def diameter(self):\n        return (self.dim**2 + self.dim**2)**(1/2)\n\n    def height(self):\n        return (self.diameter()/2 + self.dim**2)**(1/2)\n\n    def volume(self):\n        return self.dim**2 * self.height() / 3\n\n    def surface(self):\n        return self.dim**2 + self.dim**2 * 3**(1/2)"
  },
  {
    "objectID": "practicals/Key_Concepts.html#triangular-pyramid",
    "href": "practicals/Key_Concepts.html#triangular-pyramid",
    "title": "Key Concepts",
    "section": "Triangular Pyramid",
    "text": "Triangular Pyramid\nWe have chosen for triangular pyramid to inherit from regular pyramid. However, this is kind of a judgement call since there’s very little shared between the two types of pyramid and it’s arguable whether this one is actually simpler and should therefore be the parent class…\nJust to note, as well, that since all sides are equal this is an equilateral triangular pyramid. Anyway, the calculations are:\n\nThe diameter (longest line through the shape) will just be the edge: \\(d\\).\nThe volume \\(V = b * h / 3\\) where \\(b\\) is the area of an equilateral triangle.\nThe surface area will be \\(4b\\) where \\(b\\) is the area of an equilateral triangle.\n\nSo we now need two new formulas:\n\nThe height of the pyramid using (Pythagoras again): \\(h = \\sqrt{6}d/3\\).\nThe area of an equilateral triangle: \\(\\frac{\\sqrt{3}}{4} d^{2}\\)\n\nTriangular pyramids do not have a problem with mummies.\nWhy don’t you add some documentation to this class and the regular pyramid class so that we know how to use them correctly?\n\n# Triangular Pyramid class\nclass t_pyramid(pyramid): # Inherit from regular pyramid\n\n    has_mummies = False # This is for all triangular pyramids\n\n    def __init__(self, dim:float):\n        super().__init__(dim)\n        self.shape_type = 'Triangular Pyramid'\n        return\n\n    def diameter(self):\n        return self.dim\n\n    def height(self):\n        # h = sqrt(6)/3 * d\n        return 6**(1/2)/3 * self.dim\n\n    def base(self):\n        return 3**(1/2)/4 * self.dim**2\n\n    def volume(self):\n        return (1/3) * self.base() * self.height()\n\n    def surface(self):\n        return 4 * self.base()"
  },
  {
    "objectID": "practicals/Key_Concepts.html#testing-your-classes",
    "href": "practicals/Key_Concepts.html#testing-your-classes",
    "title": "Key Concepts",
    "section": "Testing Your Classes",
    "text": "Testing Your Classes\nIf you’ve implemented everything correctly then the following code should run.\n# How would you test these changes?\ns = sphere(10)\nprint(s.type())\nprint(f\"\\tVolume is: {s.volume():5.2f}\")\nprint(f\"\\tDiameter is: {s.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {s.surface():5.2f}\")\nprint(\"\")\n\nc = cube(10)\nprint(c.type())\nprint(f\"\\tVolume is: {c.volume():5.2f}\")\nprint(f\"\\tDiameter is: {c.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {c.surface():5.2f}\")\nprint(\"\")\n\np = pyramid(10)\nprint(p.type())\nprint(f\"\\tVolume is: {p.volume():5.2f}\")\nprint(f\"\\tDiameter is: {p.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {p.surface():5.2f}\")\nprint(f\"\\tHeight is: {p.height():5.2f}\")\nif p.has_mummies is True:\n    print(\"\\tMummies? Aaaaaaaaargh!\")\nelse:\n    print(\"\\tPhew, no mummies!\")\nprint(\"\")\n\np2 = t_pyramid(10)\nprint(p2.type())\nprint(f\"\\tVolume is: {p2.volume():5.2f}\")\nprint(f\"\\tDiameter is: {p2.diameter():5.2f}\")\nprint(f\"\\tSurface Area is: {p2.surface():5.2f}\")\nprint(f\"\\tHeight is: {p2.height():5.2f}\")\nif p2.has_mummies is True:\n    print(\"\\tMummies? Aaaaaaaaargh!\")\nelse:\n    print(\"\\tPhew, no mummies!\")\nprint(\"\")\n\n# Useful demonstration of how to find out if a method or attribute is\n# associated with a particular object\nif hasattr(p2,'base_area'):\n    print(f\"Shape of type '{p2.type()}' has attribute or method 'base_area'\")\nelse:\n    print(f\"Shape of type '{p2.type()}' does *not* have attribute or method 'base_area'\")\nprint(\"\")\nI get the following output:\n\n\nSphere\n    Volume is: 4188.79\n    Diameter is: 20.00\n    Surface Area is: 1256.64\n\nCube\n    Volume is: 1000.00\n    Diameter is: 17.32\n    Surface Area is: 600.00\n\nRegular Pyramid\n    Volume is: 344.92\n    Diameter is: 14.14\n    Surface Area is: 273.21\n    Height is: 10.35\n    Mummies? Aaaaaaaaargh!\n\nTriangular Pyramid\n    Volume is: 117.85\n    Diameter is: 10.00\n    Surface Area is: 173.21\n    Height is:  8.16\n    Phew, no mummies!\n\nShape of type 'Triangular Pyramid' does *not* have attribute or method 'base_area'"
  },
  {
    "objectID": "practicals/Key_Concepts.html#packaging-it-up",
    "href": "practicals/Key_Concepts.html#packaging-it-up",
    "title": "Key Concepts",
    "section": "Packaging It Up",
    "text": "Packaging It Up\nWait, you’re still working on this practical and haven’t thrown up your hands in disgust yet? OK, in that case you can have one more thing to do: turn all the shapes into a package that can be loaded via an import statement.\n\nCell Magic\nThis code allows Jupyter to reload external libraries if they are edited after you import them. When you are working on your own packages this is rather useful since you tend to make a lot of mistakes when packaging code up this way and it’s handy not to have to restart the entire notebook every time you fix a typo or change a function.\n%load_ext autoreload\n%autoreload 2\n\n\nImport Shapes\nMy suggestion is that you create a directory called shapes and copy all of the shape code (that’s the code for shape, cube, sphere, pyramid, tpyramid) into a file called __init__.py inside the shapes directory. You should then able to run the following:\nfor s in ['shape','sphere','cube','pyramid','t_pyramid']:\n    if s in locals():\n        del(s)\nfrom shapes import *\nWe need those first three lines of code to delete the existing classes from Python’s ‘memory’ so that we can be sure we’re importing the versions we saved to shapes/__init__.py.\n\n\nAdding Documentation\nIn an ideal world, this would also be the time to properly document your classes and methods. Here as some examples that you could add to the __init__.py package file.\nUnderneath the line class shape(object):, add:\n    \"\"\"Abstract base class for all ideal shape classes.\n\n    Keyword arguments:\n    dimension -- the principle dimension of the shape (default None)\n    \"\"\"\nUnderneath the line def type(self):, add:\n        \"\"\"\n        Returns the formatted name of the shape type. \n        \n        This is set automatically, but can be overwritten by setting the attribute shape_type.\n        \n        :returns: the name of the class, so shapes.cube is a `Cube` shape type\n        :rtype: str\n        \"\"\"\nThis would then allow you to run:\nfrom shapes import * # &lt;-- Change this if you didn't call your package `shapes`!\nhelp(shape)\nhelp(shape.type)"
  },
  {
    "objectID": "practicals/Key_Concepts.html#lists",
    "href": "practicals/Key_Concepts.html#lists",
    "title": "Key Concepts",
    "section": "Lists",
    "text": "Lists\nLike a list on your phone, a Python list is just an ordered collection of ‘things’. They could be pretty juch any thing. Groceries. Largest Cities in the World. Most famous Indian actors. It doesn’t matter.\n\n# A list is *create* using square brackets with items separated by commas\nmy_list = ['Apples','Bananas','Lentils','Cleaning supplies',4,'A new hoover']\n\n# Basic info\nprint(f\"Type of my_list is {type(my_list)}\")\nprint(f\"Length of my_list is {len(my_list)}\")\nprint()\n\n# The first item in the list\nprint(my_list[0])\nprint()\n\n# The last item in the list\nprint(my_list[-1])\nprint() \n\n# Loop over the list\nfor i in my_list:\n    print(i)\nprint()\n\n# Slightly different loop\nfor i in range(0,len(my_list)):\n    print(f\"Item {i} is {my_list[i]}\")\n\nType of my_list is &lt;class 'list'&gt;\nLength of my_list is 6\n\nApples\n\nA new hoover\n\nApples\nBananas\nLentils\nCleaning supplies\n4\nA new hoover\n\nItem 0 is Apples\nItem 1 is Bananas\nItem 2 is Lentils\nItem 3 is Cleaning supplies\nItem 4 is 4\nItem 5 is A new hoover"
  },
  {
    "objectID": "practicals/Key_Concepts.html#dictionaries",
    "href": "practicals/Key_Concepts.html#dictionaries",
    "title": "Key Concepts",
    "section": "Dictionaries",
    "text": "Dictionaries\nA dictionary (or ‘dict’ for short) in Python is kind of like a real dictionary: you look up values using a word or other ‘key’!\n\n# A list is *create* using square brackets with items separated by commas\nmy_dict = {'Apples':'Tasty','Bananas':'Tasty','Lentils':'Tasty','Cleaning supplies':'Not Tasty',4:'Not Tasty','A new hoover':'Not Tasty'}\n\n# Basic info\nprint(f\"Type of my_dict is {type(my_dict)}\")\nprint(f\"Size of my_dict is {len(my_dict)}\")\nprint() \n\n# The first item in the list\nprint(my_dict['Apples'])\nprint()\n\n# The last item in the list\nprint(my_dict['A new hoover'])\nprint() \n\n# Loop over the list\nfor k in my_dict.keys():\n    print(f\"{k} has value {my_dict[k]}\")\n\nType of my_dict is &lt;class 'dict'&gt;\nSize of my_dict is 6\n\nTasty\n\nNot Tasty\n\nApples has value Tasty\nBananas has value Tasty\nLentils has value Tasty\nCleaning supplies has value Not Tasty\n4 has value Not Tasty\nA new hoover has value Not Tasty"
  },
  {
    "objectID": "practicals/Key_Concepts.html#packages",
    "href": "practicals/Key_Concepts.html#packages",
    "title": "Key Concepts",
    "section": "Packages",
    "text": "Packages\nA package is just some code that someone has written and shared with the rest of the world. Some of these are built into Python, some are ones that we can install ourselves after installing Python. Here we make use of four packages using variations of the import statement…\n\nimport urllib.request\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\n\nurl = \"https://jreades.github.io/jaipur/lectures/img/Octocat.png\"\n\n# Get the data\nresponse = urllib.request.urlopen(url)\nimage_data = response.read()\n\n# Load the image\nimg = Image.open(BytesIO(image_data))\n\n# Display the image\nplt.imshow(img)"
  },
  {
    "objectID": "lectures/3.5-Git.html#how-it-works",
    "href": "lectures/3.5-Git.html#how-it-works",
    "title": "Getting to Grips with Git",
    "section": "How It Works",
    "text": "How It Works\nThe natural way normal people think about managing versions of a document is to save a copy with a new name that somehow shows which version is most recent.\nThe natural way developers used to think about managing versions of a document is to have a master copy somewhere. Everyone asks the server for the master copy, makes some changes, and then checks those changes back in.\nThis is not how Git works.\n\nThe way normal people approach this problem assumes that, usually, only one or two people are making changes. But how do you coordinate with 20 other people to find out who has the most recent copy then collect all 21 people’s changes?\nThe way developers used to approach this problem assumes that someone is in final charge. That a company or organisation runs a server which will decide whose changes are allowed, and whose are not."
  },
  {
    "objectID": "lectures/3.5-Git.html#how-git-works",
    "href": "lectures/3.5-Git.html#how-git-works",
    "title": "Getting to Grips with Git",
    "section": "How Git Works",
    "text": "How Git Works\nGit is distributed, meaning that every computer where git is installed has its own master copy.\nSo every computer has a full history of any git project (aka. repository or ‘repo’). Indeed, you don’t have to synchronise your repo with any other computer or server at all! 1\n\nIn order to make this useful, you need ways to synchronise changes between computers that all think they’re right.\n\nI’d suggest that this is leaving the benefit of free backups on the table for no good reason!"
  },
  {
    "objectID": "lectures/3.5-Git.html#github",
    "href": "lectures/3.5-Git.html#github",
    "title": "Getting to Grips with Git",
    "section": "GitHub",
    "text": "GitHub\nGitHub is nothing special to Git, just another Git server with which to negotiate changes. Do not think of GitHub as the ‘master’ copy. There isn’t one.\nThere are, however, upstream and remote repositories.\n\nAn ‘upstream’ repository is where there’s a ‘gatekeeper’: e.g. the people who run PySAL have a repo that is considered the ‘gatekeeper’ for PySAL.\nA remote repository is any repository with which your copy synchronises. So the remote repository can be ‘upstream’ or it can just be another computer you run, or you GitHub account."
  },
  {
    "objectID": "lectures/3.5-Git.html#a-dropbox-analogy",
    "href": "lectures/3.5-Git.html#a-dropbox-analogy",
    "title": "Getting to Grips with Git",
    "section": "A Dropbox Analogy",
    "text": "A Dropbox Analogy\n\nThink of JupyterLab as being like Word or Excel: an application that allows you to read/write/edit notebook files.\nThink of GitHub as being like Dropbox: a place somewhere in the cloud that files on your home machine can be backed up.\n\nBut Dropbox doesn’t have the .gitignore file!\n\nGitHub offers a lot of ‘value added’ features (like simple text editing) on top of the basic service of ‘storing files’."
  },
  {
    "objectID": "lectures/3.5-Git.html#getting-started",
    "href": "lectures/3.5-Git.html#getting-started",
    "title": "Getting to Grips with Git",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\nTerm\nMeans\n\n\n\n\nRepository (Repo)\nA project or achive stored in Git.\n\n\ninit\nTo create a new repo on your computer.\n\n\nclone\nTo make a full copy of a repo somewhere else.\n\n\n\nThis creates a local repo that is unsynchronised with anything else:\nmkdir test\ncd test\ngit init\nWhereas this creates a local clone that is fully synchronised with GitHub:\ncd .. # To move out of 'test'\ngit clone https://github.com/jreades/fsds.git"
  },
  {
    "objectID": "lectures/3.5-Git.html#working-on-a-file",
    "href": "lectures/3.5-Git.html#working-on-a-file",
    "title": "Getting to Grips with Git",
    "section": "Working on a File",
    "text": "Working on a File\n\n\n\nTerm\nMeans\n\n\n\n\nadd\nAdd a file to a repo.\n\n\nmv\nMove/Rename a file in a repo.\n\n\nrm\nRemove a file from a repo.\n\n\n\nFor example:\ncd test # Back into the new Repo\ntouch README.md # Create empty file called README.md\ngit add README.md # Add it to the repository\ngit mv README.md fileA.md # Rename it (move it)\ngit rm fileA.md # Remove it... which is an Error!\nThis produces:\nerror: the following file has changes staged in the index:\n    fileA.md\n(use --cached to keep the file, or -f to force removal)\n\nThis is telling you that you can force remove (git rm -f fileA.md) if you really want, but you’d probably be better off commiting the changes that have been ‘staged’… more on this in a second!\nAlso: no one else knows about these changes yet!"
  },
  {
    "objectID": "lectures/3.5-Git.html#looking-at-the-history",
    "href": "lectures/3.5-Git.html#looking-at-the-history",
    "title": "Getting to Grips with Git",
    "section": "Looking at the History",
    "text": "Looking at the History\n\n\n\nTerm\nMeans\n\n\n\n\ndiff\nShow changes between commits.\n\n\nstatus\nShow status of files in repo.\n\n\nlog\nShow history of commits.\n\n\n\nFor example:\ncd ../test/ # In case you weren't already there\ngit status  # What's the status\nThis produces:\nOn branch master\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n    new file:   fileA.md\n\nSo again, git is giving us hints as to the options: ‘changes to be committed’ vs. ‘unstage’ the changes. We can also see what files are to be committed (i.e. have changed)."
  },
  {
    "objectID": "lectures/3.5-Git.html#working-on-a-project-or-file",
    "href": "lectures/3.5-Git.html#working-on-a-project-or-file",
    "title": "Getting to Grips with Git",
    "section": "Working on a Project or File",
    "text": "Working on a Project or File\n\n\n\nTerm\nMeans\n\n\n\n\ncommit\nTo record changes to the repo.\n\n\nbranch\nCreate or delete branches.\n\n\ncheckout\nJump to a different branch.\n\n\n\nFor example:\ngit commit -m \"Added and then renamed the README.\"\ngit status\nYou should see:\n[master (root-commit) e7a0b25] Added and then renamed the README Markdown file.\n 1 file changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 fileA.md\n# ... and then this:\nOn branch master\nnothing to commit, working tree clean\nMake a note of the number after ‘root-commit’!"
  },
  {
    "objectID": "lectures/3.5-Git.html#recovery",
    "href": "lectures/3.5-Git.html#recovery",
    "title": "Getting to Grips with Git",
    "section": "Recovery",
    "text": "Recovery\ngit rm fileA.md\ngit status\ngit commit -m \"Removed file.\"\nls \ngit checkout &lt;number you wrote down earlier&gt;\nls \n\nSo every operation on a file is recorded in the repository: adding, renaming, deleting, and so on. And we can roll back any change at any time. For plain-text files (such as Markdown, Python and R scripts) these changes are recorded at the level of each line of code: so you can jump around through your entire history of a project and trace exactly when and what changes you (or anyone else) made."
  },
  {
    "objectID": "lectures/3.5-Git.html#a-rock-climbing-analogy",
    "href": "lectures/3.5-Git.html#a-rock-climbing-analogy",
    "title": "Getting to Grips with Git",
    "section": "A Rock-Climbing Analogy",
    "text": "A Rock-Climbing Analogy"
  },
  {
    "objectID": "lectures/3.5-Git.html#collaborating-on-a-project",
    "href": "lectures/3.5-Git.html#collaborating-on-a-project",
    "title": "Getting to Grips with Git",
    "section": "Collaborating on a Project",
    "text": "Collaborating on a Project\n\n\n\nTerm\nMeans\n\n\n\n\npull\nTo request changes on a repo from another computer.\n\n\npush\nTo send changes on a repo to another computer.\n\n\n\nFor example:\ngit push"
  },
  {
    "objectID": "lectures/3.5-Git.html#a-note-on-workflow",
    "href": "lectures/3.5-Git.html#a-note-on-workflow",
    "title": "Getting to Grips with Git",
    "section": "A Note on Workflow",
    "text": "A Note on Workflow\nSo your workflow should be:\n\nSave edits to Jupyter notebook.\nRun git add &lt;filename.ipynb&gt; to record changes to the notebook (Note: replace &lt;filename.ipynb&gt; completely with the notebook filename).\nRun git commit -m \"Adding notes based on lecture\" (or whatever message is appropriate: -m means ‘message’).\nThen run git push to push the changes to GitHub.\n\nIf any of those commands indicate that there are no changes being recorded/pushed then it might be that you’re not editing the file that you think you are (this happens to me!).\nOn the GitHub web site you may need to force reload the view of the repository: Shift + the Reload button usually does it in most browsers. You may also need to wait 5 to 10 seconds for the changes to become ‘visible’ before reloading. It’s not quite instantaeous."
  },
  {
    "objectID": "lectures/3.5-Git.html#this-is-not-easy",
    "href": "lectures/3.5-Git.html#this-is-not-easy",
    "title": "Getting to Grips with Git",
    "section": "This is not easy",
    "text": "This is not easy\n\nSource"
  },
  {
    "objectID": "lectures/3.5-Git.html#so-here-are-some-cheat-sheets",
    "href": "lectures/3.5-Git.html#so-here-are-some-cheat-sheets",
    "title": "Getting to Grips with Git",
    "section": "So here are some cheat sheets",
    "text": "So here are some cheat sheets"
  },
  {
    "objectID": "lectures/3.5-Git.html#resources",
    "href": "lectures/3.5-Git.html#resources",
    "title": "Getting to Grips with Git",
    "section": "Resources",
    "text": "Resources\n\nUnderstanding Git (Part 1) – Explain it Like I’m Five\nTrying Git\nVisualising Git\nGit Novice\nConfusing Git Terminology\nGit Cheat Sheet: Commands and Best Practices\nAndy’s R-focussed Tutorial\n\n\nI now have everything in Git repos: articles, research, presentations, modules… the uses are basically endless once you start using Markdown heavily (even if you don’t do much coding)."
  }
]